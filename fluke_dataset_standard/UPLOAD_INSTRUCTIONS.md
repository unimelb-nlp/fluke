# FLUKE Dataset Upload Instructions (Standard Format)

## âœ… **Problem Solved!**

The "arbitrary Python code execution" error has been fixed by converting the dataset to standard Parquet format. Your dataset will now work with Hugging Face's automatic dataset viewer!

## ğŸ“ **What Changed**

- âŒ Removed: Custom loading script (`fluke.py`)
- âŒ Removed: Complex JSON directory structure  
- âœ… Added: Standard Parquet files for each task
- âœ… Added: Combined `train.parquet` with all tasks
- âœ… Enabled: Automatic Hugging Face dataset viewer

## ğŸš€ **Upload Steps**

### Prerequisites

```bash
pip install huggingface_hub datasets
huggingface-cli login  # Enter your token
```

### Method 1: Python API (Recommended)

```python
from huggingface_hub import HfApi, create_repo

# Initialize API
api = HfApi()

# Create repository (replace YOUR_USERNAME)
repo_id = "YOUR_USERNAME/fluke"
create_repo(repo_id, repo_type="dataset", private=False)

# Upload the standard format dataset
api.upload_folder(
    folder_path="./fluke_dataset_standard",
    repo_id=repo_id,
    repo_type="dataset"
)

print(f"âœ… Dataset uploaded to: https://huggingface.co/datasets/{repo_id}")
```

### Method 2: Git CLI

```bash
# Create repository
huggingface-cli repo create fluke --type dataset

# Clone and upload
git clone https://huggingface.co/datasets/YOUR_USERNAME/fluke
cd fluke
cp -r ../fluke_dataset_standard/* ./
git add .
git commit -m "Add FLUKE dataset in standard format"
git push
```

## ğŸ“Š **Usage After Upload**

Your dataset will be automatically detected and users can load it with:

```python
from datasets import load_dataset

# Load all tasks combined
dataset = load_dataset("YOUR_USERNAME/fluke")

# Load specific task
coref_data = load_dataset("YOUR_USERNAME/fluke", data_files="coref.parquet")
ner_data = load_dataset("YOUR_USERNAME/fluke", data_files="ner.parquet")
sa_data = load_dataset("YOUR_USERNAME/fluke", data_files="sa.parquet")
dialogue_data = load_dataset("YOUR_USERNAME/fluke", data_files="dialogue.parquet")

# Filter by modification type
train_data = dataset["train"]
negation_examples = train_data.filter(lambda x: x["modification_type"] == "negation")
```

## âœ¨ **Benefits of Standard Format**

1. **âœ… Dataset Viewer Enabled**: Users can browse your data on Hugging Face
2. **âœ… Faster Loading**: Parquet format is optimized for ML workloads
3. **âœ… Automatic Discovery**: No custom code needed
4. **âœ… Better Integration**: Works seamlessly with datasets library
5. **âœ… Smaller Size**: Compressed Parquet files (~0.4MB total vs ~8MB original)

## ğŸ” **Verification**

After upload, check that:
1. Dataset page loads: `https://huggingface.co/datasets/YOUR_USERNAME/fluke`
2. Dataset viewer works (you'll see a data preview)
3. Loading works: `load_dataset("YOUR_USERNAME/fluke")`

## ğŸ“‹ **File Structure Uploaded**

```
fluke_dataset_standard/
â”œâ”€â”€ README.md              # Dataset card with metadata
â”œâ”€â”€ train.parquet          # Combined dataset (6,386 examples)
â”œâ”€â”€ coref.parquet         # Coreference task (1,551 examples)
â”œâ”€â”€ ner.parquet           # NER task (1,549 examples)
â”œâ”€â”€ sa.parquet            # Sentiment task (1,644 examples)
â”œâ”€â”€ dialogue.parquet      # Dialogue task (1,642 examples)
â”œâ”€â”€ requirements.txt      # Dependencies
â””â”€â”€ docs/
    â””â”€â”€ USAGE.md         # Detailed usage guide
```

## ğŸ†˜ **Troubleshooting**

If you still encounter issues:

1. **Check file sizes**: Each Parquet file should be small (~100KB each)
2. **Verify format**: Use `pandas.read_parquet()` to test locally
3. **Test loading**: Try `load_dataset()` locally before uploading
4. **Repository settings**: Ensure repository is public
5. **Token permissions**: Verify your token has write access

## ğŸ‰ **Success!**

Once uploaded, your FLUKE dataset will be:
- âœ… Discoverable on Hugging Face Hub
- âœ… Viewable in the browser
- âœ… Easy to load and use
- âœ… Professional and research-ready 