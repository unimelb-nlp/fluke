{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import re\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_space(text):\n",
    "    # Remove multiple spaces\n",
    "    text = ' '.join(text.split())\n",
    "    # lines = text.split('\\n')\n",
    "    # for i,line in enumerate(lines):\n",
    "        \n",
    "    # lines[i] = lines[i].replace('  ', ' ')\n",
    "    # Fix spacing around punctuation\n",
    "    text = re.sub(r'\\s+([.,!?])', r'\\1', text)\n",
    "    text = re.sub(r'([.,!?])\\s+', r'\\1 ', text)\n",
    "    \n",
    "    # Fix contractions\n",
    "    text = re.sub(r'\\s*\\'\\s*s\\b', \"'s\", text)\n",
    "    text = re.sub(r'\\s*n\\s*\\'\\s*t\\b', \"n't\", text)\n",
    "    text = re.sub(r'\\s*\\'\\s*ve\\b', \"'ve\", text)\n",
    "    text = re.sub(r'\\s*\\'\\s*re\\b', \"'re\", text)\n",
    "    text = re.sub(r'\\s*\\'\\s*ll\\b', \"'ll\", text)\n",
    "    text = re.sub(r'\\s*\\'\\s*d\\b', \"'d\", text)\n",
    "    text = re.sub(r'\\s*\\'\\s*m\\b', \"'m\", text)\n",
    "    \n",
    "    # Fix spaces around parentheses\n",
    "    text = re.sub(r'\\(\\s+', '(', text)\n",
    "    text = re.sub(r'\\s+\\)', ')', text)\n",
    "    \n",
    "    # Remove spaces before and after text\n",
    "    text = text.strip()\n",
    "    text = text.replace('agent 0: ','')\n",
    "    text = text.replace('agent 1: ','')\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all json and jsonl files in data directory\n",
    "json_files = glob.glob('../data/modified_data/ner/*.json*', recursive=True)\n",
    "\n",
    "# Process each file\n",
    "for file_path in json_files:\n",
    "    print(f\"Processing: {file_path}\")\n",
    "    \n",
    "    try:\n",
    "        # Read data based on file extension\n",
    "        with open(file_path, 'r', encoding='utf-8', errors='replace') as f:\n",
    "            if file_path.endswith('.jsonl'):\n",
    "                # For jsonl files, read line by line and convert to json\n",
    "                data = []\n",
    "                for line in f:\n",
    "                    try:\n",
    "                        # Handle escaped characters by using raw string\n",
    "                        line = line.replace('\\\\', '\\\\\\\\')\n",
    "                        data.append(json.loads(line))\n",
    "                    except json.JSONDecodeError as e:\n",
    "                        print(f\"Error processing line in {file_path}: {e}\")\n",
    "                        continue\n",
    "            else:\n",
    "                # For json files, load entire file\n",
    "                content = f.read()\n",
    "                content = content.replace('\\\\', '\\\\\\\\')\n",
    "                data = json.loads(content)\n",
    "        \n",
    "        # ... rest of the processing code ...\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {e}\")\n",
    "        continue    # Process all string values recursively\n",
    "    def process_strings(obj):\n",
    "        if isinstance(obj, str):\n",
    "            return remove_space(obj)\n",
    "        elif isinstance(obj, list):\n",
    "            return [process_strings(item) for item in obj]\n",
    "        elif isinstance(obj, dict):\n",
    "            return {k: process_strings(v) for k, v in obj.items()}\n",
    "        return obj\n",
    "    \n",
    "    processed_data = process_strings(data)\n",
    "    # Save processed data back to file as json\n",
    "    output_path = file_path.replace('.jsonl', '.json')\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(processed_data, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all json and jsonl files in data directory\n",
    "json_files = glob.glob('../data/modified_data/dialogue/*.json', recursive=True)\n",
    "\n",
    "# Process each file\n",
    "for file_path in json_files:\n",
    "    print(f\"Processing: {file_path}\")\n",
    "    \n",
    "    # try:\n",
    "    #     # Read data based on file extension\n",
    "    #     with open(file_path, 'r') as f:\n",
    "    #         if file_path.endswith('.json'):\n",
    "    #             # For jsonl files, read line by line and convert to json\n",
    "    #             data = []\n",
    "    #             for line in f:\n",
    "    #                 try:\n",
    "    #                     # Handle escaped characters by using raw string\n",
    "    #                     line = line.replace('\\\\', '\\\\\\\\')\n",
    "    #                     data.append(json.loads(line))\n",
    "    #                 except json.JSONDecodeError as e:\n",
    "    #                     print(f\"Error processing line in {file_path}: {e}\")\n",
    "    #                     continue\n",
    "    #         else:\n",
    "    #             # For json files, load entire file\n",
    "    #             content = f.read()\n",
    "    #             content = content.replace('\\\\', '\\\\\\\\')\n",
    "    #             data = json.loads(content)\n",
    "            \n",
    "        # ... rest of the processing code ...\n",
    "    # except Exception as e:\n",
    "    #     print(f\"Error processing file {file_path}: {e}\")\n",
    "    #     continue    # Process all string values recursively\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {e}\")\n",
    "        continue    # Process all string values recursively\n",
    "\n",
    "    def process_strings(obj):\n",
    "        if isinstance(obj, str):\n",
    "            return remove_space(obj)\n",
    "        elif isinstance(obj, list):\n",
    "            return [process_strings(item) for item in obj]\n",
    "        elif isinstance(obj, dict):\n",
    "            return {k: process_strings(v) for k, v in obj.items()}\n",
    "        return obj\n",
    "    \n",
    "    processed_data = process_strings(data)\n",
    "    \n",
    "    # Save processed data back to file as json\n",
    "    output_path = file_path.replace('.jsonl', '.json')\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(processed_data, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_files = glob.glob('../data/modified_data/ner/*.json*', recursive=True)\n",
    "\n",
    "# Process each file\n",
    "for file_path in json_files:\n",
    "    print(f\"Processing: {file_path}\")\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    new_data = []\n",
    "    flag = False\n",
    "    for sample in data:\n",
    "        if sample.get('Label') != None:\n",
    "            flag = True\n",
    "            label = sample['Label']\n",
    "            new_label = []\n",
    "            for pair in label:\n",
    "                for key, value in pair.items():\n",
    "                    item = {'text': key, 'value': value}\n",
    "                    new_label.append(item)\n",
    "            sample['label'] = new_label\n",
    "            # print(sample['label'])\n",
    "            del sample['Label']\n",
    "            new_data.append(sample)\n",
    "    if flag == True:\n",
    "        print('finished')\n",
    "        print(new_data[0])\n",
    "        with open(file_path, 'w') as f:\n",
    "            json.dump(new_data, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "import glob\n",
    "# import os\n",
    "import random\n",
    "\n",
    "# Load all JSON files from the specified directory\n",
    "json_files = glob.glob('../data/modified_data/dialogue/*_100.json', recursive=True)\n",
    "\n",
    "# Initialize a set to track unique modified_text entries\n",
    "\n",
    "# Process each file\n",
    "for file_path in json_files:\n",
    "    print(file_path)\n",
    "    if 'negation' not in file_path and 'grammatical_role' not in file_path:\n",
    "        continue\n",
    "    print(f\"Loading: {file_path}\")\n",
    "    unique_modified_texts = set()\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    new_data = []  # Reset new_data for each file\n",
    "    for sample in data:\n",
    "        if isinstance(sample, list):\n",
    "            modified_text = sample[1].get('modified_text')\n",
    "            modified_label = sample[1].get('modified_label')\n",
    "        else:\n",
    "            modified_text = sample.get('modified_text')\n",
    "            modified_label = sample.get('modified_label')\n",
    "        \n",
    "        if modified_label and modified_text and modified_text not in unique_modified_texts:\n",
    "            unique_modified_texts.add(modified_text)\n",
    "            new_data.append(sample)\n",
    "\n",
    "    # Sample more data if we have less than 100 entries\n",
    "    if len(new_data) < 100:\n",
    "        # Load the original data again without the _100 suffix\n",
    "        original_file_path = file_path.replace('_100', '')\n",
    "        if 'capitalization' in original_file_path or 'punctuation' in original_file_path or 'typo' in original_file_path:\n",
    "            continue\n",
    "        with open(original_file_path, 'r') as f:\n",
    "            original_data = json.load(f)\n",
    "        \n",
    "        # Filter out already included modified_texts\n",
    "        if isinstance(sample, list):\n",
    "            additional_samples = [s for s in original_data if s[1].get('modified_text') not in unique_modified_texts and s[1].get('modified_label') != 2]\n",
    "        else:\n",
    "            additional_samples = [s for s in original_data if s.get('modified_text') not in unique_modified_texts and s.get('modified_label') != 2]\n",
    "\n",
    "        random.shuffle(additional_samples)  # Shuffle to get random samples\n",
    "\n",
    "        # Add samples until we reach 100\n",
    "        for sample in additional_samples:\n",
    "            if len(new_data) >= 100:\n",
    "                break\n",
    "            new_data.append(sample)\n",
    "            if isinstance(sample, list):\n",
    "                unique_modified_texts.add(sample[1].get('modified_text'))\n",
    "            else:\n",
    "                unique_modified_texts.add(sample.get('modified_text'))\n",
    "\n",
    "    # Save the deduplicated data back to a new JSON file for each original file\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(new_data, f, indent=2)\n",
    "\n",
    "    print(f\"Preserve: {len(new_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the active to passive data\n",
    "active_to_passive_file_path = '../data/modified_data/coref/active_to_passive.json'\n",
    "with open(active_to_passive_file_path, 'r') as f:\n",
    "    active_to_passive_data = json.load(f)\n",
    "\n",
    "# Replace modified_candidates with original_candidates\n",
    "for sample in active_to_passive_data:\n",
    "    sample['modified_candidates'] = sample['original_candidates']\n",
    "\n",
    "# Save the modified data back to the same file or a new file\n",
    "output_active_to_passive_path = '../data/modified_data/coref/active_to_passive.json'\n",
    "with open(output_active_to_passive_path, 'w') as f:\n",
    "    json.dump(active_to_passive_data, f, indent=2)\n",
    "\n",
    "print(f\"Modified active to passive data saved to: {output_active_to_passive_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all JSON files in the specified directory\n",
    "json_files = glob.glob('../data/modified_data/coref/*.json')\n",
    "\n",
    "# Define the files to exclude\n",
    "exclude_files = [\n",
    "    'capitalization.json',\n",
    "    'punctuation.json',\n",
    "    'typo_bias.json',\n",
    "    'grammatical_role.json',\n",
    "    'negation.json'\n",
    "]\n",
    "\n",
    "# Process each JSON file\n",
    "for file_path in json_files:\n",
    "    if os.path.basename(file_path) in exclude_files:\n",
    "        continue\n",
    "\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Replace modified_pronoun with original_pronoun\n",
    "    for sample in data:\n",
    "        sample['modified_pronoun'] = sample['original_pronoun']\n",
    "\n",
    "    # Save the modified data back to the same file\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "\n",
    "    print(f\"Modified pronouns in: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Initialize counters\n",
    "negation_differences = 0\n",
    "grammatical_role_differences = 0\n",
    "\n",
    "# Load all JSON files in the specified directory\n",
    "json_files = os.listdir('../data/modified_data/coref/')\n",
    "\n",
    "# Process each JSON file\n",
    "for file_name in json_files:\n",
    "    if file_name in ['negation.json', 'grammatical_role.json']:\n",
    "        with open(f'../data/modified_data/coref/{file_name}', 'r') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        # Count differences in modified_label and original_label, ignoring label 2\n",
    "        for sample in data:\n",
    "            if sample['modified_label'] != sample['original_label'] and sample['modified_label'] != 2:\n",
    "                if file_name == 'negation.json':\n",
    "                    negation_differences += 1\n",
    "                elif file_name == 'grammatical_role.json':\n",
    "                    grammatical_role_differences += 1\n",
    "\n",
    "print(f\"Differences in negation: {negation_differences}\")\n",
    "print(f\"Differences in grammatical_role: {grammatical_role_differences}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the reference CSV file with indices\n",
    "reference_df = pd.read_json('../preprocessing/train_dev_test_data/coref/test.json')\n",
    "\n",
    "# Create a mapping of text to index for O(1) lookup\n",
    "text_to_idx = {row['text']: idx for idx, row in reference_df.iterrows()}\n",
    "\n",
    "# Load all JSON files in the specified directory\n",
    "json_files = os.listdir('../data/modified_data/coref/')\n",
    "\n",
    "# Process each JSON file\n",
    "for file_name in json_files:\n",
    "    file_path = f'../data/modified_data/coref/{file_name}'\n",
    "    print(file_path)\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Add index to each sample based on exact text matching\n",
    "    for sample in tqdm(data):\n",
    "        if sample['original_text'] in text_to_idx:\n",
    "            sample['index'] = int(text_to_idx[sample['original_text']])\n",
    "        else:\n",
    "            print(f\"Warning: No exact match found for sample in {file_name}\")\n",
    "            \n",
    "    # Save the modified data back to the same file\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "\n",
    "    print(f\"Added indices to: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the reference CSV file with indices for sentiment analysis\n",
    "reference_df_sentiment = pd.read_csv('./results/sa/gpt4o-0shot-sst2.csv')\n",
    "\n",
    "# Load all JSON files in the sentiment analysis directory\n",
    "json_files_sentiment = os.listdir('../data/modified_data/sa/')\n",
    "\n",
    "# Process each JSON file\n",
    "for file_name in json_files_sentiment:\n",
    "    file_path = f'../data/modified_data/sa/{file_name}'\n",
    "    print(file_path)\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Add index to each sample based on fuzzy matching original_text\n",
    "    for sample in tqdm(data):\n",
    "        # Find best matching row in reference CSV using fuzzy matching\n",
    "        best_match_ratio = 0\n",
    "        best_match_idx = None\n",
    "        \n",
    "        for idx, ref_text in enumerate(reference_df_sentiment['text']):\n",
    "            similarity = string_similarity(sample['original_text'], ref_text)\n",
    "            if similarity > best_match_ratio:\n",
    "                best_match_ratio = similarity\n",
    "                best_match_idx = idx\n",
    "        \n",
    "        if best_match_ratio >= 0.9:\n",
    "            sample['index'] = int(best_match_idx)\n",
    "        else:\n",
    "            print(f\"Warning: No matching text found (threshold 0.9) for sample in {file_name}\")\n",
    "            # break\n",
    "            \n",
    "    # Save the modified data back to the same file\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "\n",
    "    print(f\"Added indices to: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the reference CSV file with indices\n",
    "reference_df = pd.read_json('../preprocessing/train_dev_test_data/dialogue/test.json')\n",
    "\n",
    "# Create a mapping of text to index for O(1) lookup\n",
    "text_to_idx = {row['dialogue'][-1]: idx for idx, row in reference_df.iterrows()}\n",
    "\n",
    "# Load only the specified JSON files\n",
    "json_files = ['grammatical_role.json', 'negation.json', \n",
    "              'capitalization_100.json', 'grammatical_role_100.json', 'negation_100.json', \n",
    "              'punctuation_100.json', 'typo_bias_100.json']\n",
    "\n",
    "# Process each JSON file\n",
    "for file_name in json_files:\n",
    "    file_path = f'../data/modified_data/dialogue/{file_name}'\n",
    "    print(file_path)\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Add index to each sample based on exact text matching\n",
    "    new_data = []\n",
    "    for sample_tuple in tqdm(data):\n",
    "        sample = sample_tuple  # Get the sample dict from tuple\n",
    "        if sample['original_text'] in text_to_idx:\n",
    "            idx = int(text_to_idx[sample['original_text']])\n",
    "            new_data.append([idx, sample])  # Create new tuple with index first\n",
    "        else:\n",
    "            print(f\"Warning: No exact match found for sample in {file_name}\")\n",
    "            \n",
    "    # Save the modified data back to the same file\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(new_data, f, indent=2)\n",
    "\n",
    "    print(f\"Added indices to: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test.json and get sample with index 3301\n",
    "with open('../preprocessing/train_dev_test_data/dialogue/test.json', 'r') as f:\n",
    "    test_data = json.load(f)\n",
    "    \n",
    "sample_3301 = test_data[3301]\n",
    "print(json.dumps(sample_3301, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file\n",
    "csv_path = './results/dialogue/claude-3-5-sonnet-0shot-dialogue.csv'\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Load test.json for reference\n",
    "with open('../preprocessing/train_dev_test_data/dialogue/test.json', 'r') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "# Create mapping of test dialogues to indices\n",
    "test_dialogues = {sample['dialogue'][-1]: idx for idx, sample in enumerate(test_data)}\n",
    "\n",
    "# Function to find best match using difflib\n",
    "def find_best_match(text, candidates):\n",
    "    best_ratio = 0\n",
    "    best_match = None\n",
    "    for candidate, idx in candidates.items():\n",
    "        ratio = difflib.SequenceMatcher(None, text, candidate).ratio()\n",
    "        if ratio > best_ratio:\n",
    "            best_ratio = ratio\n",
    "            best_match = (idx, ratio)\n",
    "    return best_match\n",
    "\n",
    "# Add index column based on matching\n",
    "indices = []\n",
    "for _, row in tqdm(df.iterrows()):\n",
    "    dialog = row['dialog'].split('\\n')[-1].replace('agent 0: ', '').replace('agent 1: ', '')  # Get last message\n",
    "    match = find_best_match(dialog, test_dialogues)\n",
    "    if match and match[1] > 0.9:  # Only keep matches with ratio > 0.9\n",
    "        indices.append(match[0])\n",
    "    else:\n",
    "        print(f\"No match found for {dialog}\")\n",
    "        indices.append(-1)  # Use -1 for no match found\n",
    "\n",
    "# Add indices column to dataframe\n",
    "df['index'] = indices\n",
    "\n",
    "# Save updated CSV\n",
    "df.to_csv(csv_path, index=False)\n",
    "\n",
    "print(f\"Added indices to {csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import ast\n",
    "\n",
    "# Get specific json files\n",
    "json_dir = '../data/modified_data/ner'\n",
    "target_files = ['coordinating_conjunction_100.json', 'grammatical_role_100.json', 'geographical_bias_100.json']\n",
    "\n",
    "# Process each target file\n",
    "for filename in target_files:\n",
    "    filepath = os.path.join(json_dir, filename)\n",
    "    \n",
    "    # Load the JSON data\n",
    "    with open(filepath, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Process each item in the data\n",
    "    for item in tqdm(data):\n",
    "        # Convert label fields if they exist\n",
    "        for field in ['label', 'modified_label', 'original_label']:\n",
    "            if field in item and isinstance(item[field], list):\n",
    "                # Handle original_label and modified_label format conversion\n",
    "                if field in ['original_label', 'modified_label']:\n",
    "                    # Convert list of list format to single list\n",
    "                    if isinstance(item[field], list) and len(item[field]) > 0 and isinstance(item[field][0], list):\n",
    "                        item[field] = item[field][0]\n",
    "                        continue\n",
    "\n",
    "                # Handle list of strings containing JSON\n",
    "                for i, label in enumerate(item[field]):\n",
    "                    if isinstance(label, str):\n",
    "                        try:\n",
    "                            # Handle both single dict strings and list of dict strings\n",
    "                            if label.startswith('['):\n",
    "                                # Parse list of dicts\n",
    "                                # First try to parse as a Python literal\n",
    "                                try:\n",
    "                                    label_list = ast.literal_eval(label)\n",
    "                                except:\n",
    "                                    # If that fails, try cleaning and parsing each item individually\n",
    "                                    label = label.strip('[]')\n",
    "                                    label_list = [l.strip() for l in label.split(',')]\n",
    "                                \n",
    "                                # Handle each dict in the list separately\n",
    "                                parsed_labels = []\n",
    "                                for l in label_list:\n",
    "                                    try:\n",
    "                                        # Try parsing as Python literal first\n",
    "                                        parsed = ast.literal_eval(str(l))\n",
    "                                        parsed_labels.append(parsed)\n",
    "                                    except:\n",
    "                                        try:\n",
    "                                            # Try JSON parsing with quote replacement\n",
    "                                            cleaned = str(l).replace(\"'\", '\"').replace('\\\\\"', '\"')\n",
    "                                            parsed = json.loads(cleaned)\n",
    "                                            parsed_labels.append(parsed)\n",
    "                                        except:\n",
    "                                            # Last resort - manual string cleaning\n",
    "                                            cleaned = str(l).replace(\"\\\\\", \"\").replace(\"'\", '\"')\n",
    "                                            cleaned = cleaned.replace('\"{', '{').replace('}\"', '}')\n",
    "                                            parsed = json.loads(cleaned)\n",
    "                                            parsed_labels.append(parsed)\n",
    "                                            \n",
    "                                item[field][i] = parsed_labels\n",
    "                            else:\n",
    "                                # Parse single dict\n",
    "                                try:\n",
    "                                    item[field][i] = ast.literal_eval(label)\n",
    "                                except:\n",
    "                                    item[field][i] = json.loads(label.replace(\"'\", '\"'))\n",
    "                        except Exception as e:\n",
    "                            print(f\"Failed to convert {field}[{i}] in file {filename}\")\n",
    "                            print(f\"Value: {label}\")\n",
    "                            print(f\"Error: {str(e)}\")\n",
    "                            continue\n",
    "                \n",
    "    # Save the updated data back to file\n",
    "    with open(filepath, 'w') as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "        \n",
    "print(\"Completed processing target JSON files\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the reference CSV file with indices\n",
    "reference_df = pd.read_json('../preprocessing/train_dev_test_data/ner/fewnerd_sample_test.json')\n",
    "\n",
    "# Create a mapping of text to index for O(1) lookup\n",
    "text_to_idx = {row['text']: idx for idx, row in reference_df.iterrows()}\n",
    "\n",
    "# Load all JSON files in the specified directory\n",
    "json_files = [f for f in os.listdir('../data/modified_data/ner/') if f.endswith('.json')]\n",
    "# Filter to only process specific files\n",
    "# target_files = ['grammatical_role_100.json', 'coordinating_conjunction_100.json', 'geographical_bias_100.json']\n",
    "# target_files = ['derivation.json']\n",
    "\n",
    "# json_files = [f for f in json_files if f in target_files]\n",
    "\n",
    "# Process each JSON file\n",
    "for file_name in json_files:\n",
    "    file_path = f'../data/modified_data/ner/{file_name}'\n",
    "    print(file_path)\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Add index to each sample based on exact text matching\n",
    "    for sample in tqdm(data):\n",
    "        # Find best match using difflib ratio with threshold 0.9\n",
    "        if 'index' in sample and 'original_text' in sample and 'original_label' in sample:\n",
    "            continue\n",
    "        if 'original_text' not in sample:\n",
    "            sample['original_text'] = sample['text']\n",
    "        if 'original_label' not in sample:\n",
    "            sample['original_label'] = sample['label']\n",
    "        best_match = None\n",
    "        best_ratio = 0\n",
    "        for ref_text in text_to_idx:\n",
    "            # original_text = sample['original_text'] if 'original_text' in sample else sample['text']\n",
    "            \n",
    "            ratio = difflib.SequenceMatcher(None, sample['original_text'], ref_text).ratio()\n",
    "            if ratio > 0.9 and ratio > best_ratio:\n",
    "                best_ratio = ratio\n",
    "                best_match = ref_text\n",
    "                \n",
    "        if best_match:\n",
    "            sample['index'] = int(text_to_idx[best_match])\n",
    "        else:\n",
    "            print(sample['original_text'])\n",
    "            print(f\"Warning: No exact match found for sample in {file_name}\")\n",
    "            \n",
    "    # Save the modified data back to the same file\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "\n",
    "    print(f\"Added indices to: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import difflib\n",
    "\n",
    "# Load the reference data with original labels\n",
    "reference_df = pd.read_json('../preprocessing/train_dev_test_data/ner/fewnerd_sample_test.json')\n",
    "\n",
    "# Create mapping of text to original labels\n",
    "text_to_labels = {row['text']: row['label'] for _, row in reference_df.iterrows()}\n",
    "\n",
    "# Process each JSON file\n",
    "json_files = [f for f in os.listdir('../preprocessing/data_for_phase2/ner/') if f.endswith('.json')]\n",
    "# Filter to only include derivation.json\n",
    "json_files = ['derivation.json']\n",
    "\n",
    "for file_name in json_files:\n",
    "    file_path = f'../preprocessing/data_for_phase2/ner/{file_name}'\n",
    "    \n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Add original labels to each sample based on text matching\n",
    "    for sample in tqdm(data):\n",
    "        # Find best match using difflib ratio with threshold 0.9\n",
    "        best_match = None\n",
    "        best_ratio = 0\n",
    "        for ref_text in text_to_labels:\n",
    "            ratio = difflib.SequenceMatcher(None, sample['original_text'], ref_text).ratio()\n",
    "            if ratio > 0.9 and ratio > best_ratio:\n",
    "                best_ratio = ratio\n",
    "                best_match = ref_text\n",
    "                \n",
    "        if best_match:\n",
    "            sample['original_label'] = text_to_labels[best_match]\n",
    "        else:\n",
    "            print(sample['original_text'])\n",
    "            print(f\"Warning: No match found for sample in {file_name}\")\n",
    "            \n",
    "    # Save the modified data back to the same file\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "\n",
    "    print(f\"Added original labels to: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the comparison CSV\n",
    "comparison_df = pd.read_csv('/Users/hungcoreft/workspace/ood-modification/preprocessing/scripts/grammatical_role_coref_comparison.csv')\n",
    "\n",
    "# Drop rows with Keep == 0 and drop Changes column\n",
    "filtered_df = comparison_df[comparison_df['Keep'].isin([1, np.nan])].drop('Changes', axis=1)\n",
    "print(len(filtered_df))\n",
    "# Load the original JSON file\n",
    "json_path = '../data/modified_data/coref/grammatical_role_100.json'\n",
    "with open(json_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Create mapping of indices to data entries\n",
    "data_dict = {item['index']: item for item in data}\n",
    "\n",
    "# Update data based on filtered rows\n",
    "for _, row in filtered_df.iterrows():\n",
    "    # print(row)\n",
    "    idx = row['index']\n",
    "    if idx in data_dict:\n",
    "        # data_dict[idx]['original_text'] = row['original_text']\n",
    "        # data_dict[idx]['modified_text'] = row['modified_text']\n",
    "        # data_dict[idx]['original_label'] = row['original_label']\n",
    "        data_dict[idx]['modified_label'] = row['modified_label']\n",
    "        data_dict[idx]['modified_pronoun'] = row['modified_pronoun']\n",
    "        data_dict[idx]['modified_candidates'] = ast.literal_eval(row['modified_candidates'])\n",
    "\n",
    "# Convert back to list and maintain only filtered indices\n",
    "filtered_data = [data_dict[idx] for idx in filtered_df['index']]\n",
    "\n",
    "# Save filtered data back to JSON\n",
    "with open(json_path, 'w') as f:\n",
    "    json.dump(filtered_data, f, indent=2)\n",
    "\n",
    "print(f\"Filtered and saved {len(filtered_data)} entries to {json_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the comparison CSV\n",
    "comparison_df = pd.read_csv('/Users/hungcoreft/workspace/ood-modification/preprocessing/scripts/grammatical_role_ner_comparison.csv')\n",
    "filtered_df = comparison_df\n",
    "# Drop rows with Keep == 0 and drop Changes column\n",
    "# filtered_df = comparison_df[comparison_df['Keep'].isin([1, np.nan])].drop('Changes', axis=1)\n",
    "print(len(filtered_df))\n",
    "# Load the original JSON file\n",
    "json_path = '../data/modified_data/ner/grammatical_role_100.json'\n",
    "with open(json_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Create mapping of indices to data entries\n",
    "data_dict = {item['index']: item for item in data}\n",
    "\n",
    "# Update data based on filtered rows\n",
    "for _, row in filtered_df.iterrows():\n",
    "    # print(row)\n",
    "    idx = row['index']\n",
    "    if idx in data_dict:\n",
    "        # data_dict[idx]['original_text'] = row['original_text']\n",
    "        # data_dict[idx]['modified_text'] = row['modified_text']\n",
    "        # data_dict[idx]['original_label'] = row['original_label']\n",
    "        modified_label = ast.literal_eval(row['modified_label'])\n",
    "        data_dict[idx]['modified_label'] = modified_label\n",
    "        # data_dict[idx]['modified_pronoun'] = row['modified_pronoun']\n",
    "        # data_dict[idx]['modified_candidates'] = row['modified_candidates']\n",
    "\n",
    "# Convert back to list and maintain only filtered indices\n",
    "filtered_data = [data_dict[idx] for idx in filtered_df['index']]\n",
    "\n",
    "# Save filtered data back to JSON\n",
    "with open(json_path, 'w') as f:\n",
    "    json.dump(filtered_data, f, indent=2)\n",
    "\n",
    "print(f\"Filtered and saved {len(filtered_data)} entries to {json_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the comparison CSV\n",
    "comparison_df = pd.read_csv('./derivation_coref.csv')\n",
    "\n",
    "# Drop rows with Keep == 0 and drop Changes column\n",
    "filtered_df = comparison_df[comparison_df['Keep'].isin(['Yes', np.nan])].drop('Changes', axis=1)\n",
    "print(len(filtered_df))\n",
    "# Load the original JSON file\n",
    "json_path = '../data/modified_data/coref/derivation_100.json'\n",
    "with open(json_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Create mapping of indices to data entries\n",
    "data_dict = {item['index']: item for item in data}\n",
    "\n",
    "# Update data based on filtered rows\n",
    "for _, row in filtered_df.iterrows():\n",
    "    # print(row)\n",
    "    idx = row['index']\n",
    "    if idx in data_dict:\n",
    "        # data_dict[idx]['original_text'] = row['original_text']\n",
    "        data_dict[idx]['modified_text'] = row['modified_text']\n",
    "        # data_dict[idx]['original_label'] = row['original_label']\n",
    "        data_dict[idx]['modified_label'] = row['modified_label']\n",
    "        data_dict[idx]['modified_pronoun'] = row['modified_pronoun']\n",
    "        data_dict[idx]['modified_candidates'] = ast.literal_eval(row['modified_candidates'])\n",
    "\n",
    "# Convert back to list and maintain only filtered indices\n",
    "filtered_data = [data_dict[idx] for idx in filtered_df['index']]\n",
    "\n",
    "# Save filtered data back to JSON\n",
    "with open(json_path, 'w') as f:\n",
    "    json.dump(filtered_data, f, indent=2)\n",
    "\n",
    "print(f\"Filtered and saved {len(filtered_data)} entries to {json_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the comparison CSV\n",
    "comparison_df = pd.read_csv('./derivation_sentiment.csv')\n",
    "\n",
    "# Drop rows with Keep == 0 and drop Changes column\n",
    "filtered_df = comparison_df[comparison_df['Keep'].isin(['Yes', np.nan])].drop('Changes', axis=1)\n",
    "print(len(filtered_df))\n",
    "# Load the original JSON file\n",
    "json_path = '../data/modified_data/sa/derivation_100.json'\n",
    "with open(json_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Create mapping of indices to data entries\n",
    "data_dict = {item['index']: item for item in data}\n",
    "\n",
    "# Update data based on filtered rows\n",
    "for _, row in filtered_df.iterrows():\n",
    "    # print(row)\n",
    "    idx = row['index']\n",
    "    if idx in data_dict:\n",
    "        # data_dict[idx]['original_text'] = row['original_text']\n",
    "        data_dict[idx]['modified_text'] = row['modified_text']\n",
    "        # data_dict[idx]['original_label'] = row['original_label']\n",
    "        # data_dict[idx]['modified_label'] = row['modified_label']\n",
    "        # data_dict[idx]['modified_pronoun'] = row['modified_pronoun']\n",
    "        # data_dict[idx]['modified_candidates'] = row['modified_candidates']\n",
    "\n",
    "# Convert back to list and maintain only filtered indices\n",
    "filtered_data = [data_dict[idx] for idx in filtered_df['index']]\n",
    "print(filtered_data[0])\n",
    "# Save filtered data back to JSON\n",
    "with open(json_path, 'w') as f:\n",
    "    json.dump(filtered_data, f, indent=2)\n",
    "\n",
    "print(f\"Filtered and saved {len(filtered_data)} entries to {json_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the comparison CSV\n",
    "comparison_df = pd.read_csv('./derivation_ner.csv')\n",
    "\n",
    "# Drop rows with Keep == 0 and drop Changes column\n",
    "filtered_df = comparison_df[comparison_df['Keep'].isin(['Yes', np.nan])].drop('Changes', axis=1)\n",
    "print(len(filtered_df))\n",
    "# Load the original JSON file\n",
    "\n",
    "json_path = '../preprocessing/train_dev_test_data/ner/fewnerd_sample_test.json'\n",
    "with open(json_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Create mapping of indices to data entries\n",
    "data_dict = {i: item for i, item in enumerate(data)}\n",
    "\n",
    "\n",
    "# Update data based on filtered rows\n",
    "for _, row in filtered_df.iterrows():\n",
    "    # print(row)\n",
    "    idx = row['index']\n",
    "    if idx in data_dict:\n",
    "        # data_dict[idx]['original_text'] = row['original_text']\n",
    "        data_dict[idx]['modified_text'] = row['modified_text']\n",
    "        # data_dict[idx]['original_label'] = row['original_label']\n",
    "        modified_label = ast.literal_eval(row['modified_label'])\n",
    "        data_dict[idx]['modified_label'] = modified_label\n",
    "        # data_dict[idx]['modified_pronoun'] = row['modified_pronoun']\n",
    "        # data_dict[idx]['modified_candidates'] = row['modified_candidates']\n",
    "\n",
    "# Convert back to list and maintain only filtered indices\n",
    "filtered_data = [data_dict[idx] for idx in filtered_df['index']]\n",
    "print(filtered_data[1])\n",
    "# Save filtered data back to JSON\n",
    "json_path = '../data/modified_data/ner/derivation_100.json'\n",
    "with open(json_path, 'w') as f:\n",
    "    json.dump(filtered_data, f, indent=2)\n",
    "\n",
    "print(f\"Filtered and saved {len(filtered_data)} entries to {json_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the comparison CSV\n",
    "comparison_df = pd.read_csv('/Users/hungcoreft/workspace/ood-modification/preprocessing/scripts/grammatical_role_dialogue_comparison.csv')\n",
    "\n",
    "# Drop rows with Keep == 0 and drop Changes column\n",
    "print(len(comparison_df))\n",
    "\n",
    "filtered_df = comparison_df[comparison_df['Keep'].isin([1, np.nan])].drop('Changes', axis=1)\n",
    "print(len(filtered_df))\n",
    "# Load the original JSON file\n",
    "json_path = '../data/modified_data/dialogue/grammatical_role.json'\n",
    "with open(json_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Create mapping of indices to data entries\n",
    "data_dict = {index: item for index, item in data}\n",
    "\n",
    "# Update data based on filtered rows\n",
    "for _, row in filtered_df.iterrows():\n",
    "    # print(row)\n",
    "    idx = row['index']\n",
    "    if idx in data_dict:\n",
    "        # data_dict[idx]['original_text'] = row['original_text']\n",
    "        # data_dict[idx]['modified_text'] = row['modified_text']\n",
    "        # data_dict[idx]['original_label'] = row['original_label']\n",
    "        data_dict[idx]['modified_label'] = row['modified_label']\n",
    "        # data_dict[idx]['modified_pronoun'] = row['modified_pronoun']\n",
    "        # data_dict[idx]['modified_candidates'] = row['modified_candidates']\n",
    "\n",
    "# Convert back to list and maintain only filtered indices\n",
    "filtered_data = [[idx,data_dict[idx]] for idx in filtered_df['index']]\n",
    "# print(filtered_data[0])\n",
    "# Save filtered data back to JSON\n",
    "with open(json_path, 'w') as f:\n",
    "    json.dump(filtered_data, f, indent=2)\n",
    "\n",
    "print(f\"Filtered and saved {len(filtered_data)} entries to {json_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the comparison CSV\n",
    "comparison_df = pd.read_csv('./derivation_dialog.csv')\n",
    "\n",
    "# Drop rows with Keep == 0 and drop Changes column\n",
    "print(len(comparison_df))\n",
    "\n",
    "filtered_df = comparison_df[comparison_df['Keep'].isin([\"Yes\", np.nan])].drop('Changes', axis=1)\n",
    "print(len(filtered_df))\n",
    "# Load the original JSON file\n",
    "\n",
    "# Load test data to get dialogue indices\n",
    "test_path = '../preprocessing/train_dev_test_data/dialogue/test.json'\n",
    "with open(test_path, 'r') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "# Create mapping of last turn text to index\n",
    "label_mapping = {\n",
    "    'is_contradiction': 1,\n",
    "    'no_contradiction': 0\n",
    "}\n",
    "samples = []\n",
    "for _, row in filtered_df.iterrows():\n",
    "    # print(row)\n",
    "    idx = row['index']\n",
    "    sample = test_data[idx]\n",
    "    last_turn = sample['turns'][-1]['text']\n",
    "    sample['modified_text'] = row['modified_text']\n",
    "    # samples.append(sample)\n",
    "    sample['dialog_context'] = [turn['text'] for turn in sample['turns'][:-1]]\n",
    "    sample['original_text'] = sample['turns'][-1]['text']\n",
    "    sample['label'] = label_mapping[sample['label']]\n",
    "    item = [idx, sample]\n",
    "    samples.append(item)\n",
    "# Convert back to list and maintain only filtered indices\n",
    "print(len(samples))\n",
    "print(samples[1])\n",
    "# print(filtered_data[0])\n",
    "# Save filtered data back to JSON\n",
    "json_path = '../data/modified_data/dialogue/derivation_100.json'\n",
    "with open(json_path, 'w') as f:\n",
    "    json.dump(samples, f, indent=2)\n",
    "\n",
    "print(f\"Filtered and saved {len(samples)} entries to {json_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load both JSON files\n",
    "file1_path = '../data/modified_data/ner/grammatical_role_100.json'\n",
    "file2_path = '../data/modified_data/ner/grammatical_role_100_compare.json'\n",
    "\n",
    "with open(file1_path, 'r') as f1, open(file2_path, 'r') as f2:\n",
    "    data1 = json.load(f1)\n",
    "    data2 = json.load(f2)\n",
    "\n",
    "# Create sets of modified texts from both files\n",
    "texts1 = {item['modified_text'] for item in data1}\n",
    "texts2 = {item['modified_text'] for item in data2}\n",
    "\n",
    "# Print total counts\n",
    "print(f\"Total texts in file 1: {len(texts1)}\")\n",
    "print(f\"Total texts in file 2: {len(texts2)}\")\n",
    "print(f\"Texts in both files: {len(texts1.intersection(texts2))}\")\n",
    "\n",
    "# Find texts present in only one file\n",
    "only_in_file1 = texts1 - texts2\n",
    "only_in_file2 = texts2 - texts1\n",
    "\n",
    "# Print texts present only in file 1\n",
    "print(f\"\\nTexts ONLY in file 1 ({len(only_in_file1)}):\")\n",
    "for text in only_in_file1:\n",
    "    print(f\"\\nText: {text}\")\n",
    "\n",
    "# Print texts present only in file 2  \n",
    "print(f\"\\nTexts ONLY in file 2 ({len(only_in_file2)}):\")\n",
    "for text in only_in_file2:\n",
    "    print(f\"\\nText: {text}\")\n",
    "\n",
    "# Print some examples of texts in both files\n",
    "common_texts = texts1.intersection(texts2)\n",
    "print(f\"\\nExample texts present in BOTH files (showing first 3):\")\n",
    "for text in list(common_texts)[:3]:\n",
    "    print(f\"\\nText: {text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('/Users/hungcoreft/workspace/ood-modification/preprocessing/scripts/coordinating_conjunction_ner_comparison_detailed.csv', 'r') as f:\n",
    "    csv_reader = csv.DictReader(f)\n",
    "    comparison_data = [row for row in csv_reader]\n",
    "for row in comparison_data:\n",
    "    row['index'] = row['index']\n",
    "    row['original_text'] = row['original_text']\n",
    "    row['modified_text'] = row['modified_text']\n",
    "    row['modified_label'] = ast.literal_eval(row['modified_label'])\n",
    "    row['original_label'] = ast.literal_eval(row['original_label'])\n",
    "json.dump(comparison_data, open('coordinating_conjunction_ner_compare.json', 'w'), indent=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1_path = '../data/modified_data/ner/coordinating_conjunction_100.json'\n",
    "file2_path = '../data/modified_data/ner/coordinating_conjunction_100 copy.json'\n",
    "\n",
    "with open(file1_path, 'r') as f1, open(file2_path, 'r') as f2:\n",
    "    data1 = json.load(f1)\n",
    "    data2 = json.load(f2)\n",
    "\n",
    "# Create sets of modified texts from both files\n",
    "texts1 = {item['modified_text'] for item in data1}\n",
    "texts2 = {item['modified_text'] for item in data2}\n",
    "\n",
    "# Print total counts\n",
    "print(f\"Total texts in file 1: {len(texts1)}\")\n",
    "print(f\"Total texts in file 2: {len(texts2)}\")\n",
    "print(f\"Texts in both files: {len(texts1.intersection(texts2))}\")\n",
    "\n",
    "# Find texts present in only one file\n",
    "only_in_file1 = texts1 - texts2\n",
    "only_in_file2 = texts2 - texts1\n",
    "\n",
    "# Print texts present only in file 1\n",
    "print(f\"\\nTexts ONLY in file 1 ({len(only_in_file1)}):\")\n",
    "for text in only_in_file1:\n",
    "    print(f\"\\nText: {text}\")\n",
    "\n",
    "# Print texts present only in file 2  \n",
    "print(f\"\\nTexts ONLY in file 2 ({len(only_in_file2)}):\")\n",
    "for text in only_in_file2:\n",
    "    print(f\"\\nText: {text}\")\n",
    "\n",
    "# Print some examples of texts in both files\n",
    "common_texts = texts1.intersection(texts2)\n",
    "print(f\"\\nExample texts present in BOTH files (showing first 3):\")\n",
    "for text in list(common_texts)[:3]:\n",
    "    print(f\"\\nText: {text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read both CSV files\n",
    "old_file = pd.read_csv('/Users/hungcoreft/workspace/ood-modification/preprocessing/scripts/derivation_old_ner_comparison_detailed.csv')\n",
    "new_file = pd.read_csv('/Users/hungcoreft/workspace/ood-modification/preprocessing/scripts/derivation_ner_comparison_detailed.csv')\n",
    "\n",
    "print(len(old_file))\n",
    "print(len(new_file))\n",
    "# Create sets of modified texts from both files\n",
    "old_texts = set(old_file['modified_text'])\n",
    "new_texts = set(new_file['modified_text'])\n",
    "\n",
    "print(len(old_texts))\n",
    "print(len(new_texts))\n",
    "# Find texts present only in old file\n",
    "only_in_old = old_texts - new_texts\n",
    "\n",
    "# Get rows that are only in old file\n",
    "samples_df = old_file[old_file['modified_text'].isin(only_in_old)]\n",
    "\n",
    "# Save to CSV\n",
    "samples_df.to_csv('derivation_ner_comparison_samples.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "# Read the existing data from data_after_phase2\n",
    "with open('../data/modified_data/dialogue/derivation.json', 'r') as f:\n",
    "    existing_data = json.load(f)\n",
    "\n",
    "existing_data = [sample[1] for sample in existing_data]\n",
    "# Create set of existing modified texts\n",
    "existing_texts = set(item['modified_text'] for item in existing_data)\n",
    "\n",
    "# Read the filtered data\n",
    "with open('../preprocessing/filtered_data/dialogue/derivation.json', 'r') as f:\n",
    "    filtered_data = json.load(f)\n",
    "\n",
    "print(filtered_data[0])\n",
    "# Get items from filtered data that aren't in existing data\n",
    "new_items = [item for item in filtered_data if item['modified_text'] not in existing_texts]\n",
    "\n",
    "# Sample 50 items randomly\n",
    "# Sample 50 items randomly\n",
    "sampled_items = random.sample(new_items, min(100, len(new_items)))\n",
    "\n",
    "# Load test data to get indices\n",
    "with open('../preprocessing/train_dev_test_data/dialogue/test.json', 'r') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "# Create mapping of test indices\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "# Create mapping of test indices using fuzzy matching\n",
    "test_indices = {item['dialogue'][-1]: i for i, item in enumerate(test_data)}\n",
    "\n",
    "# Map sampled items to test indices using fuzzy matching\n",
    "matched_items = []\n",
    "for item in sampled_items:\n",
    "    for test_text, idx in test_indices.items():\n",
    "        if SequenceMatcher(None, item['modified_text'], test_text).ratio() > 0.9:\n",
    "            item['index'] = idx\n",
    "            matched_items.append(item)\n",
    "            break\n",
    "\n",
    "sampled_items = matched_items[:50]\n",
    "# Save sampled items to a new JSON file\n",
    "# Find differences between original and modified text\n",
    "# Convert sampled items to DataFrame\n",
    "df = pd.DataFrame(sampled_items)\n",
    "\n",
    "# Add changes column by comparing original and modified text\n",
    "changes = []\n",
    "for _, row in df.iterrows():\n",
    "    original = row['original_text'].split()\n",
    "    modified = row['modified_text'].split()\n",
    "    \n",
    "    item_changes = []\n",
    "    for o, m in zip(original, modified):\n",
    "        if o != m:\n",
    "            item_changes.append(f\"{o} -> {m}\")\n",
    "    \n",
    "    changes.append(', '.join(item_changes))\n",
    "\n",
    "df['Changes'] = changes\n",
    "\n",
    "# Keep only desired columns\n",
    "df = df[['index', 'original_text', 'modified_text', 'Changes']]\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv('derivation_samples_dialogue.csv', index=False)\n",
    "print(f\"Sampled {len(sampled_items)} new items\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_drops_and_significance(row):\n",
    "    colors = [''] * len(row)\n",
    "    if row['original_res'] > row['modified_res']:\n",
    "        colors = ['background-color: red'] * len(row)\n",
    "        # If p-value < 0.05, add bold text\n",
    "        if 'p_value' in row and row['p_value'] is not None and row['p_value'] < 0.05:\n",
    "            colors = ['background-color: red; font-weight: bold'] * len(row)\n",
    "    return colors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the detailed results for negation for each model\n",
    "sonnet_detailed = pd.read_csv('results/dialogue/claude-3-5-sonnet-0shot-negation_100.csv')\n",
    "gpt4_detailed = pd.read_csv('results/dialogue/gpt4o-0shot-negation_100.csv')\n",
    "llama_detailed = pd.read_csv('results/dialogue/llama-0shot-negation_100.csv')\n",
    "\n",
    "# Calculate accuracy for original and modified examples by type for each model\n",
    "type_results = []\n",
    "for type_name in sonnet_detailed['type'].unique():\n",
    "    # Get data for each model for this type\n",
    "    sonnet_data = sonnet_detailed[sonnet_detailed['type'] == type_name]\n",
    "    gpt4_data = gpt4_detailed[gpt4_detailed['type'] == type_name]\n",
    "    llama_data = llama_detailed[llama_detailed['type'] == type_name]\n",
    "    \n",
    "    # Calculate accuracies for each model\n",
    "    models_original_acc = []\n",
    "    models_modified_acc = []\n",
    "    \n",
    "    for data in [sonnet_data, gpt4_data, llama_data]:\n",
    "        original_acc = (data['original_pred'] == data['original_label']).sum() / len(data)\n",
    "        modified_acc = (data['modified_pred'] == data['modified_label']).sum() / len(data)\n",
    "        models_original_acc.append(original_acc)\n",
    "        models_modified_acc.append(modified_acc)\n",
    "    \n",
    "    # Average across models\n",
    "    avg_original_acc = sum(models_original_acc) / len(models_original_acc)\n",
    "    avg_modified_acc = sum(models_modified_acc) / len(models_modified_acc)\n",
    "    \n",
    "    # Calculate differences\n",
    "    difference = -(avg_original_acc - avg_modified_acc)\n",
    "    pct_difference = -round((avg_original_acc - avg_modified_acc) / avg_original_acc * 100, 2)\n",
    "\n",
    "    # Perform t-test on combined results\n",
    "    all_original_correct = []\n",
    "    all_modified_correct = []\n",
    "    for data in [sonnet_data, gpt4_data, llama_data]:\n",
    "        all_original_correct.extend((data['original_pred'] == data['original_label']).astype(int))\n",
    "        all_modified_correct.extend((data['modified_pred'] == data['modified_label']).astype(int))\n",
    "    \n",
    "    _, p_value = stats.ttest_rel(all_original_correct, all_modified_correct)\n",
    "    \n",
    "    type_results.append({\n",
    "        'type': type_name,\n",
    "        'num_samples': len(sonnet_data),  # Total samples across all models\n",
    "        'original_res': round(avg_original_acc * 100, 2),\n",
    "        'modified_res': round(avg_modified_acc * 100, 2),\n",
    "        'difference': round(difference * 100, 2),\n",
    "        'pct_difference': round(pct_difference, 2),\n",
    "        'p_value': p_value\n",
    "    })\n",
    "\n",
    "# Create and display type-based results dataframe\n",
    "type_results_df = pd.DataFrame(type_results)\n",
    "\n",
    "# Apply the same styling as before\n",
    "styled_type_results = type_results_df.round(2).style.apply(highlight_drops_and_significance, axis=1)\n",
    "styled_type_results\n",
    "type_results_df.to_csv('dialog_negation_type_results.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the detailed results for sentiment analysis for each model\n",
    "sonnet_detailed = pd.read_csv('results/sa/claude-3-5-sonnet-0shot-negation_100.csv')\n",
    "gpt4_detailed = pd.read_csv('results/sa/gpt4o-0shot-negation_100.csv')\n",
    "llama_detailed = pd.read_csv('results/sa/llama-0shot-negation_100.csv')\n",
    "\n",
    "# Calculate accuracy for original and modified examples by type for each model\n",
    "type_results = []\n",
    "for type_name in sonnet_detailed['type'].unique():\n",
    "    # Get data for each model for this type\n",
    "    sonnet_data = sonnet_detailed[sonnet_detailed['type'] == type_name]\n",
    "    gpt4_data = gpt4_detailed[gpt4_detailed['type'] == type_name]\n",
    "    llama_data = llama_detailed[llama_detailed['type'] == type_name]\n",
    "    \n",
    "    # Calculate accuracies for each model\n",
    "    models_original_acc = []\n",
    "    models_modified_acc = []\n",
    "    \n",
    "    for data in [sonnet_data, gpt4_data, llama_data]:\n",
    "        original_acc = (data['original_pred'] == data['original_label']).sum() / len(data)\n",
    "        modified_acc = (data['modified_pred'] == data['modified_label']).sum() / len(data)\n",
    "        models_original_acc.append(original_acc)\n",
    "        models_modified_acc.append(modified_acc)\n",
    "    \n",
    "    # Average across models\n",
    "    avg_original_acc = sum(models_original_acc) / len(models_original_acc)\n",
    "    avg_modified_acc = sum(models_modified_acc) / len(models_modified_acc)\n",
    "    \n",
    "    # Calculate differences\n",
    "    difference = -(avg_original_acc - avg_modified_acc)\n",
    "    pct_difference = -round((avg_original_acc - avg_modified_acc) / avg_original_acc * 100, 2)\n",
    "\n",
    "    # Perform t-test on combined results\n",
    "    all_original_correct = []\n",
    "    all_modified_correct = []\n",
    "    for data in [sonnet_data, gpt4_data, llama_data]:\n",
    "        all_original_correct.extend((data['original_pred'] == data['original_label']).astype(int))\n",
    "        all_modified_correct.extend((data['modified_pred'] == data['modified_label']).astype(int))\n",
    "    \n",
    "    _, p_value = stats.ttest_rel(all_original_correct, all_modified_correct)\n",
    "    \n",
    "    type_results.append({\n",
    "        'type': type_name,\n",
    "        'num_samples': len(sonnet_data),  # Total samples across all models\n",
    "        'original_res': round(avg_original_acc * 100, 2),\n",
    "        'modified_res': round(avg_modified_acc * 100, 2),\n",
    "        'difference': round(difference * 100, 2),\n",
    "        'pct_difference': round(pct_difference, 2),\n",
    "        'p_value': p_value\n",
    "    })\n",
    "\n",
    "# Create and display type-based results dataframe\n",
    "type_results_df = pd.DataFrame(type_results)\n",
    "\n",
    "# Apply the same styling as before\n",
    "styled_type_results = type_results_df.round(2).style.apply(highlight_drops_and_significance, axis=1)\n",
    "styled_type_results\n",
    "type_results_df.to_csv('sentiment_negation_type_results.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the detailed results for sentiment analysis for each model\n",
    "sonnet_detailed = pd.read_csv('results/coref/claude-3-5-sonnet-0shot-negation_100.csv')\n",
    "gpt4_detailed = pd.read_csv('results/coref/gpt4o-0shot-negation_100.csv')\n",
    "llama_detailed = pd.read_csv('results/coref/llama-0shot-negation_100.csv')\n",
    "\n",
    "# Calculate accuracy for original and modified examples by type for each model\n",
    "type_results = []\n",
    "for type_name in sonnet_detailed['type'].unique():\n",
    "    # Get data for each model for this type\n",
    "    sonnet_data = sonnet_detailed[sonnet_detailed['type'] == type_name]\n",
    "    gpt4_data = gpt4_detailed[gpt4_detailed['type'] == type_name]\n",
    "    llama_data = llama_detailed[llama_detailed['type'] == type_name]\n",
    "    \n",
    "    # Calculate accuracies for each model\n",
    "    models_original_acc = []\n",
    "    models_modified_acc = []\n",
    "    \n",
    "    for data in [sonnet_data, gpt4_data, llama_data]:\n",
    "        original_acc = (data['original_pred'] == data['original_label']).sum() / len(data)\n",
    "        modified_acc = (data['modified_pred'] == data['modified_label']).sum() / len(data)\n",
    "        models_original_acc.append(original_acc)\n",
    "        models_modified_acc.append(modified_acc)\n",
    "    \n",
    "    # Average across models\n",
    "    avg_original_acc = sum(models_original_acc) / len(models_original_acc)\n",
    "    avg_modified_acc = sum(models_modified_acc) / len(models_modified_acc)\n",
    "    \n",
    "    # Calculate differences\n",
    "    difference = -(avg_original_acc - avg_modified_acc)\n",
    "    pct_difference = -round((avg_original_acc - avg_modified_acc) / avg_original_acc * 100, 2)\n",
    "\n",
    "    # Perform t-test on combined results\n",
    "    all_original_correct = []\n",
    "    all_modified_correct = []\n",
    "    for data in [sonnet_data, gpt4_data, llama_data]:\n",
    "        all_original_correct.extend((data['original_pred'] == data['original_label']).astype(int))\n",
    "        all_modified_correct.extend((data['modified_pred'] == data['modified_label']).astype(int))\n",
    "    \n",
    "    _, p_value = stats.ttest_rel(all_original_correct, all_modified_correct)\n",
    "    \n",
    "    type_results.append({\n",
    "        'type': type_name,\n",
    "        'num_samples': len(sonnet_data),  # Total samples across all models\n",
    "        'original_res': round(avg_original_acc * 100, 2),\n",
    "        'modified_res': round(avg_modified_acc * 100, 2),\n",
    "        'difference': round(difference * 100, 2),\n",
    "        'pct_difference': round(pct_difference, 2),\n",
    "        'p_value': p_value\n",
    "    })\n",
    "\n",
    "# Create and display type-based results dataframe\n",
    "type_results_df = pd.DataFrame(type_results)\n",
    "\n",
    "# Apply the same styling as before\n",
    "styled_type_results = type_results_df.round(2).style.apply(highlight_drops_and_significance, axis=1)\n",
    "styled_type_results\n",
    "type_results_df.to_csv('coreference_negation_type_results.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_f1_ent(gold_entities, predicted_entities):\n",
    "    \"\"\"\n",
    "    Calculates the F1 score given the true labels and predicted labels.\n",
    "    \"\"\"\n",
    "    # print(\"Input types:\")\n",
    "    # print(f\"gold_entities type: {type(gold_entities)}\")\n",
    "    # print(f\"predicted_entities type: {type(predicted_entities)}\")\n",
    "    \n",
    "    if predicted_entities is None:\n",
    "        return 0.0, 0.0, 0.0\n",
    "\n",
    "    true_entities = {}\n",
    "    pred_entities = {}\n",
    "    \n",
    "    # Convert to empty list if NaN\n",
    "    def handle_nan(entities):\n",
    "        # print(f\"Handling NaN for: {type(entities)}\")\n",
    "        # If it's already a list, return it as is\n",
    "        if isinstance(entities, list):\n",
    "            return entities\n",
    "        # Handle pandas/numpy types\n",
    "        if isinstance(entities, (pd.Series, np.ndarray)):\n",
    "            nan_check = pd.isna(entities)\n",
    "            if isinstance(nan_check, (pd.Series, np.ndarray)):\n",
    "                if nan_check.any():\n",
    "                    return \"[]\"\n",
    "            elif nan_check:\n",
    "                return \"[]\"\n",
    "        # Handle single values\n",
    "        elif pd.isna(entities):\n",
    "            return \"[]\"\n",
    "        return entities\n",
    "\n",
    "    # Handle NaN cases\n",
    "    gold_entities = handle_nan(gold_entities)\n",
    "    predicted_entities = handle_nan(predicted_entities)\n",
    "            \n",
    "    # Parse strings if needed\n",
    "    if isinstance(gold_entities, str):\n",
    "        gold_entities = ast.literal_eval(gold_entities)\n",
    "    if isinstance(predicted_entities, str):\n",
    "        predicted_entities = ast.literal_eval(predicted_entities)\n",
    "\n",
    "    # Process gold entities\n",
    "    # print(gold_entities)\n",
    "    for entity in gold_entities:\n",
    "        # print(entity)\n",
    "        if isinstance(entity, str):\n",
    "            entity = ast.literal_eval(entity)\n",
    "        if entity.get('text') is not None:\n",
    "            true_entities[entity['text']] = entity['value']\n",
    "        else:\n",
    "            for key, value in entity.items():\n",
    "                true_entities[key] = value\n",
    "    \n",
    "    # Process predicted entities\n",
    "    for entity in predicted_entities:\n",
    "        if isinstance(entity, str):\n",
    "            entity = ast.literal_eval(entity)\n",
    "        if entity.get('text') is not None:  \n",
    "            pred_entities[entity['text']] = entity['value']\n",
    "        else:\n",
    "            for key, value in entity.items():\n",
    "                pred_entities[key] = value\n",
    "\n",
    "    # Calculate metrics\n",
    "    true_positives = sum(1 for text in true_entities if text in pred_entities and true_entities[text] == pred_entities[text])\n",
    "    false_positives = sum(1 for text in pred_entities if text not in true_entities)\n",
    "    false_negatives = sum(1 for text in true_entities if text not in pred_entities)\n",
    "\n",
    "    if true_positives == 0:\n",
    "        return 0.0, 0.0, 0.0\n",
    "\n",
    "    precision = true_positives / (true_positives + false_positives)\n",
    "    recall = true_positives / (true_positives + false_negatives)\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    return precision, recall, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the detailed results for sentiment analysis for each model\n",
    "sonnet_detailed = pd.read_csv('results/ner/claude-0shot-negation_100.csv')\n",
    "gpt4_detailed = pd.read_csv('results/ner/gpt4o-0shot-negation_100.csv')\n",
    "llama_detailed = pd.read_csv('results/ner/llama-0shot-negation_100.csv')\n",
    "def convert_string_to_entities(entity_str):\n",
    "    \"\"\"Convert string representation of entities to proper format\"\"\"\n",
    "    if isinstance(entity_str, str):\n",
    "        try:\n",
    "            # Convert string to list of dicts\n",
    "            entities = ast.literal_eval(entity_str)\n",
    "            # Handle nested lists by flattening\n",
    "            if isinstance(entities, list):\n",
    "                # Handle double nested lists\n",
    "                if len(entities) > 0 and isinstance(entities[0], list):\n",
    "                    entities = entities[0]\n",
    "                # Handle list of dicts with text/value format\n",
    "                if len(entities) > 0 and isinstance(entities[0], dict):\n",
    "                    # Handle format with text/value keys\n",
    "                    if 'text' in entities[0]:\n",
    "                        return entities\n",
    "                    # Handle format with single key-value pair\n",
    "                    if len(entities[0]) == 1:\n",
    "                        converted = []\n",
    "                        for e in entities:\n",
    "                            for text, value in e.items():\n",
    "                                converted.append({'text': text, 'value': value})\n",
    "                        return converted\n",
    "                    # Handle format with multiple key-value pairs\n",
    "                    converted = []\n",
    "                    for e in entities:\n",
    "                        for text, value in e.items():\n",
    "                            if isinstance(value, str):\n",
    "                                converted.append({'text': text, 'value': value})\n",
    "                    return converted\n",
    "            return entities\n",
    "        except:\n",
    "            return []\n",
    "    return entity_str\n",
    "\n",
    "# Calculate accuracy for original and modified examples by type for each model\n",
    "type_results = []\n",
    "for type_name in sonnet_detailed['type'].unique():\n",
    "    # Get data for each model for this type\n",
    "    sonnet_data = sonnet_detailed[sonnet_detailed['type'] == type_name]\n",
    "    gpt4_data = gpt4_detailed[gpt4_detailed['type'] == type_name]\n",
    "    llama_data = llama_detailed[llama_detailed['type'] == type_name]\n",
    "    \n",
    "\n",
    "    # Calculate accuracies for each model\n",
    "    models_original_f1 = []\n",
    "    models_modified_f1 = []\n",
    "    \n",
    "    for data in [sonnet_data, gpt4_data, llama_data]:\n",
    "        # Calculate F1 for each row then average\n",
    "        original_f1s = []\n",
    "        modified_f1s = []\n",
    "        \n",
    "        for _, row in data.iterrows():\n",
    "            original_pred_ents = convert_string_to_entities(row['original_pred'])\n",
    "            original_label_ents = convert_string_to_entities(row['original_label'])\n",
    "            original_f1 = calculate_f1_ent(original_pred_ents, original_label_ents)\n",
    "            original_f1s.append(original_f1)\n",
    "            \n",
    "            modified_pred_ents = convert_string_to_entities(row['modified_pred'])\n",
    "            modified_label_ents = convert_string_to_entities(row['modified_label'])\n",
    "            modified_f1 = calculate_f1_ent(modified_pred_ents, modified_label_ents)\n",
    "            modified_f1s.append(modified_f1)\n",
    "        \n",
    "        models_original_f1.append(np.mean(original_f1s))\n",
    "        models_modified_f1.append(np.mean(modified_f1s))\n",
    "    # Average across models\n",
    "    avg_original_f1 = sum(models_original_f1) / len(models_original_f1)\n",
    "    avg_modified_f1 = sum(models_modified_f1) / len(models_modified_f1)\n",
    "    \n",
    "    # Calculate differences\n",
    "    difference = -(avg_original_f1 - avg_modified_f1)\n",
    "    pct_difference = -round((avg_original_f1 - avg_modified_f1) / avg_original_f1 * 100, 2)\n",
    "\n",
    "    # Perform t-test on combined results\n",
    "    all_original_f1s = []\n",
    "    all_modified_f1s = []\n",
    "    for data in [sonnet_data, gpt4_data, llama_data]:\n",
    "        for _, row in data.iterrows():\n",
    "            original_pred_ents = convert_string_to_entities(row['original_pred'])\n",
    "            original_label_ents = convert_string_to_entities(row['original_label'])\n",
    "            original_f1 = calculate_f1_ent(original_pred_ents, original_label_ents)\n",
    "            all_original_f1s.append(original_f1)\n",
    "            \n",
    "            modified_pred_ents = convert_string_to_entities(row['modified_pred'])\n",
    "            modified_label_ents = convert_string_to_entities(row['modified_label'])\n",
    "            modified_f1 = calculate_f1_ent(modified_pred_ents, modified_label_ents)\n",
    "            all_modified_f1s.append(modified_f1)\n",
    "    \n",
    "    _, p_value = stats.ttest_rel(all_original_f1s, all_modified_f1s)\n",
    "    \n",
    "    type_results.append({\n",
    "        'type': type_name,\n",
    "        'num_samples': len(sonnet_data),  # Total samples across all models\n",
    "        'original_res': round(avg_original_f1 * 100, 2),\n",
    "        'modified_res': round(avg_modified_f1 * 100, 2),\n",
    "        'difference': round(difference * 100, 2),\n",
    "        'pct_difference': round(pct_difference, 2),\n",
    "        'p_value': p_value\n",
    "    })\n",
    "\n",
    "# Create and display type-based results dataframe\n",
    "type_results_df = pd.DataFrame(type_results)\n",
    "\n",
    "# Apply the same styling as before\n",
    "styled_type_results = type_results_df.round(2).style.apply(highlight_drops_and_significance, axis=1)\n",
    "styled_type_results\n",
    "type_results_df.to_csv('ner_negation_type_results.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load OOD results for coreference resolution\n",
    "ood_results = pd.read_csv('OOD_Results - sentiment_analysis.csv')\n",
    "\n",
    "# Keep first column and percentage columns and format for LaTeX\n",
    "columns_to_keep = ['Unnamed: 0', 'BERT (%)', 'GPT2 (%)', 'T5 (%)', 'GPT4o (%)', 'CLAUDE (%)', 'LLAMA (%)']\n",
    "\n",
    "# Function to format first column in bold and numbers with colors\n",
    "def format_value(x, column):\n",
    "    if column == 'Unnamed: 0':\n",
    "        return f\"\\\\textbf{{{x}}}\"\n",
    "    else:\n",
    "        value = float(x)\n",
    "        # Define color intensity based on value\n",
    "        intensity = min(abs(value)/10, 1.0) # Scale intensity, max at 10% difference\n",
    "        if value > 0:\n",
    "            return f\"\\\\cellcolor{{green!{int(intensity*50)}}}{{$+${abs(value):.2f}}}\"\n",
    "        else:\n",
    "            return f\"\\\\cellcolor{{red!{int(intensity*50)}}}{{$-${abs(value):.2f}}}\"\n",
    "\n",
    "# Add midrules between categories\n",
    "prev_category = None\n",
    "formatted_rows = []\n",
    "for _, row in ood_results.iloc[:-2].iterrows():\n",
    "    category = row['Unnamed: 0'].split(':')[0]\n",
    "    if prev_category and category != prev_category:\n",
    "        formatted_rows.append({col: '\\\\midrule' if col == 'Unnamed: 0' else '' for col in columns_to_keep})\n",
    "    formatted_row = {col: format_value(row[col], col) for col in columns_to_keep}\n",
    "    formatted_rows.append(formatted_row)\n",
    "    prev_category = category\n",
    "\n",
    "ood_results_with_midrules = pd.DataFrame(formatted_rows)\n",
    "\n",
    "latex_table = ood_results_with_midrules.to_latex(\n",
    "    index=False,\n",
    "    escape=False,\n",
    "    caption=\"Out-of-Distribution Results for SENT (% Difference)\", \n",
    "    label=\"tab:ood_coref\"\n",
    ")\n",
    "\n",
    "# Save LaTeX table\n",
    "with open('ood_senti_table.tex', 'w') as f:\n",
    "    f.write(latex_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./results/coref/claude-3-5-sonnet-0shot-discourse_100.csv')\n",
    "df.head()\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "from scipy import stats\n",
    "\n",
    "# Calculate accuracies\n",
    "df['original_acc'] = df['original_pred'] == df['original_label']\n",
    "df['modified_acc'] = df['modified_pred'] == df['modified_label']\n",
    "\n",
    "# Convert boolean to int for t-test\n",
    "original_acc = df['original_acc'].astype(int)\n",
    "modified_acc = df['modified_acc'].astype(int)\n",
    "\n",
    "# Perform paired t-test\n",
    "t_stat, p_value = stats.ttest_rel(original_acc, modified_acc, alternative='less')\n",
    "\n",
    "print(\"Paired t-test results:\")\n",
    "print(f\"t-statistic: {t_stat:.4f}\")\n",
    "print(f\"p-value: {p_value:.4f}\")\n",
    "\n",
    "# Create contingency table for McNemar's test\n",
    "n11 = sum((df['original_pred'] == df['original_label']) & (df['modified_pred'] == df['modified_label']))  # Both correct\n",
    "n10 = sum((df['original_pred'] == df['original_label']) & (df['modified_pred'] != df['modified_label']))  # Original correct, modified wrong\n",
    "n01 = sum((df['original_pred'] != df['original_label']) & (df['modified_pred'] == df['modified_label']))  # Original wrong, modified correct  \n",
    "n00 = sum((df['original_pred'] != df['original_label']) & (df['modified_pred'] != df['modified_label']))  # Both wrong\n",
    "\n",
    "# Create contingency table\n",
    "contingency_table = [[n11, n10],\n",
    "                    [n01, n00]]\n",
    "\n",
    "# Perform McNemar's test\n",
    "mcnemar_result = mcnemar(contingency_table, exact=True)\n",
    "\n",
    "print(\"\\nMcNemar's test results:\")\n",
    "print(f\"Statistic: {mcnemar_result.statistic:.4f}\")\n",
    "print(f\"P-value: {mcnemar_result.pvalue:.4f}\")\n",
    "print(\"\\nContingency table:\")\n",
    "print(f\"Both correct (n11): {n11}\")\n",
    "print(f\"Original correct, modified wrong (n10): {n10}\")\n",
    "print(f\"Original wrong, modified correct (n01): {n01}\")\n",
    "print(f\"Both wrong (n00): {n00}\")\n",
    "\n",
    "# Calculate and print accuracies and percentage difference\n",
    "print('\\nAccuracies:')\n",
    "print(f\"Original accuracy: {original_acc.mean():.3f}\")\n",
    "print(f\"Modified accuracy: {modified_acc.mean():.3f}\")\n",
    "print(f\"Percentage difference: {((modified_acc.mean() - original_acc.mean()) / original_acc.mean()) * 100:.1f}%\")\n",
    "\n",
    "# Perform binomial test\n",
    "n_trials = len(df)\n",
    "n_original_correct = sum(original_acc)\n",
    "n_modified_correct = sum(modified_acc)\n",
    "\n",
    "# Binomial test for original accuracy\n",
    "binom_original = stats.binomtest(n_original_correct, n_trials, p=original_acc.mean(), alternative='less')\n",
    "print(\"\\nBinomial test results:\")\n",
    "print(f\"Original vs random chance:\")\n",
    "print(f\"p-value: {binom_original.pvalue:.4f}\")\n",
    "\n",
    "# Binomial test for modified accuracy\n",
    "binom_modified = stats.binomtest(n_modified_correct, n_trials, p=original_acc.mean(), alternative='less')\n",
    "print(f\"Modified vs random chance:\")\n",
    "print(f\"p-value: {binom_modified.pvalue:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./results/coref/llama-0shot-active_to_passive_100.csv')\n",
    "df.head()\n",
    "print(len(df))\n",
    "\n",
    "\n",
    "# Calculate accuracies\n",
    "df['original_acc'] = df['original_pred'] == df['original_label']\n",
    "df['modified_acc'] = df['modified_pred'] == df['modified_label']\n",
    "\n",
    "# Convert boolean to int for t-test\n",
    "original_acc = df['original_acc'].astype(int)\n",
    "modified_acc = df['modified_acc'].astype(int)\n",
    "\n",
    "# Perform paired t-test\n",
    "t_stat, p_value = stats.ttest_rel(original_acc, modified_acc, alternative='greater')\n",
    "\n",
    "print(\"Paired t-test results:\")\n",
    "print(f\"t-statistic: {t_stat:.4f}\")\n",
    "print(f\"p-value: {p_value:.4f}\")\n",
    "\n",
    "# Create contingency table for McNemar's test\n",
    "n11 = sum((df['original_pred'] == df['original_label']) & (df['modified_pred'] == df['modified_label']))  # Both correct\n",
    "n10 = sum((df['original_pred'] == df['original_label']) & (df['modified_pred'] != df['modified_label']))  # Original correct, modified wrong\n",
    "n01 = sum((df['original_pred'] != df['original_label']) & (df['modified_pred'] == df['modified_label']))  # Original wrong, modified correct  \n",
    "n00 = sum((df['original_pred'] != df['original_label']) & (df['modified_pred'] != df['modified_label']))  # Both wrong\n",
    "\n",
    "# Create contingency table\n",
    "contingency_table = [[n11, n10],\n",
    "                    [n01, n00]]\n",
    "\n",
    "# Perform McNemar's test\n",
    "mcnemar_result = mcnemar(contingency_table, exact=True)\n",
    "\n",
    "print(\"\\nMcNemar's test results:\")\n",
    "print(f\"Statistic: {mcnemar_result.statistic:.4f}\")\n",
    "print(f\"P-value: {mcnemar_result.pvalue:.4f}\")\n",
    "print(\"\\nContingency table:\")\n",
    "print(f\"Both correct (n11): {n11}\")\n",
    "print(f\"Original correct, modified wrong (n10): {n10}\")\n",
    "print(f\"Original wrong, modified correct (n01): {n01}\")\n",
    "print(f\"Both wrong (n00): {n00}\")\n",
    "\n",
    "# Calculate and print accuracies and percentage difference\n",
    "print('\\nAccuracies:')\n",
    "print(f\"Original accuracy: {original_acc.mean():.3f}\")\n",
    "print(f\"Modified accuracy: {modified_acc.mean():.3f}\")\n",
    "print(f\"Percentage difference: {((modified_acc.mean() - original_acc.mean()) / original_acc.mean()) * 100:.1f}%\")\n",
    "\n",
    "# Perform binomial test\n",
    "n_trials = len(df)\n",
    "n_original_correct = sum(original_acc)\n",
    "n_modified_correct = sum(modified_acc)\n",
    "\n",
    "# Binomial test for original accuracy\n",
    "binom_original = stats.binomtest(n_original_correct, n_trials, p=original_acc.mean())\n",
    "print(\"\\nBinomial test results:\")\n",
    "print(f\"Original vs random chance:\")\n",
    "print(f\"p-value: {binom_original.pvalue:.4f}\")\n",
    "\n",
    "# Binomial test for modified accuracy\n",
    "binom_modified = stats.binomtest(n_modified_correct, n_trials, p=original_acc.mean())\n",
    "print(f\"Modified vs random chance:\")\n",
    "print(f\"p-value: {binom_modified.pvalue:.4f}\")\n",
    "\n",
    "# Perform Wilcoxon signed-rank test\n",
    "wilcoxon_result = stats.wilcoxon(original_acc, modified_acc, alternative='two-sided')\n",
    "\n",
    "print(\"\\nWilcoxon signed-rank test results:\")\n",
    "print(f\"Statistic: {wilcoxon_result.statistic:.4f}\")\n",
    "print(f\"P-value: {wilcoxon_result.pvalue:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./results/dialogue/claude-3-5-sonnet-0shot-geographical_bias_100.csv')\n",
    "df.head()\n",
    "print(len(df))\n",
    "\n",
    "\n",
    "# Calculate accuracies\n",
    "df['original_acc'] = df['original_pred'] == df['original_label']\n",
    "df['modified_acc'] = df['modified_pred'] == df['modified_label']\n",
    "\n",
    "# Convert boolean to int for t-test\n",
    "original_acc = df['original_acc'].astype(int)\n",
    "modified_acc = df['modified_acc'].astype(int)\n",
    "\n",
    "# Perform paired t-test\n",
    "t_stat, p_value = stats.ttest_rel(original_acc, modified_acc, alternative='two-sided')\n",
    "\n",
    "print(\"Paired t-test results:\")\n",
    "print(f\"t-statistic: {t_stat:.4f}\")\n",
    "print(f\"p-value: {p_value:.4f}\")\n",
    "\n",
    "# Create contingency table for McNemar's test\n",
    "n11 = sum((df['original_pred'] == df['original_label']) & (df['modified_pred'] == df['modified_label']))  # Both correct\n",
    "n10 = sum((df['original_pred'] == df['original_label']) & (df['modified_pred'] != df['modified_label']))  # Original correct, modified wrong\n",
    "n01 = sum((df['original_pred'] != df['original_label']) & (df['modified_pred'] == df['modified_label']))  # Original wrong, modified correct  \n",
    "n00 = sum((df['original_pred'] != df['original_label']) & (df['modified_pred'] != df['modified_label']))  # Both wrong\n",
    "\n",
    "# Create contingency table\n",
    "contingency_table = [[n11, n10],\n",
    "                    [n01, n00]]\n",
    "\n",
    "# Perform McNemar's test\n",
    "mcnemar_result = mcnemar(contingency_table, exact=True)\n",
    "\n",
    "print(\"\\nMcNemar's test results:\")\n",
    "print(f\"Statistic: {mcnemar_result.statistic:.4f}\")\n",
    "print(f\"P-value: {mcnemar_result.pvalue:.4f}\")\n",
    "print(\"\\nContingency table:\")\n",
    "print(f\"Both correct (n11): {n11}\")\n",
    "print(f\"Original correct, modified wrong (n10): {n10}\")\n",
    "print(f\"Original wrong, modified correct (n01): {n01}\")\n",
    "print(f\"Both wrong (n00): {n00}\")\n",
    "\n",
    "# Calculate and print accuracies and percentage difference\n",
    "print('\\nAccuracies:')\n",
    "print(f\"Original accuracy: {original_acc.mean():.3f}\")\n",
    "print(f\"Modified accuracy: {modified_acc.mean():.3f}\")\n",
    "print(f\"Percentage difference: {((modified_acc.mean() - original_acc.mean()) / original_acc.mean()) * 100:.1f}%\")\n",
    "\n",
    "# Perform binomial test\n",
    "n_trials = len(df)\n",
    "n_original_correct = sum(original_acc)\n",
    "n_modified_correct = sum(modified_acc)\n",
    "\n",
    "# Binomial test for original accuracy\n",
    "binom_original = stats.binomtest(n_original_correct, n_trials, p=original_acc.mean())\n",
    "print(\"\\nBinomial test results:\")\n",
    "print(f\"Original vs random chance:\")\n",
    "print(f\"p-value: {binom_original.pvalue:.4f}\")\n",
    "\n",
    "# Binomial test for modified accuracy\n",
    "binom_modified = stats.binomtest(n_modified_correct, n_trials, p=original_acc.mean(), alternative='two-sided')\n",
    "print(f\"Modified vs random chance:\")\n",
    "print(f\"p-value: {binom_modified.pvalue:.4f}\")\n",
    "\n",
    "# Perform Wilcoxon signed-rank test\n",
    "wilcoxon_result = stats.wilcoxon(original_acc, modified_acc, alternative='two-sided')\n",
    "\n",
    "print(\"\\nWilcoxon signed-rank test results:\")\n",
    "print(f\"Statistic: {wilcoxon_result.statistic:.4f}\")\n",
    "print(f\"P-value: {wilcoxon_result.pvalue:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import ast\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('../data/modified_data/coref/active_to_passive_fix.csv')\n",
    "\n",
    "# Read the JSON file\n",
    "with open('../data/modified_data/coref/active_to_passive_100.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Create mapping of indices to data entries\n",
    "# data_dict = {item['index']: item for item in data}\n",
    "\n",
    "# Update data based on filtered rows\n",
    "for idx, row in df.iterrows():\n",
    "    # Update the entry with new values from CSV\n",
    "    data[idx]['original_label'] = int(row['original_label'])\n",
    "    data[idx]['modified_label'] = int(row['modified_label'])\n",
    "    data[idx]['original_pronoun'] = row['original_pronoun']\n",
    "    data[idx]['modified_pronoun'] = row['modified_pronoun']\n",
    "    data[idx]['original_candidates'] = ast.literal_eval(row['original_candidates'])\n",
    "    data[idx]['modified_candidates'] = ast.literal_eval(row['modified_candidates'])\n",
    "\n",
    "# Convert back to list and write to file\n",
    "updated_data = data\n",
    "with open('../data/modified_data/coref/active_to_passive_100.json', 'w') as f:\n",
    "    json.dump(updated_data, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import ast\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('../data/modified_data/coref/length_bias_fix.csv')\n",
    "\n",
    "# Read the JSON file\n",
    "with open('../data/modified_data/coref/length_bias_100.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Create mapping of indices to data entries\n",
    "# data_dict = {item['index']: item for item in data}\n",
    "\n",
    "# Update data based on filtered rows\n",
    "for idx, row in df.iterrows():\n",
    "    # Update the entry with new values from CSV\n",
    "    data[idx]['modified_label'] = int(row['modified_label'])\n",
    "    data[idx]['modified_pronoun'] = row['modified_pronoun']\n",
    "    data[idx]['modified_candidates'] = ast.literal_eval(row['modified_candidates'])\n",
    "    data[idx]['modified_text'] = row['modified_text']\n",
    "    \n",
    "\n",
    "# Convert back to list and write to file\n",
    "updated_data = data\n",
    "with open('../data/modified_data/coref/length_bias_100.json', 'w') as f:\n",
    "    json.dump(updated_data, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import ast\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('../data/modified_data/dialogue/sentiment_100_fix.csv')\n",
    "\n",
    "# Read the JSON file\n",
    "with open('../data/modified_data/dialogue/sentiment_100.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "# Update data based on filtered rows\n",
    "for idx, row in df.iterrows():\n",
    "    # Update the entry with new values from CSV\n",
    "    data[idx][1]['modified_label'] = int(row['modified_label'])\n",
    "\n",
    "# Convert back to list and write to file\n",
    "updated_data = data\n",
    "with open('../data/modified_data/dialogue/sentiment_100.json', 'w') as f:\n",
    "    json.dump(updated_data, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import ast\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('../data/modified_data/dialogue/grammatical_role_100_fix.csv')\n",
    "\n",
    "# Read the JSON file\n",
    "with open('../data/modified_data/dialogue/sentiment_100.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "# Update data based on filtered rows\n",
    "for idx, row in df.iterrows():\n",
    "    # Update the entry with new values from CSV\n",
    "    data[idx][1]['modified_label'] = int(row['modified_label'])\n",
    "\n",
    "# Convert back to list and write to file\n",
    "updated_data = data\n",
    "with open('../data/modified_data/dialogue/sentiment_100.json', 'w') as f:\n",
    "    json.dump(updated_data, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Get all *_100.json files in coref's directory except excluded ones\n",
    "excluded = []\n",
    "json_files = []\n",
    "for f in glob.glob('../data/modified_data/coref/*_100.json'):\n",
    "    filename = os.path.basename(f).replace('_100.json','')\n",
    "    if filename not in excluded:\n",
    "        json_files.append(f)\n",
    "\n",
    "for file_path in json_files:\n",
    "    print(f\"Processing {file_path}\")\n",
    "    \n",
    "    # Read JSON file\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    modified = False\n",
    "    # Track seen entries to remove duplicates\n",
    "    seen_entries = set()\n",
    "    filtered_data = []\n",
    "    \n",
    "    for entry in data:\n",
    "        # Create tuple of key fields to check for duplicates\n",
    "        entry_key = (entry['original_text'], entry['modified_text'], \n",
    "                    tuple(entry['original_candidates']), tuple(entry['modified_candidates']),\n",
    "                    entry['original_pronoun'], entry['modified_pronoun'])\n",
    "        \n",
    "        if entry_key not in seen_entries:\n",
    "            seen_entries.add(entry_key)\n",
    "            \n",
    "            # Check if modified pronoun exists in modified text\n",
    "            # if entry['modified_pronoun'] not in entry['modified_text']:\n",
    "            #     print(f\"Missing pronoun {entry['modified_pronoun']} in modified text: {entry['modified_text']}\")\n",
    "            #     modified = True\n",
    "                \n",
    "            # # Check if modified candidates exist in modified text\n",
    "            # for candidate in entry['modified_candidates']:\n",
    "            #     if candidate not in entry['modified_text']:\n",
    "            #         print(f\"Missing candidate {candidate} in modified text: {entry['modified_text']}\")\n",
    "            #         modified = True\n",
    "                    \n",
    "            # Reset modified label if it differs from original\n",
    "            if entry['modified_label'] != entry['original_label']:\n",
    "                print(f\"Label mismatch in: {entry['modified_text']}\")\n",
    "                # entry['modified_label'] = entry['original_label']\n",
    "                modified = True\n",
    "                \n",
    "            filtered_data.append(entry)\n",
    "        else:\n",
    "            print(f\"Found duplicate entry: {entry['modified_text']}\")\n",
    "            modified = True\n",
    "    \n",
    "    # Replace data with deduplicated version\n",
    "    data[:] = filtered_data\n",
    "    # Save changes if modifications were made\n",
    "    if modified:\n",
    "        print(f\"Modified {file_path}\")\n",
    "        print('===================')\n",
    "        # with open(file_path, 'w') as f:\n",
    "        #     json.dump(data, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Get all CSV files matching the pattern\n",
    "csv_files = glob.glob('results/ner/*-0shot-*_100_new.csv')\n",
    "\n",
    "def parse_entities(entities):\n",
    "    if isinstance(entities, str):\n",
    "        entities = ast.literal_eval(entities)\n",
    "    parsed_entities = []\n",
    "    for item in entities:\n",
    "    \n",
    "        if 'text' in item:\n",
    "            label = item['value'] if 'value' in item else item['label']\n",
    "            parsed_entities.append({'text': item['text'].strip(), 'label': label.strip()})\n",
    "        else:\n",
    "            for key, value in item.items():\n",
    "                parsed_entities.append({'text': key.strip(), 'label': value.strip()})\n",
    "    parsed_entities = sorted(parsed_entities, key=lambda x: x['text'])\n",
    "    return parsed_entities\n",
    "\n",
    "for file_path in csv_files:\n",
    "    print(f\"Processing {file_path}\")\n",
    "    \n",
    "    # Read CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    df['modified_label'] = df['modified_label'].apply(parse_entities)\n",
    "    df['original_label'] = df['original_label'].apply(parse_entities)\n",
    "    df['modified_pred'] = df['modified_pred'].apply(parse_entities)\n",
    "    df['original_pred'] = df['original_pred'].apply(parse_entities)\n",
    "\n",
    "    # Save back to CSV\n",
    "    df.to_csv(file_path.replace('_100_new.csv', '_100_compare.csv'), index=False)\n",
    "    print(f\"Saved formatted {file_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all results files\n",
    "files_and_tasks = {\n",
    "    '../pretrained_language_models/ner/ner_modification_results_combined.csv': 'NER',\n",
    "    '../pretrained_language_models/coreference_resolution/tmp/coreference_combined_results.csv': 'Coreference', \n",
    "    '../pretrained_language_models/dialogue_contradiction_detection/tmp/dialogue_combined_results.csv': 'Dialogue',\n",
    "    '../pretrained_language_models/sentiment_analysis/sentiment_analysis_results.csv': 'Sentiment'\n",
    "}\n",
    "\n",
    "# Read and combine all dataframes\n",
    "dfs = []\n",
    "for file_path, task in files_and_tasks.items():\n",
    "    df = pd.read_csv(file_path)\n",
    "    if 'mean_f1_pct_change' in df.columns:\n",
    "        df = df.rename(columns={'mean_f1_pct_change': 'pct_difference'})\n",
    "    if 'percentage_diff' in df.columns:\n",
    "        df = df.rename(columns={'percentage_diff': 'pct_difference'})\n",
    "    if 'pct_diff' in df.columns:\n",
    "        df = df.rename(columns={'pct_diff': 'pct_difference'})\n",
    "    df['task'] = task\n",
    "    dfs.append(df)\n",
    "\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "# Keep only the specified columns\n",
    "combined_df = combined_df[['model', 'task', 'modification', 'pct_difference']]\n",
    "\n",
    "# Normalize model names\n",
    "def normalize_model_name(model):\n",
    "    model = model.lower()\n",
    "    if 'bert' in model:\n",
    "        return 'bert'\n",
    "    elif 'gpt-2' in model or 'gpt2' in model:\n",
    "        return 'gpt2'\n",
    "    elif 't5' in model:\n",
    "        return 't5'\n",
    "    elif 'gpt4' in model or 'gpt-4' in model:\n",
    "        return 'gpt4o'\n",
    "    elif 'claude' in model:\n",
    "        return 'claude'\n",
    "    elif 'llama' in model:\n",
    "        return 'llama'\n",
    "    return model\n",
    "print(combined_df['model'].unique())\n",
    "combined_df['model'] = combined_df['model'].apply(normalize_model_name)\n",
    "print(combined_df['model'].unique())\n",
    "# Take absolute value of pct_difference\n",
    "combined_df['abs_pct_diff'] = combined_df['pct_difference'].abs()\n",
    "combined_df.to_csv('combined_df.csv', index=False)\n",
    "# Create pivot table with modifications as rows and models as columns\n",
    "# Using mean since we want to see average impact across all tasks\n",
    "final_df = pd.pivot_table(combined_df, \n",
    "                         values='abs_pct_diff',\n",
    "                         index='modification',\n",
    "                         columns='model',\n",
    "                         aggfunc='mean').round(4)\n",
    "\n",
    "# Add mean across models for each modification\n",
    "final_df['average'] = final_df.mean(axis=1).round(4)\n",
    "\n",
    "# Add mean across modifications for each model\n",
    "model_avgs = final_df.mean().round(4)\n",
    "final_df.loc['avg_across_mods'] = model_avgs\n",
    "\n",
    "# Create LaTeX table\n",
    "mod_mapping = {\n",
    "    'temporal_bias': 'B: Tem',\n",
    "    'geographical_bias': 'B: Geo', \n",
    "    'length_bias': 'B: Len',\n",
    "    'typo_bias': 'O: Spell',\n",
    "    'capitalization': 'O: Cap',\n",
    "    'punctuation': 'O: Punc',\n",
    "    'derivation': 'M: Deri',\n",
    "    'compound_word': 'M: Com',\n",
    "    'active_to_passive': 'Sx: Voice',\n",
    "    'grammatical_role': 'Sx: Gra',\n",
    "    'coordinating_conjunction': 'Sx: Conj',\n",
    "    'concept_replacement': 'Sm: Con',\n",
    "    'negation': 'P: Neg',\n",
    "    'discourse': 'P: Disc',\n",
    "    'sentiment': 'P: Senti',\n",
    "    'casual': 'G: Cas',\n",
    "    'dialectal': 'G: Dial'\n",
    "}\n",
    "\n",
    "model_order = ['bert', 'gpt2', 't5', 'gpt4o', 'claude', 'llama', 'average']\n",
    "\n",
    "# Reorder and rename modifications\n",
    "final_df = final_df[model_order]\n",
    "final_df.index = [mod_mapping.get(idx, idx) for idx in final_df.index]\n",
    "\n",
    "latex_table = \"\\\\begin{table}[t]\\n\\\\centering\\n\\\\begin{tabular}{l|\" + \"c\"*len(model_order) + \"}\\n\"\n",
    "latex_table += \"\\\\hline\\nModification & \" + \" & \".join(model_order) + \" \\\\\\\\\\n\\\\hline\\n\"\n",
    "\n",
    "current_prefix = None\n",
    "\n",
    "# Get max values for last column and row for scaling\n",
    "avg_col_max = final_df['average'].max()\n",
    "avg_row_max = final_df.loc['avg_across_mods'].max()\n",
    "\n",
    "for idx in mod_mapping.values():\n",
    "    if idx in final_df.index:\n",
    "        # Add hline when prefix changes\n",
    "        prefix = idx.split(':')[0]\n",
    "        if current_prefix is not None and prefix != current_prefix:\n",
    "            latex_table += \"\\\\hline\\n\"\n",
    "        current_prefix = prefix\n",
    "        \n",
    "        # Bold the modification name\n",
    "        bold_idx = f\"\\\\textbf{{{idx}}}\"\n",
    "        row = []\n",
    "        \n",
    "        for col in model_order:\n",
    "            val = final_df.loc[idx, col]\n",
    "            if col == 'average':  # For the last column\n",
    "                intensity = int((val / avg_col_max) * 30)  # Scale based on average column max\n",
    "                row.append(f\"\\\\cellcolor{{brown!{intensity}}} {val:.2f}\")\n",
    "            else:\n",
    "                row.append(f\"{val:.2f}\")\n",
    "        latex_table += f\"{bold_idx} & \" + \" & \".join(row) + \" \\\\\\\\\\n\"\n",
    "\n",
    "latex_table += \"\\\\hline\\n\"\n",
    "avg_row = []\n",
    "for col in model_order:\n",
    "    val = final_df.loc['avg_across_mods', col]\n",
    "    if col == 'average':\n",
    "        intensity = int((val / avg_row_max) * 30)  # Scale based on average row max\n",
    "        avg_row.append(f\"\\\\cellcolor{{yellow!{intensity}}} {val:.2f}\")\n",
    "    else:\n",
    "        intensity = int((val / avg_row_max) * 30)  # Scale based on average row max\n",
    "        avg_row.append(f\"\\\\cellcolor{{purple!{intensity}}} {val:.2f}\")\n",
    "latex_table += \"\\\\textbf{Average} & \" + \" & \".join(avg_row) + \" \\\\\\\\\\n\"\n",
    "latex_table += \"\\\\hline\\n\\\\end{tabular}\\n\"\n",
    "latex_table += \"\\\\caption{Average absolute percentage change in performance across different modifications and models}\\n\"\n",
    "latex_table += \"\\\\label{tab:modifications}\\n\\\\end{table}\"\n",
    "print(latex_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(4.7+3.6+2.4+5.6)/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
