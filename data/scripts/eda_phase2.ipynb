{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from statsmodels.stats.inter_rater import fleiss_kappa,aggregate_raters\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "import krippendorff\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = '../phase2_annotations_part1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_files = [file for file in os.listdir(dir) if file.endswith('.json')]\n",
    "annotation_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_types = set()\n",
    "for file in annotation_files:\n",
    "    task = file.split('-')[1]\n",
    "    task = task.replace('.json', '')\n",
    "    # print(task)\n",
    "    test_types.add(task)\n",
    "len(test_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_all = {}\n",
    "for file in annotation_files:\n",
    "    fn = os.path.join(dir, file)\n",
    "    with open(fn, 'rb') as f:\n",
    "        batch_name = file.split('.json')[0]\n",
    "        annotations = json.load(f)\n",
    "        obj = {batch_name: annotations}\n",
    "        annotations_all.update(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(annotations_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks_name = ['conversational_qa', 'coreference_resolution', 'dialogue_contradiction_detection', 'named_entity_recognition', 'sentiment_analysis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "modification_name = ['active_to_passive', 'capitalization', 'casual','compound_word', 'concept_replacement', 'coordinating_conjunction', 'derivation', 'dialectal', 'discourse', 'geographical_bias', 'grammatical_role', 'length_bias', 'negation', 'punctuation', 'sentiment', 'temporal_bias', 'typo_bias']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "have_subtest = ['concept_replacement', 'dialectal', 'discourse', 'negation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(annotations_all['grammatical_role-phaseTwo1103-coreference_resolution-batch2']['5c16a14c5ca98a00018d30c4']['answers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(annotations_all['grammatical_role-phaseTwo1103-coreference_resolution-batch2']['5fcfedeec803e31be21a9483']['answers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_data(filter_list, filter = False):\n",
    "    annotation_dict = {}\n",
    "    for task in modification_name:\n",
    "        \n",
    "        annotation_dict[task] = []\n",
    "        # test_name = task\n",
    "        \n",
    "    for batch_name in annotations_all.keys():\n",
    "        # print(batch_name)\n",
    "        task = batch_name.split('-')[0]\n",
    "        bigtask_name = batch_name.split('-')[2]\n",
    "        test_name = task \n",
    "        release = batch_name.split('-')[1]\n",
    "        \n",
    "        for task_name in modification_name:\n",
    "            if task_name == task:\n",
    "                \n",
    "                df_task = []\n",
    "                for annotator, value in annotations_all[batch_name].items():\n",
    "                    if filter:\n",
    "                        if annotator in filter_list:\n",
    "                            continue\n",
    "                    # for sample in annotator['answers']:\n",
    "                    # print(annotator)\n",
    "                    # print(value['answers'])\n",
    "                    df = pd.DataFrame(value['answers'])\n",
    "                    subtests = []\n",
    "                    tests = [test_name]*len(df)\n",
    "                    tasks = [bigtask_name]*len(df)\n",
    "                    new_row = {}\n",
    "                    for i, row in df.iterrows():\n",
    "                        if row['is_control'] == True:\n",
    "                            subtest = 'control'\n",
    "                        else:\n",
    "                            if release == 'release0808':\n",
    "                                if task_name == 'concept_replacement':\n",
    "                                    subtest = 'synonym'\n",
    "                                elif task_name == 'discourse':\n",
    "                                    subtest = 'reverse'\n",
    "                                elif task_name == 'negation':\n",
    "                                    subtest = 'verbal'\n",
    "                                else:\n",
    "                                    subtest = ''\n",
    "                            else:\n",
    "                                subtest = row['original_question'].get('type','')\n",
    "                            if test_name not in have_subtest:\n",
    "                                subtest = test_name\n",
    "                        subtests.append(subtest)\n",
    "                    if len(df) == 22:\n",
    "                        worker_id = list(df['worker_id'])[0]\n",
    "                        if df['is_control'].value_counts().get(True) < 3:\n",
    "                            is_control = True\n",
    "                            answer = 1\n",
    "                            subtest = 'control'\n",
    "                        else:\n",
    "                            is_control = False\n",
    "                            answer = 0\n",
    "                        # if bigtask_name == 'active_to_passive':\n",
    "                            # print(df)\n",
    "                        new_row = {'time': None, 'instance_index': len(df)+1, 'worker_id': worker_id,\t'explanation': None, 'answer': str(answer),\t'original_question': 0,\t'is_control':is_control, 'subtest': subtest, 'test': test_name, 'task': bigtask_name}\n",
    "                        \n",
    "                    elif len(df) < 16:\n",
    "                        print(task, annotator) #some batches with less than 12 questions\n",
    "                        continue\n",
    "                    df['test'] = tests\n",
    "                    df['task'] = tasks\n",
    "                    df['subtest'] = subtests\n",
    "                    if len(df) == 22:\n",
    "                        new_row_df = pd.DataFrame([new_row])\n",
    "                        df = pd.concat([df,new_row_df], ignore_index = True)\n",
    "                    df_task.append(df)\n",
    "                if len(df_task) <= 1:\n",
    "                    continue\n",
    "                df_task = pd.concat(df_task)    \n",
    "                annotation_dict[task_name].append(df_task)\n",
    "        # break\n",
    "    return annotation_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = construct_data(filter_list = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations['grammatical_role'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations['grammatical_role'][2]['subtest'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_modes_and_counts(arr):\n",
    "    # print(len(arr))\n",
    "    # for row in arr:\n",
    "        # print(len(row))\n",
    "    arr = list(arr)\n",
    "    arr = np.array(arr)\n",
    "    # print(arr)\n",
    "    _, cols = arr.shape\n",
    "    modes = []\n",
    "    counts = []\n",
    "    \n",
    "    # Process each column separately\n",
    "    for j in range(cols):\n",
    "        values, value_counts = np.unique(arr[:, j], return_counts=True)\n",
    "        max_count = value_counts.max()\n",
    "        # Find all values that appear max times\n",
    "        col_modes = values[value_counts == max_count]\n",
    "        # print(col_modes[0])\n",
    "        # print(max_count[0])\n",
    "        \n",
    "        modes.append(col_modes[0])\n",
    "        counts.append(max_count)\n",
    "    \n",
    "    modes = np.array(modes)\n",
    "    counts = np.array(counts)\n",
    "    return modes, counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_xor_logical(arr1, arr2):\n",
    "    # Using logical_xor for element-wise comparison\n",
    "    arr1 = np.asarray(arr1)\n",
    "    arr2 = np.asarray(arr2)\n",
    "    \n",
    "    # Convert strings to boolean based on non-emptiness\n",
    "    # bool1 = arr1 != ''\n",
    "    # bool2 = arr2 != ''\n",
    "    # print(bool1)\n",
    "    # print(bool2)\n",
    "    # Perform logical XOR\n",
    "    return np.logical_xor(arr1, arr2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_krippendof_for_each_batch(df_batch):\n",
    "    annotation_data = []\n",
    "    # print(df_batch)\n",
    "    for group in df_batch.groupby('worker_id'):\n",
    "        preds = np.array(group[1]['answer']).astype(str)\n",
    "        annotation_data.append(preds)\n",
    "    # print(annotation_data)\n",
    "\n",
    "    data = np.array(annotation_data)\n",
    "    # print(data)\n",
    "    # Transpose the data\n",
    "    # transposed_data = data.T\n",
    "\n",
    "    # Transform the transposed data\n",
    "    # aggregated_data, categories = aggregate_raters(transposed_data)\n",
    "    # print(aggregated_data)\n",
    "    # Calculate Fleiss' Kappa\n",
    "    alpha = krippendorff.alpha(data, level_of_measurement='nominal')\n",
    "    # kappa = fleiss_kappa(aggregated_data, method='randolph')\n",
    "    # print(f\"Krippendorff alpha: {alpha:.4f}\")\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_fleiss_kappa_for_each_batch(df_batch):\n",
    "    annotation_data = []\n",
    "    for group in df_batch.groupby('worker_id'):\n",
    "        preds = np.array(group[1]['answer']).astype(str)\n",
    "        annotation_data.append(preds)\n",
    "    # print(len(annotation_data))\n",
    "    # for row in annotation_data:\n",
    "    #     print(len(row))\n",
    "    data = np.array(annotation_data)\n",
    "\n",
    "    # print(data)\n",
    "    # print(data)\n",
    "    # Transpose the data\n",
    "    transposed_data = data.T\n",
    "\n",
    "    # Transform the transposed data\n",
    "    aggregated_data, categories = aggregate_raters(transposed_data)\n",
    "    # print(aggregated_data)\n",
    "    # Calculate Fleiss' Kappa\n",
    "    kappa = fleiss_kappa(aggregated_data, method='randolph')\n",
    "    # print(f\"Fleiss' Kappa: {kappa:.4f}\")\n",
    "    return kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_majority(batch, df_performance = None):\n",
    "    annotation_data = []\n",
    "    annotation_data_no_control = []\n",
    "    annotation_data_control_only = []\n",
    "    cur_best_annotator_score = 0\n",
    "    best_preds = None\n",
    "    for group in batch.groupby('worker_id'):\n",
    "        # print(df_annotator[df_annotator[['success_rate_control_only']] == group[0]])\n",
    "        preds = list(group[1]['answer'])\n",
    "        answers_no_control = group[1][group[1]['is_control'] != True]['answer'] \n",
    "        answers_control_only = group[1][group[1]['is_control'] == True]['answer'] \n",
    "        if df_performance.empty != True:\n",
    "            # print(df_performance)\n",
    "            # print(group[0])\n",
    "            annotator_score = df_performance[df_performance['annotator_id'] == group[0]]['success_rate_control_only'].values[0]\n",
    "            # print(annotator_score)\n",
    "            if annotator_score > cur_best_annotator_score:\n",
    "                best_preds = preds\n",
    "                cur_best_annotator_score = annotator_score\n",
    "\n",
    "        # answers_control_only = group[1][group[1]['is_control'] == True]['answer'] \n",
    "\n",
    "\n",
    "        annotation_data.append(preds)\n",
    "        annotation_data_no_control.append(answers_no_control)\n",
    "        annotation_data_control_only.append(answers_control_only)\n",
    "\n",
    "\n",
    "    # print(annotation_data)\n",
    "    # modes  = stats.mode(annotation_data,axis = 0,keepdims = True).mode[0]\n",
    "    # counts   = stats.mode(annotation_data,axis = 0,keepdims = True).count[0]\n",
    "    # print(annotation_data)\n",
    "    modes, counts = find_modes_and_counts(annotation_data)\n",
    "    # print(stats.mode(annotation_data_no_control, axis=0, keepdims = True))\n",
    "    # print(annotation_data_no_control)\n",
    "    # modes_no_control = stats.mode(annotation_data_no_control, axis=0, keepdims = True).mode[0]\n",
    "    # counts_no_control = stats.mode(annotation_data_no_control, axis=0, keepdims = True).count[0]\n",
    "    modes_no_control, counts_no_control = find_modes_and_counts(annotation_data_no_control)\n",
    "    if df_performance.empty != True:\n",
    "        for i, count in enumerate(counts):\n",
    "            if count <= len(batch.groupby('worker_id')) // 2:\n",
    "                modes[i] = best_preds[i]\n",
    "\n",
    "    # modes_control_only = stats.mode(annotation_data_control_only, axis=0).mode[0]\n",
    "    modes_control_only, counts_control_only = find_modes_and_counts(annotation_data_control_only)\n",
    "    # modes_control_only = np.zeros(len(modes) - len(modes_no_control), dtype = int)\n",
    "    \n",
    "    # print(modes.mode[0])\n",
    "    return modes, modes_no_control, modes_control_only, counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_success_rate_majority(batch, df_performance = None):\n",
    "    rates = []\n",
    "    rates_no_control = []\n",
    "    rates_control_only = []\n",
    "    rates_gpt4 = []\n",
    "    modes, modes_no_control, modes_control_only, majority_counts = get_majority(batch, df_performance)\n",
    "    \n",
    "    # print(modes)\n",
    "    # print(modes_no_control)\n",
    "    # print(modes_control_only)\n",
    "    df_annotator = []\n",
    "    # answers = np.array(batch.groupby('worker_id')[0][1]['answer'])\n",
    "    # print(len(batch.groupby('worker_id')))\n",
    "    majority = len(batch.groupby('worker_id')) // 2\n",
    "    # annotator_counts = len(batch.groupby('worker_id'))\n",
    "    retain_count = 0\n",
    "    total_count = 0\n",
    "    retain_samples = [None]*len(modes)\n",
    "    # retain = False\n",
    "    \n",
    "    for j,group in enumerate(batch.groupby('worker_id')):\n",
    "        # if j == 1:\n",
    "            # print(len(group[1]))\n",
    "        for i,sample in group[1].iterrows():\n",
    "            if sample['is_control'] == False:\n",
    "                item = {}\n",
    "                if type(sample['original_question']) == int:\n",
    "                        continue\n",
    "                item['original_question'] = sample['original_question']                       \n",
    "                item['task'] = sample['task']\n",
    "                item['test'] = sample['test']\n",
    "                item['subtest'] = sample['subtest']\n",
    "                # item['label'] = sample['answer']\n",
    "                item['label'] = modes[i]\n",
    "                item['explanation'] = sample['explanation']\n",
    "                retain_samples[i] = item\n",
    "        retain = True\n",
    "        answers = np.array(list(group[1]['answer']))\n",
    "        answers_no_control = np.array(group[1][group[1]['is_control'] != True]['answer']) \n",
    "        answers_control_only = np.array(group[1][group[1]['is_control'] == True]['answer'])\n",
    "\n",
    "        \n",
    "        correct_answer = answers == modes\n",
    "        success_rate = np.count_nonzero(correct_answer) / len(answers)\n",
    "\n",
    "        # print('Answer', answers_no_control)\n",
    "        # print('Modes', modes_no_control)\n",
    "        # print(np.logical_xor(answers_no_control, modes_no_control))\n",
    "        correct_answer_no_control =  answers_no_control == modes_no_control\n",
    "\n",
    "        success_rate_no_control = np.count_nonzero(correct_answer_no_control) / len(answers_no_control)\n",
    "\n",
    "        correct_answer_control_only = answers_control_only == modes_control_only\n",
    "        success_rate_control_only = np.count_nonzero(correct_answer_control_only) / len(answers_control_only)\n",
    "\n",
    "        # retain_count += np.count_nonzero(modes_no_control)\n",
    "        # total_count += len(answers_no_control)\n",
    "\n",
    "        success_rate_gpt4 = np.count_nonzero(modes_no_control) / len(modes_no_control)\n",
    "        negative_count = np.count_nonzero(answers_no_control == 0) \n",
    "        # print('Success rate for annotator with control', group[0], success_rate)\n",
    "        # print('Success rate for annotator without control', group[0], success_rate_no_control)\n",
    "        # print('Success rate for annotator control only', group[0], success_rate_control_only)\n",
    "        df_annotator_row = {}\n",
    "        df_annotator_row['annotator_id'] = group[0]\n",
    "        df_annotator_row['success_rate_with_control'] = success_rate\n",
    "        df_annotator_row['success_rate_without_control'] = success_rate_no_control\n",
    "        df_annotator_row['success_rate_control_only'] = success_rate_control_only\n",
    "        df_annotator_row['negative_count'] = negative_count\n",
    "        # df_annotator_row['counts'] = counts\n",
    "        df_annotator.append(df_annotator_row)\n",
    "\n",
    "        rates.append(success_rate)\n",
    "        rates_no_control.append(success_rate_no_control)\n",
    "        rates_control_only.append(success_rate_control_only)\n",
    "        rates_gpt4.append(success_rate_gpt4)\n",
    "\n",
    "    # print(majority_counts)\n",
    "    # print(majority)\n",
    "    majority_rate = (majority_counts>majority).sum()/ len(answers)\n",
    "    # print(len(retain_samples))\n",
    "    total_count = len(answers_no_control)\n",
    "    retain_count = len(retain_samples)\n",
    "    # retain_count = np.count_nonzero(modes_no_control)\n",
    "    # print(total_count, retain_count)\n",
    "    # print(retain_samples)\n",
    "    retain_samples = [x for x in retain_samples if x is not None]\n",
    "\n",
    "    return np.mean(rates), np.mean(rates_no_control), np.mean(rates_control_only), np.mean(rates_gpt4), majority_rate, retain_samples, total_count, retain_count, df_annotator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_all(filter_list, filter, hit_function, df_performance = None):\n",
    "    df_task = []\n",
    "    df_annotator  = []\n",
    "    df_subtask = []\n",
    "    df_retain_task = []\n",
    "    annotation_dict = construct_data(filter_list, filter)\n",
    "    for key,value in annotation_dict.items():\n",
    "        # print(key)\n",
    "        kappa_task = []\n",
    "        krippendorff_task = []\n",
    "        success_rate_task = []\n",
    "        success_rate_task_no_control = []\n",
    "        success_rate_task_control_only = []\n",
    "        success_rate_gpt4_task = []\n",
    "        majority_rate_task = []\n",
    "        for i, batch in enumerate(value):\n",
    "            # print(batch)\n",
    "            # if i==0:\n",
    "            test_name = key\n",
    "            task_name = list(batch['task'])[0]\n",
    "            subtest_name = list(batch['subtest'])\n",
    "            for name in subtest_name:\n",
    "                if name != 'control':\n",
    "                    subtest_name = name\n",
    "                    break\n",
    "            print(task_name, test_name, subtest_name)\n",
    "                \n",
    "            \n",
    "            kappa_batch = calculate_fleiss_kappa_for_each_batch(batch)\n",
    "            krippendorff_batch = calculate_krippendof_for_each_batch(batch)\n",
    "            if hit_function == 'majority':\n",
    "                success_rate_batch, success_rate_batch_no_control, success_rate_batch_control_only, success_rate_gpt4_batch, majority_rate_batch, retain_samples_batch, total_count_batch, retain_count_batch,  df_annotator_row = calculate_success_rate_majority(batch, df_performance)\n",
    "            # print('Fleiss kappa batch', i+1, kappa_batch)\n",
    "            # print('Krippendorff alpha batch', i+1, krippendorff_batch)\n",
    "\n",
    "            kappa_task.append(kappa_batch)\n",
    "            krippendorff_task.append(krippendorff_batch)\n",
    "\n",
    "            # print('Success rate batch with control', i+1, success_rate_batch)\n",
    "            # print('Success rate batch without control', i+1, success_rate_batch_no_control)\n",
    "            success_rate_task.append(success_rate_batch)\n",
    "            success_rate_task_no_control.append(success_rate_batch_no_control)\n",
    "            success_rate_task_control_only.append(success_rate_batch_control_only)\n",
    "            success_rate_gpt4_task.append(success_rate_gpt4_batch)\n",
    "            majority_rate_task.append(majority_rate_batch)\n",
    "            # annotator_counts_task.append(annotator_counts_batch)\n",
    "            df_annotator.extend(df_annotator_row) \n",
    "            # df_majority_counts.extend(majority_counts)\n",
    "            # df_annotator_counts.append(annotator_counts_batch)\n",
    "            df_retain_task.extend(retain_samples_batch)\n",
    "            df_subtask_row = {}\n",
    "            df_subtask_row['task'] = task_name\n",
    "            df_subtask_row['test'] = test_name\n",
    "            df_subtask_row['subtest'] = subtest_name\n",
    "            df_subtask_row['total'] = total_count_batch\n",
    "            df_subtask_row['retain'] = retain_count_batch\n",
    "            df_subtask.append(df_subtask_row)\n",
    "        # print('Fleiss kappa for', key, np.mean(kappa_task))\n",
    "        # print('Krippendorff alpha for', key, np.mean(krippendorff_task))\n",
    "\n",
    "        # print('Success rate task with control', key, np.mean(success_rate_task))\n",
    "        # print('Success rate task without control', key, np.mean(success_rate_task_no_control))\n",
    "        # print('Success rate task control only', key, np.mean(success_rate_task_control_only))\n",
    "\n",
    "        df_task_row = {}\n",
    "        df_task_row['task'] = key\n",
    "        df_task_row['kappa'] = np.mean(kappa_task)\n",
    "        df_task_row['krippendorff'] = np.mean(krippendorff_task)\n",
    "\n",
    "        df_task_row['success_rate_with_control'] = np.mean(success_rate_task)\n",
    "        df_task_row['success_rate_without_control'] = np.mean(success_rate_task_no_control)\n",
    "        df_task_row['success_rate_control_only'] = np.mean(success_rate_task_control_only)\n",
    "        df_task_row['success_rate_gpt4'] = np.mean(success_rate_gpt4_task)\n",
    "        df_task_row['majority_rate'] = np.mean(majority_rate_task)\n",
    "        # df_task_row['annotator_counts'] = sum(annotator_counts_task)\n",
    "        # print('annotator count',df_annotator_counts)\n",
    "        df_task.append(df_task_row)\n",
    "    df_task = pd.DataFrame(data = df_task)\n",
    "    df_subtask = pd.DataFrame(data = df_subtask)\n",
    "    df_annotator = pd.DataFrame(data = df_annotator)\n",
    "    # print(df_retain_task)\n",
    "    df_retain_task = pd.DataFrame(data = df_retain_task)\n",
    "    return df_task, df_annotator, df_retain_task, df_subtask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_task, df_annotator, df_retain_task, df_subtask = calculate_all(filter_list = None, filter = False, hit_function = 'majority', df_performance = pd.DataFrame())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_task = df_task.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_task = df_task.drop(columns=['success_rate_gpt4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_annotator_group = df_annotator.groupby('annotator_id').mean().reset_index()\n",
    "df_annotator_group['success_rate_control_only'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_retain_task['original_question'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_retain_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialog_label_mapping = {0:'No', 1: 'Yes', 2: 'Hard to say'}\n",
    "sentiment_label_mapping = {0: 'Negative', 1: 'Positive'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for i, row in df_retain_task.iterrows():\n",
    "    task  = row['task']\n",
    "    test = row['test']\n",
    "    if task == 'coreference_resolution':\n",
    "        task_dir = 'coref'\n",
    "    elif task == 'dialog_contradiction_detection':\n",
    "        task_dir = 'dialogue'\n",
    "    elif task == 'sentiment_analysis':\n",
    "        task_dir = 'sa'\n",
    "    file_to_load = '../data_for_phase2/' + task_dir + '/' + test + '.json'\n",
    "    df_original = pd.read_json(file_to_load)\n",
    "    # print(row['original_question'])\n",
    "    if row['original_question'].get('index_in_phase1_annotated_data')!=None:\n",
    "        index = row['original_question']['index_in_phase1_annotated_data']\n",
    "        # print(df_original.iloc[index])\n",
    "        # if df_original.iloc[index].get('original_label')!=None:\n",
    "        original_label = df_original.iloc[index]['original_label']\n",
    "        candidates = df_original.iloc[index]['original_candidates']\n",
    "        # print(candidates)\n",
    "        original_label = candidates[original_label]\n",
    "\n",
    "        # else:\n",
    "            # original_label = df_original.iloc[index]['label']\n",
    "            \n",
    "    elif row['original_question'].get('index_in_original_testset')!=None:\n",
    "        index = row['original_question']['index_in_original_testset']\n",
    "        # print(index)\n",
    "        # print(df_original)\n",
    "        original_label = df_original.loc[df_original[0] == index][1].values[0]['label']\n",
    "        original_label = dialog_label_mapping[original_label]\n",
    "        # print()\n",
    "        # print(original_label)\n",
    "    else:\n",
    "        # print(row['original_question'])\n",
    "        index = row['original_question']['idx']\n",
    "        original_label = df_original.loc[df_original['idx'] == index]['label'].values[0]\n",
    "        original_label = sentiment_label_mapping[original_label]\n",
    "\n",
    "        # print(original_label)\n",
    "    row['original_label'] = original_label\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_phase2  = pd.DataFrame(data = rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_phase2[df_phase2['task'] == 'coreference_resolution']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_phase2['label_change'] = (df_phase2['label'] != df_phase2['original_label']).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_phase2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_phase2[df_phase2['task'] == 'coreference_resolution']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_change_rate = (df_phase2.groupby(['task', 'test'])['label_change']\n",
    "                    .mean()  # Calculate mean of label_change (will give us the rate)\n",
    "                    .multiply(100)  # Convert to percentage\n",
    "                    .round(2)  # Round to 2 decimal places\n",
    "                    .reset_index())  # Convert from Series to DataFrame\n",
    "\n",
    "# Display the results\n",
    "print(\"Label Change Rate (%) by Task and Test:\")\n",
    "print(label_change_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate label change statistics for each group\n",
    "label_change_stats = (df_phase2.groupby(['task', 'test'])\n",
    "                     .agg({\n",
    "                         'label_change': ['size','sum', 'mean' ]  # mean for rate, sum for mismatches, size for total\n",
    "                     })\n",
    "                     .round(4))\n",
    "\n",
    "# Flatten column names and reset index\n",
    "label_change_stats.columns = ['total_samples', 'samples_with_label_change' ,'change_rate' ]\n",
    "label_change_stats = label_change_stats.reset_index()\n",
    "\n",
    "# Convert rate to percentage\n",
    "label_change_stats['change_rate'] = label_change_stats['change_rate'] * 100\n",
    "\n",
    "# Sort by change rate in descending order (optional)\n",
    "label_change_stats = label_change_stats.sort_values('change_rate', ascending=False)\n",
    "\n",
    "print(\"Label Change Statistics by Task and Test:\")\n",
    "print(label_change_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_change_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_annotators = list(df_annotator_group.loc[df_annotator_group['success_rate_control_only'] < 0.5]['annotator_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(filter_annotators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_task_filter, df_annotator_filter, df_retain_task_filter, df_subtask_filter = calculate_all(filter_list = filter_annotators, filter = True, hit_function = 'majority', df_performance= df_annotator_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "rows_coref = []\n",
    "rows_sentiment = []\n",
    "rows_dialog = []\n",
    "\n",
    "for i, row in df_retain_task_filter.iterrows():\n",
    "    task  = row['task']\n",
    "    test = row['test']\n",
    "    if task == 'coreference_resolution':\n",
    "        task_dir = 'coref'\n",
    "    elif task == 'dialog_contradiction_detection':\n",
    "        task_dir = 'dialogue'\n",
    "    elif task == 'sentiment_analysis':\n",
    "        task_dir = 'sa'\n",
    "    file_to_load = '../data_for_phase2/' + task_dir + '/' + test + '.json'\n",
    "    df_original = pd.read_json(file_to_load)\n",
    "    # print(row['original_question'])\n",
    "    if row['original_question'].get('index_in_phase1_annotated_data')!=None:\n",
    "        index = row['original_question']['index_in_phase1_annotated_data']\n",
    "        # print(df_original.iloc[index])\n",
    "        # if df_original.iloc[index].get('original_label')!=None:\n",
    "        original_label = df_original.iloc[index]['original_label']\n",
    "        candidates = df_original.iloc[index]['original_candidates']\n",
    "        # print(candidates)\n",
    "        original_label = candidates[original_label]\n",
    "        original_row = df_original.iloc[index]\n",
    "        original_row['modified_label'] = row['label']\n",
    "        original_row['test'] = row['test']\n",
    "        rows_coref.append(original_row)\n",
    "        # else:\n",
    "            # original_label = df_original.iloc[index]['label']\n",
    "            \n",
    "    elif row['original_question'].get('index_in_original_testset')!=None:\n",
    "        index = row['original_question']['index_in_original_testset']\n",
    "        # print(index)\n",
    "        # print(df_original)\n",
    "        original_label = df_original.loc[df_original[0] == index][1].values[0]['label']\n",
    "        original_label = dialog_label_mapping[original_label]\n",
    "        original_row = df_original.loc[df_original[0] == index][1].values[0]\n",
    "        # print()\n",
    "        original_row['modified_label'] = row['label']\n",
    "        original_row['test'] = row['test']\n",
    "\n",
    "        # print(original_label)\n",
    "        rows_dialog.append(original_row)\n",
    "    else:\n",
    "        # print(row['original_question'])\n",
    "        index = row['original_question']['idx']\n",
    "        original_label = df_original.loc[df_original['idx'] == index]['label'].values[0]\n",
    "        original_label = sentiment_label_mapping[original_label]\n",
    "        original_row = df_original.loc[df_original['idx'] == index]\n",
    "        original_row['modified_label'] = row['label']\n",
    "        original_row['test'] = row['test']\n",
    "        for i, item in original_row.iterrows():\n",
    "            print(item)\n",
    "            rows_sentiment.append(item)\n",
    "        \n",
    "        # print(original_label)\n",
    "    row['original_label'] = original_label\n",
    "\n",
    "    \n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_coref = pd.DataFrame(data = rows_coref)\n",
    "df_coref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentiment = pd.DataFrame(data = rows_sentiment)\n",
    "df_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dialog = pd.DataFrame(data = rows_dialog)\n",
    "df_dialog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_phase2  = pd.DataFrame(data = rows)\n",
    "df_phase2['label_change'] = (df_phase2['label'] != df_phase2['original_label']).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_change_stats = (df_phase2.groupby(['task', 'test'])\n",
    "                     .agg({\n",
    "                         'label_change': ['size','sum', 'mean' ]  # mean for rate, sum for mismatches, size for total\n",
    "                     })\n",
    "                     .round(4))\n",
    "\n",
    "# Flatten column names and reset index\n",
    "label_change_stats.columns = ['total_samples', 'samples_with_label_change' ,'change_rate' ]\n",
    "label_change_stats = label_change_stats.reset_index()\n",
    "\n",
    "# Convert rate to percentage\n",
    "label_change_stats['change_rate'] = label_change_stats['change_rate'] * 100\n",
    "\n",
    "# Sort by change rate in descending order (optional)\n",
    "label_change_stats = label_change_stats.sort_values('change_rate', ascending=False)\n",
    "\n",
    "print(\"Label Change Statistics by Task and Test:\")\n",
    "print(label_change_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_change_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_annotator_group = df_annotator.groupby('annotator_id').mean().reset_index()\n",
    "df_annotator_group['success_rate_control_only'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_annotators = list(df_annotator_group.loc[df_annotator_group['success_rate_control_only'] < 0.5]['annotator_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subtask_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_retain_task.to_csv('df_data_phase2.csv',index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_phase2['task']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_phase2.to_csv('df_data_phase2.csv',index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_phase2_coref = df_phase2[df_phase2['task'] == 'coreference_resolution']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_phase2_coref['original_question'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for i, row in df_phase2_coref.iterrows():\n",
    "    test = row['test']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_coref_task = df_coref.groupby('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for group, frame in df_coref_task:\n",
    "    to_write = '../data_after_phase2/coref/' + group + '.json'\n",
    "    obj = frame.to_dict(orient='records')\n",
    "    print(obj)\n",
    "    with open(to_write,'w') as f:\n",
    "        json.dump(obj, f)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentiment_task = df_sentiment.groupby('test')\n",
    "for group, frame in df_sentiment_task:\n",
    "    to_write = '../data_after_phase2/sa/' + group + '.json'\n",
    "    obj = frame.to_dict(orient='records')\n",
    "    print(obj)\n",
    "    with open(to_write,'w') as f:\n",
    "        json.dump(obj, f)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dialog_task = df_dialog.groupby('test')\n",
    "for group, frame in df_dialog_task:\n",
    "    to_write = '../data_after_phase2/dialogue/' + group + '.json'\n",
    "    obj = frame.to_dict(orient='records')\n",
    "    print(obj)\n",
    "    with open(to_write,'w') as f:\n",
    "        json.dump(obj, f)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
