{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import dspy\n",
    "import openai\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import ast\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "openai.organization = os.getenv('OPENAI_ORGANIZATION')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = dspy.LM('openai/gpt-4o', temperature=0, max_tokens=1023)\n",
    "dspy.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = pd.read_json('../preprocessing/train_dev_test_data/ner/fewnerd_sample_test.json', encoding_errors='replace')\n",
    "ds = ds.to_dict('records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_space(text):\n",
    "    \"\"\"Clean up spacing and formatting in dialogue text.\"\"\"\n",
    "    lines = text.split('\\n')\n",
    "    cleaned_lines = []\n",
    "    \n",
    "    for line in lines:\n",
    "        # Remove multiple spaces\n",
    "        cleaned = ' '.join(line.split())\n",
    "        \n",
    "        # Fix spacing around punctuation\n",
    "        cleaned = re.sub(r'\\s+([.,!?:;])', r'\\1', cleaned)\n",
    "        cleaned = re.sub(r'([.,!?:;])\\s+', r'\\1 ', cleaned)\n",
    "        \n",
    "        # Fix contractions\n",
    "        cleaned = re.sub(r'\\s*\\'\\s*s\\b', \"'s\", cleaned)\n",
    "        cleaned = re.sub(r'\\s*n\\s*\\'\\s*t\\b', \"n't\", cleaned)\n",
    "        cleaned = re.sub(r'\\s*\\'\\s*ve\\b', \"'ve\", cleaned)\n",
    "        cleaned = re.sub(r'\\s*\\'\\s*re\\b', \"'re\", cleaned)\n",
    "        cleaned = re.sub(r'\\s*\\'\\s*ll\\b', \"'ll\", cleaned)\n",
    "        cleaned = re.sub(r'\\s*\\'\\s*d\\b', \"'d\", cleaned)\n",
    "        cleaned = re.sub(r'\\s*\\'\\s*m\\b', \"'m\", cleaned)\n",
    "        \n",
    "        # Fix spaces around parentheses\n",
    "        cleaned = re.sub(r'\\(\\s+', '(', cleaned)\n",
    "        cleaned = re.sub(r'\\s+\\)', ')', cleaned)\n",
    "        \n",
    "        # Remove leading/trailing whitespace\n",
    "        cleaned = cleaned.strip()\n",
    "        \n",
    "        cleaned_lines.append(cleaned)\n",
    "        \n",
    "    return '\\n'.join(cleaned_lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    dspy.Example({ \n",
    "                  \"text\" : r[\"text\"], \n",
    "                  \"label\": str(r['label'])\n",
    "                }\n",
    "                  ).with_inputs(\"text\")\n",
    "    \n",
    "    for r in ds\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = examples[0]\n",
    "for k, v in example.items():\n",
    "    print(f\"\\n{k.upper()}:\\n\")\n",
    "    print(v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_f1_ent(gold_entities, predicted_entities):\n",
    "    \"\"\"\n",
    "    Calculates the F1 score given the true labels and predicted labels.\n",
    "    \"\"\"\n",
    "    # print(\"Input types:\")\n",
    "    # print(f\"gold_entities type: {type(gold_entities)}\")\n",
    "    # print(f\"predicted_entities type: {type(predicted_entities)}\")\n",
    "    \n",
    "    if predicted_entities is None:\n",
    "        return 0.0, 0.0, 0.0\n",
    "\n",
    "    true_entities = {}\n",
    "    pred_entities = {}\n",
    "    \n",
    "    # Convert to empty list if NaN\n",
    "    def handle_nan(entities):\n",
    "        # print(f\"Handling NaN for: {type(entities)}\")\n",
    "        # If it's already a list, return it as is\n",
    "        if isinstance(entities, list):\n",
    "            return entities\n",
    "        # Handle pandas/numpy types\n",
    "        if isinstance(entities, (pd.Series, np.ndarray)):\n",
    "            nan_check = pd.isna(entities)\n",
    "            if isinstance(nan_check, (pd.Series, np.ndarray)):\n",
    "                if nan_check.any():\n",
    "                    return \"[]\"\n",
    "            elif nan_check:\n",
    "                return \"[]\"\n",
    "        # Handle single values\n",
    "        elif pd.isna(entities):\n",
    "            return \"[]\"\n",
    "        return entities\n",
    "\n",
    "    # Handle NaN cases\n",
    "    gold_entities = handle_nan(gold_entities)\n",
    "    predicted_entities = handle_nan(predicted_entities)\n",
    "            \n",
    "    # Parse strings if needed\n",
    "    if isinstance(gold_entities, str):\n",
    "        gold_entities = ast.literal_eval(gold_entities)\n",
    "    if isinstance(predicted_entities, str):\n",
    "        predicted_entities = ast.literal_eval(predicted_entities)\n",
    "\n",
    "    # Process gold entities\n",
    "    # print(gold_entities)\n",
    "    for entity in gold_entities:\n",
    "        # print(entity)\n",
    "        if isinstance(entity, str):\n",
    "            entity = ast.literal_eval(entity)\n",
    "        if entity.get('text') is not None:\n",
    "            true_entities[entity['text']] = entity['value']\n",
    "        else:\n",
    "            for key, value in entity.items():\n",
    "                true_entities[key] = value\n",
    "    \n",
    "    # Process predicted entities\n",
    "    for entity in predicted_entities:\n",
    "        if isinstance(entity, str):\n",
    "            entity = ast.literal_eval(entity)\n",
    "        if entity.get('text') is not None:  \n",
    "            pred_entities[entity['text']] = entity['value']\n",
    "        else:\n",
    "            for key, value in entity.items():\n",
    "                pred_entities[key] = value\n",
    "\n",
    "    # Calculate metrics\n",
    "    true_positives = sum(1 for text in true_entities if text in pred_entities and true_entities[text] == pred_entities[text])\n",
    "    false_positives = sum(1 for text in pred_entities if text not in true_entities)\n",
    "    false_negatives = sum(1 for text in true_entities if text not in pred_entities)\n",
    "\n",
    "    if true_positives == 0:\n",
    "        return 0.0, 0.0, 0.0\n",
    "\n",
    "    precision = true_positives / (true_positives + false_positives)\n",
    "    recall = true_positives / (true_positives + false_negatives)\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    return precision, recall, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_prediction(pred):\n",
    "    matches = re.findall(r\"\\[\\{.*\\}\\]\", pred)\n",
    "    # print(matches)\n",
    "    parsed_answer = matches[-1] if matches else \"\"\n",
    "    if parsed_answer == \"\":\n",
    "        return {}\n",
    "    parsed_answer = ast.literal_eval(parsed_answer)\n",
    "    return parsed_answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_metric(true, prediction, trace=None):\n",
    "    pred = prediction.label\n",
    "    \n",
    "    matches = re.findall(r\"\\[\\{.*\\}\\]\", pred)\n",
    "    # print(matches)\n",
    "    parsed_answer = matches[-1] if matches else \"\"\n",
    "    if parsed_answer == \"\":\n",
    "        return 0.0\n",
    "    parsed_answer = ast.literal_eval(parsed_answer)\n",
    "    # print(type(parsed_answer))\n",
    "    gold_entities = ast.literal_eval(true.label)\n",
    "    # print(parsed_answer)\n",
    "    precision, recall, f1_score = calculate_f1_ent(gold_entities=gold_entities, predicted_entities= parsed_answer)\n",
    "    return f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.evaluate import Evaluate\n",
    "\n",
    "# evaluate = Evaluate(devset= examples, metric=eval_metric, num_threads=1, display_progress=True, display_table=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ent(dspy.Signature):\n",
    "    \"\"\"Extract named entities from the text. Possible entity type: ART, BUILDING, EVENT, LOCATION, ORGANIZATION, OTHER, PERSON, PRODUCT\"\"\"\n",
    "    text = dspy.InputField()\n",
    "    label = dspy.OutputField(desc=\"The list of named entities in the text: [{'text': the text span, 'value': the entity label},].\", prefix = 'Entities:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleEnt(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.prog = dspy.Predict(Ent)\n",
    "\n",
    "    def forward(self, text):\n",
    "\n",
    "        return self.prog(text = text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_ent = SimpleEnt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = simple_ent(text=example.text)\n",
    "print(\"\\nTEXT:\\n\")\n",
    "print(example.text)\n",
    "\n",
    "print(\"\\nANSWER:\\n\")\n",
    "print(example.label)\n",
    "print(\"\\nPREDICTION:\\n\")\n",
    "print(pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metric(example, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.inspect_history(\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate = Evaluate(devset= examples, metric=eval_metric, num_threads=6, display_progress=True, display_table=10, return_outputs= True, return_all_scores=True)\n",
    "results = evaluate(simple_ent)\n",
    "items = []\n",
    "for sample in results[1]:\n",
    "\n",
    "    item = {}\n",
    "    sentence = sample[0]['text']\n",
    "    label = sample[0]['label']\n",
    "    if sample[1] == {}:\n",
    "        pred = {}\n",
    "    else:\n",
    "        pred = sample[1]['label']\n",
    "    item['text'] = sentence\n",
    "    item['label'] = label\n",
    "    item['pred'] = pred\n",
    "    items.append(item)\n",
    "df_result = pd.DataFrame(data = items)\n",
    "df_result.to_csv('results/ner/gpt4o-0shot-ner.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_modified_set(ds, program):\n",
    "    examples = [\n",
    "    dspy.Example({ \n",
    "                  \"text\" : remove_space(r[\"modified_text\"]), \n",
    "                  \"label\": str(r['modified_label']),\n",
    "                  \"original_text\": remove_space(r['original_text']),\n",
    "                  \"original_label\": str(r['original_label']),\n",
    "                  \"index\": r['index'],\n",
    "                  \"type\": r['subtype'] if 'subtype' in r else None\n",
    "                #   \"original_label\": str(r['original_label'])\n",
    "                }\n",
    "                  ).with_inputs(\"text\")\n",
    "    \n",
    "    for r in ds\n",
    "    ]\n",
    "    evaluate = Evaluate(devset= examples, metric=eval_metric, num_threads=6, display_progress=True, display_table=1, return_outputs= True, return_all_scores=True, provide_traceback=True)\n",
    "    results = evaluate(program)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ent(dspy.Signature):\n",
    "    \"\"\"Extract named entities from the text. Possible entity type: ART, BUILDING, EVENT, LOCATION, ORGANIZATION, OTHER, PERSON, PRODUCT\"\"\"\n",
    "    text = dspy.InputField()\n",
    "    label = dspy.OutputField(desc=\"The list of named entities in the text: [{\\\"text\\\": the text span, \\\"value\\\": the entity label},].\", prefix = 'Entities:')\n",
    "\n",
    "class SimpleEnt(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.prog = dspy.Predict(Ent)\n",
    "\n",
    "    def forward(self, text):\n",
    "\n",
    "        return self.prog(text = text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import difflib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure GPT-4 as the language model\n",
    "original_pred_ds = pd.read_csv('results/ner/gpt4o-0shot-ner.csv', index_col=False)\n",
    "original_pred_ds['text'] = original_pred_ds['text'].apply(lambda x: remove_space(x.encode('utf-8').decode('unicode-escape')))  # Replace 'your_function' with the actual function\n",
    "\n",
    "# Get specific json files we want to process\n",
    "json_files = glob.glob('../data/modified_data/ner/*_100.json')\n",
    "\n",
    "for json_file in json_files:\n",
    "    # Load the json file\n",
    "    # if 'dialectal' not in json_file:\n",
    "    #     continue\n",
    "    print(json_file)\n",
    "    with open(json_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    simple_ent = SimpleEnt()\n",
    "    results = evaluate_modified_set(data,simple_ent)\n",
    "\n",
    "    # Convert results to dataframe\n",
    "    items = []\n",
    "    for sample in results[1]:\n",
    "        if 'Lawrence' in sample[0]['text']:\n",
    "            print(sample)\n",
    "        item = {}\n",
    "        # print(sample)\n",
    "        if sample[1] == {}:\n",
    "            pred = '[]'    \n",
    "        else:\n",
    "            pred = sample[1]['label']\n",
    "        sentence = sample[0]['text']\n",
    "        label = sample[0]['label'] \n",
    "        item['text'] = sentence\n",
    "        # print(label)\n",
    "        # item['modified_label'] = [{entity['text']: entity['value']} for entity in label]\n",
    "        item['modified_label'] = label\n",
    "        pred = extract_prediction(pred)\n",
    "        item['modified_pred'] = pred\n",
    "        item['modified_pred'] = [{entity['text']: entity['value']} for entity in pred]\n",
    "        original_text = sample[0]['original_text'].encode('utf-8').decode('unicode-escape')\n",
    "        # print(sample)\n",
    "        item['original_text'] = original_text\n",
    "        index = sample[0]['index']\n",
    "        # Find the best match for the original_text in the original_pred_ds['text'] using difflib\n",
    "        matches = original_pred_ds['pred'].iloc[index] if index < len(original_pred_ds) else []\n",
    "        item['original_pred'] = matches if matches else '[]'\n",
    "        # item['original_pred'] = original_pred_ds.loc[original_pred_ds['text'] == matches[0], 'pred'].values[0] if matches else None\n",
    "        item['original_label'] = sample[0]['original_label']\n",
    "        \n",
    "        # Check if original_label is NaN and assign modified_label if it is\n",
    "        if pd.isna(item['original_label']):\n",
    "            item['original_label'] = item['modified_label']\n",
    "        item['type'] = sample[0]['type']\n",
    "        items.append(item)\n",
    "    \n",
    "    df_result = pd.DataFrame(data=items)\n",
    "    \n",
    "    # Save results with filename based on input json\n",
    "    output_filename = f\"results/ner/gpt4o-0shot-{json_file.split('/')[-1].replace('.json', '')}_new.csv\"\n",
    "    df_result.to_csv(output_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_files = glob.glob('results/ner/gpt4o-0shot-*_100.csv')\n",
    "\n",
    "aggregated_results = []\n",
    "\n",
    "def convert_string_to_entities(entity_str):\n",
    "    \"\"\"Convert string representation of entities to proper format\"\"\"\n",
    "    if isinstance(entity_str, str):\n",
    "        try:\n",
    "            # Convert string to list of dicts\n",
    "            entities = ast.literal_eval(entity_str)\n",
    "            # Handle nested lists by flattening\n",
    "            if isinstance(entities, list):\n",
    "                # Handle double nested lists\n",
    "                if len(entities) > 0 and isinstance(entities[0], list):\n",
    "                    entities = entities[0]\n",
    "                # Handle list of dicts with text/value format\n",
    "                if len(entities) > 0 and isinstance(entities[0], dict):\n",
    "                    # Handle format with text/value keys\n",
    "                    if 'text' in entities[0]:\n",
    "                        return entities\n",
    "                    # Handle format with single key-value pair\n",
    "                    if len(entities[0]) == 1:\n",
    "                        converted = []\n",
    "                        for e in entities:\n",
    "                            for text, value in e.items():\n",
    "                                converted.append({'text': text, 'value': value})\n",
    "                        return converted\n",
    "                    # Handle format with multiple key-value pairs\n",
    "                    converted = []\n",
    "                    for e in entities:\n",
    "                        for text, value in e.items():\n",
    "                            if isinstance(value, str):\n",
    "                                converted.append({'text': text, 'value': value})\n",
    "                    return converted\n",
    "            return entities\n",
    "        except:\n",
    "            return []\n",
    "    return entity_str\n",
    "\n",
    "for file in result_files:\n",
    "    # Extract modification type from filename\n",
    "    mod_type = file.split('-')[-1].replace('.csv','')\n",
    "    print(mod_type)\n",
    "    # Read results file\n",
    "    df = pd.read_csv(file)\n",
    "\n",
    "    # Collect all predictions and labels for micro F1 calculation\n",
    "    all_original_labels = []\n",
    "    all_original_preds = []\n",
    "    all_modified_labels = []\n",
    "    all_modified_preds = []\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        # Convert string representations to proper format\n",
    "        original_label = convert_string_to_entities(row['original_label'])\n",
    "        original_pred = convert_string_to_entities(row['original_pred'])\n",
    "        modified_label = convert_string_to_entities(row['modified_label'])\n",
    "        modified_pred = convert_string_to_entities(row['modified_pred'])\n",
    "\n",
    "        # Append to combined lists\n",
    "        all_original_labels.extend(original_label)\n",
    "        all_original_preds.extend(original_pred)\n",
    "        all_modified_labels.extend(modified_label)\n",
    "        all_modified_preds.extend(modified_pred)\n",
    "\n",
    "    # Calculate micro F1 scores using calculate_f1_ent\n",
    "    original_precision, original_recall, original_f1 = calculate_f1_ent(all_original_labels, all_original_preds)\n",
    "    modified_precision, modified_recall, modified_f1 = calculate_f1_ent(all_modified_labels, all_modified_preds)\n",
    "    \n",
    "    # Calculate the difference between original and modified F1 scores\n",
    "    difference = -round(original_f1 - modified_f1, 2)\n",
    "    \n",
    "    # Calculate percentage difference with respect to original F1\n",
    "    pct_difference = -round((original_f1 - modified_f1) / original_f1 * 100, 2) if original_f1 != 0 else 0\n",
    "    \n",
    "    # Perform t-test between original and modified predictions\n",
    "    t_stat, p_value = stats.ttest_ind(\n",
    "        (df['original_pred'] == df['original_label']).astype(float),\n",
    "        (df['modified_pred'] == df['modified_label']).astype(float)\n",
    "    )\n",
    "    \n",
    "    aggregated_results.append({\n",
    "        'task': 'named_entity_recognition',\n",
    "        'modification': mod_type,\n",
    "        'original_res': round(original_f1 , 2),  # Convert to percentage\n",
    "        'modified_res': round(modified_f1 , 2),  # Convert to percentage\n",
    "        'difference': modified_f1 - original_f1,\n",
    "        'pct_difference': pct_difference,\n",
    "        'p_value': p_value,\n",
    "        'original_precision': round(original_precision , 2),  # Convert to percentage\n",
    "        'original_recall': round(original_recall , 2),  # Convert to percentage\n",
    "        'modified_precision': round(modified_precision , 2),  # Convert to percentage\n",
    "        'modified_recall': round(modified_recall , 2)  # Convert to percentage\n",
    "    })\n",
    "\n",
    "# Create final results dataframe\n",
    "results_df = pd.DataFrame(aggregated_results)\n",
    "\n",
    "# Sort the results based on modification_name\n",
    "modification_name = ['temporal_bias_100', 'geographical_bias_100','length_bias_100', 'typo_bias_100', 'capitalization_100', 'punctuation_100', 'derivation_100', 'compound_word_100','active_to_passive_100','grammatical_role_100', 'coordinating_conjunction_100', 'concept_replacement_100','negation_100','discourse_100','sentiment_100','casual_100', 'dialectal_100']\n",
    "results_df['modification'] = pd.Categorical(results_df['modification'], categories=modification_name, ordered=True)\n",
    "results_df = results_df.sort_values(by='modification')\n",
    "\n",
    "# Calculate averages across all modifications\n",
    "avg_original = results_df['original_res'].mean()\n",
    "avg_modified = results_df['modified_res'].mean()\n",
    "# Calculate average difference directly from original and modified means\n",
    "# We can't take mean of differences because individual differences may cancel each other out\n",
    "# and not reflect the true overall performance change between original and modified models\n",
    "avg_difference =  avg_original - avg_modified\n",
    "avg_pct_difference = results_df['pct_difference'].mean()\n",
    "avg_orig_precision = results_df['original_precision'].mean()\n",
    "avg_orig_recall = results_df['original_recall'].mean()\n",
    "avg_mod_precision = results_df['modified_precision'].mean()\n",
    "avg_mod_recall = results_df['modified_recall'].mean()\n",
    "\n",
    "# Add averages as a new row\n",
    "results_df.loc[len(results_df)] = {\n",
    "    'task': 'named_entity_recognition',\n",
    "    'modification': 'average',\n",
    "    'original_res': round(avg_original, 2),\n",
    "    'modified_res': round(avg_modified, 2),\n",
    "    'difference': -round(avg_difference, 2),\n",
    "    'pct_difference': round(avg_pct_difference, 2),\n",
    "    'p_value': None,\n",
    "    'original_precision': round(avg_orig_precision, 2),\n",
    "    'original_recall': round(avg_orig_recall, 2),\n",
    "    'modified_precision': round(avg_mod_precision, 2),\n",
    "    'modified_recall': round(avg_mod_recall, 2)\n",
    "}\n",
    "\n",
    "print(\"\\n\")\n",
    "results_df.to_csv('results/ner/gpt4o-DP.csv')\n",
    "\n",
    "# Apply styling to highlight rows where original_res > modified_res and significant p-values\n",
    "def highlight_drops_and_significance(row):\n",
    "    colors = [''] * len(row)\n",
    "    if row['original_res'] > row['modified_res']:\n",
    "        colors = ['background-color: red'] * len(row)\n",
    "        # If p-value < 0.05, add bold text\n",
    "        if 'p_value' in row and row['p_value'] is not None and row['p_value'] < 0.05:\n",
    "            colors = ['background-color: red; font-weight: bold'] * len(row)\n",
    "    return colors\n",
    "\n",
    "results_df.round(2).style.apply(highlight_drops_and_significance, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
