{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import dspy\n",
    "import openai\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import json\n",
    "import random\n",
    "from dotenv import load_dotenv\n",
    "import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "openai.organization = os.getenv('OPENAI_ORGANIZATION')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = dspy.LM('openrouter/anthropic/claude-3.5-sonnet', temperature=0, max_tokens=1024)\n",
    "dspy.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds = pd.read_json('data/sa/test.json', lines= True)\n",
    "# ds = ds.to_dict('records')\n",
    "# ds = load_dataset('stanfordnlp/sst2')['validation']\n",
    "ds = pd.read_json('../preprocessing/train_dev_test_data/sa/test.json')\n",
    "ds = ds.to_dict('records')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_space(text):\n",
    "    \"\"\"Clean up spacing and formatting in dialogue text.\"\"\"\n",
    "    lines = text.split('\\n')\n",
    "    cleaned_lines = []\n",
    "    \n",
    "    for line in lines:\n",
    "        # Remove multiple spaces\n",
    "        cleaned = ' '.join(line.split())\n",
    "        \n",
    "        # Fix spacing around punctuation\n",
    "        cleaned = re.sub(r'\\s+([.,!?:;])', r'\\1', cleaned)\n",
    "        cleaned = re.sub(r'([.,!?:;])\\s+', r'\\1 ', cleaned)\n",
    "        \n",
    "        # Fix contractions\n",
    "        cleaned = re.sub(r'\\s*\\'\\s*s\\b', \"'s\", cleaned)\n",
    "        cleaned = re.sub(r'\\s*n\\s*\\'\\s*t\\b', \"n't\", cleaned)\n",
    "        cleaned = re.sub(r'\\s*\\'\\s*ve\\b', \"'ve\", cleaned)\n",
    "        cleaned = re.sub(r'\\s*\\'\\s*re\\b', \"'re\", cleaned)\n",
    "        cleaned = re.sub(r'\\s*\\'\\s*ll\\b', \"'ll\", cleaned)\n",
    "        cleaned = re.sub(r'\\s*\\'\\s*d\\b', \"'d\", cleaned)\n",
    "        cleaned = re.sub(r'\\s*\\'\\s*m\\b', \"'m\", cleaned)\n",
    "        \n",
    "        # Fix spaces around parentheses\n",
    "        cleaned = re.sub(r'\\(\\s+', '(', cleaned)\n",
    "        cleaned = re.sub(r'\\s+\\)', ')', cleaned)\n",
    "        \n",
    "        # Remove leading/trailing whitespace\n",
    "        cleaned = cleaned.strip()\n",
    "        \n",
    "        cleaned_lines.append(cleaned)\n",
    "        \n",
    "    return '\\n'.join(cleaned_lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    dspy.Example({ \n",
    "                  \"text\": remove_space(r[\"sentence\"]), \n",
    "                  \"label\": r[\"label\"]}\n",
    "                  \n",
    "                  ).with_inputs(\"text\") \n",
    "    for r in ds\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = examples[835]\n",
    "for k, v in example.items():\n",
    "    print(f\"\\n{k.upper()}:\\n\")\n",
    "    print(v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_prediction(text):\n",
    "    matches = re.findall(r'\\b[0-2]\\b', text)\n",
    "    # print(matches)\n",
    "    parsed_answer = matches[-1] if matches else \"\"\n",
    "    return parsed_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_metric(true, prediction, trace=None):\n",
    "    pred = prediction.label\n",
    "    matches = re.findall(r'\\b[0-2]\\b', pred)\n",
    "    parsed_answer = matches[-1] if matches else \"\"\n",
    "    # print(parsed_answer)\n",
    "    return parsed_answer == str(true.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the original test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.evaluate import Evaluate\n",
    "\n",
    "evaluate = Evaluate(devset= examples, metric=eval_metric, num_threads=6, display_progress=True, display_table=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentiment(dspy.Signature):\n",
    "    \"\"\"Classify sentiment of the given text. Answer with 1 for positive, 0 for negative.\"\"\"\n",
    "    text = dspy.InputField()\n",
    "    label = dspy.OutputField(prefix = 'Answer:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleSentiment(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.prog = dspy.Predict(Sentiment)\n",
    "\n",
    "    def forward(self, text):\n",
    "        return self.prog(text=text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_sentiment = SimpleSentiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = simple_sentiment(text=example.text)\n",
    "print(\"\\nQUESTION:\\n\")\n",
    "print(example.text)\n",
    "# print(\"\\nANSWER:\\n\")\n",
    "# print(example.label)\n",
    "print(\"\\nPREDICTION:\\n\")\n",
    "print(pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metric(example, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate = Evaluate(devset= examples, metric=eval_metric, num_threads=6, display_progress=True, display_table=10, return_outputs= True, return_all_scores=True)\n",
    "results = evaluate(simple_sentiment)\n",
    "items = []\n",
    "for sample in results[1]:\n",
    "    item = {}\n",
    "    sentence = sample[0]['text']\n",
    "    label = sample[0]['label']\n",
    "    pred = sample[1]['label']\n",
    "    item['text'] = sentence\n",
    "    item['label'] = label\n",
    "    item['pred'] = pred\n",
    "    items.append(item)\n",
    "df_result = pd.DataFrame(data = items)\n",
    "df_result.to_csv('results/sa/claude-3-5-sonnet-0shot-sst2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoTSentiment(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.prog = dspy.ChainOfThought(Sentiment)\n",
    "\n",
    "    def forward(self, text):\n",
    "        return self.prog(text=text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cot_sentiment = CoTSentiment()\n",
    "pred = cot_sentiment(text=example.text)\n",
    "print(\"\\nQUESTION:\\n\")\n",
    "print(example.text)\n",
    "# print(\"\\nANSWER:\\n\")\n",
    "# print(example.label)\n",
    "print(\"\\nPREDICTION:\\n\")\n",
    "print(pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.inspect_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate = Evaluate(devset= examples, metric=eval_metric, num_threads=6, display_progress=True, display_table=10, return_outputs= True, return_all_scores=True)\n",
    "results = evaluate(cot_sentiment)\n",
    "items = []\n",
    "for sample in results[1]:\n",
    "    item = {}\n",
    "    sentence = sample[0]['text']\n",
    "    label = sample[0]['label']\n",
    "    pred = sample[1]['label']\n",
    "    item['text'] = sentence\n",
    "    item['label'] = label\n",
    "    item['pred'] = pred\n",
    "    items.append(item)\n",
    "df_result = pd.DataFrame(data = items)\n",
    "df_result.to_csv('claude-3-5-sonnet-0shot-cot-sst2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate by modification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without label change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_modified_set(ds, program):\n",
    "    examples = [\n",
    "    dspy.Example({ \n",
    "                  \"text\" : remove_space(r['modified_text']), \n",
    "                  \"original_text\": remove_space(r['original_text']),\n",
    "                  \"label\": int(r['label']),\n",
    "                  \"modified_label\": int(r['label'])\n",
    "                }\n",
    "                  ).with_inputs(\"text\") \n",
    "    for r in ds\n",
    "    ]\n",
    "    evaluate = Evaluate(devset= examples, metric=eval_metric, num_threads=6, display_progress=True, display_table=1, return_outputs= True, return_all_scores=True)\n",
    "    results = evaluate(program)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentiment(dspy.Signature):\n",
    "    \"\"\"Classify sentiment of the given text. Answer with 1 for positive, 0 for negative.\"\"\"\n",
    "    text = dspy.InputField()\n",
    "    label = dspy.OutputField(prefix = 'Answer:')\n",
    "class SimpleSentiment(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.prog = dspy.Predict(Sentiment)\n",
    "\n",
    "    def forward(self, text):\n",
    "        return self.prog(text=text)\n",
    "simple_sentiment = SimpleSentiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Configure GPT-4 as the language model\n",
    "original_pred_ds = pd.read_csv('results/sa/claude-3-5-sonnet-0shot-sst2.csv', index_col=False)\n",
    "original_pred_ds['text'] = original_pred_ds['text'].apply(lambda x: remove_space(x).encode('utf-8').decode('unicode-escape'))\n",
    "\n",
    "# Get all json files in the specified directory\n",
    "json_files = glob.glob('../data/modified_data/sa/*_100.json')\n",
    "\n",
    "for json_file in json_files:\n",
    "    # Load the json file\n",
    "    print(json_file)\n",
    "    if 'grammatical_role' in json_file or 'negation' in json_file:\n",
    "        continue\n",
    "    with open(json_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    results = evaluate_modified_set(data,simple_sentiment)\n",
    "\n",
    "    \n",
    "    # Convert results to dataframe\n",
    "    items = []\n",
    "    for sample in results[1]:\n",
    "        item = {}\n",
    "        sentence = sample[0]['text']\n",
    "        label = sample[0]['label'] \n",
    "        pred = sample[1]['label']\n",
    "        item['text'] = sentence\n",
    "        item['modified_label'] = label\n",
    "        pred = extract_prediction(pred)\n",
    "        item['modified_pred'] = pred\n",
    "        # original_text = sample[0]['original_text']\n",
    "        original_text = sample[0]['original_text'].encode('utf-8').decode('unicode-escape')\n",
    "        # original_text = r\"{}\".format(original_text)\n",
    "        # print(original_text)\n",
    "        item['original_label'] = sample[0]['label']\n",
    "        item['original_text'] = original_text\n",
    "        item['original_pred'] = original_pred_ds.loc[original_pred_ds['text'] == original_text]['pred'].values[0]\n",
    "        items.append(item)\n",
    "    \n",
    "    df_result = pd.DataFrame(data=items)\n",
    "    \n",
    "    # Save results with filename based on input json\n",
    "    output_filename = f\"results/sa/claude-3-5-sonnet-0shot-{json_file.split('/')[-1].replace('.json', '')}.csv\"\n",
    "    df_result.to_csv(output_filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With label change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_modified_set(ds, program):\n",
    "    examples = [\n",
    "    dspy.Example({ \n",
    "                  \"text\" : remove_space(r['modified_text']), \n",
    "                  \"original_text\": remove_space(r['original_text']),\n",
    "                  \"label\": int(r['modified_label']) if r.get('modified_label') else int(r['label']),\n",
    "                  \"original_label\": int(r['label']),\n",
    "                  \"index\": r['index'],\n",
    "                  \"type\": r['type']\n",
    "                }\n",
    "                  ).with_inputs(\"text\") \n",
    "    for r in ds\n",
    "    ]\n",
    "    evaluate = Evaluate(devset= examples, metric=eval_metric, num_threads=6, display_progress=True, display_table=1, return_outputs= True, return_all_scores=True)\n",
    "    results = evaluate(program)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentiment(dspy.Signature):\n",
    "    \"\"\"Classify sentiment of the given text. Answer with 1 for positive, 0 for negative.\"\"\"\n",
    "    text = dspy.InputField()\n",
    "    label = dspy.OutputField(prefix = 'Answer:')\n",
    "class SimpleSentiment(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.prog = dspy.Predict(Sentiment)\n",
    "\n",
    "    def forward(self, text):\n",
    "        return self.prog(text=text)\n",
    "simple_sentiment = SimpleSentiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_pred_ds = pd.read_csv('results/sa/claude-3-5-sonnet-0shot-sst2.csv', index_col=False)\n",
    "original_pred_ds['text'] = original_pred_ds['text'].apply(lambda x: remove_space(x).encode('utf-8').decode('unicode-escape'))\n",
    "\n",
    "# Get all json files in the specified directory\n",
    "json_files = glob.glob('../data/modified_data/sa/*_100.json')\n",
    "\n",
    "for json_file in json_files:\n",
    "    # Load the json file\n",
    "    print(json_file)\n",
    "    if not any(x in json_file for x in ['negation']):\n",
    "        continue\n",
    "    with open(json_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    results = evaluate_modified_set(data,simple_sentiment)\n",
    "\n",
    "    \n",
    "    # Convert results to dataframe\n",
    "    items = []\n",
    "    for sample in results[1]:\n",
    "        item = {}\n",
    "        sentence = sample[0]['text']\n",
    "        pred = sample[1]['label']\n",
    "        item['text'] = sentence\n",
    "        item['modified_label'] = sample[0]['label'] \n",
    "        pred = extract_prediction(pred)\n",
    "        item['modified_pred'] = pred\n",
    "        # original_text = sample[0]['original_text']\n",
    "        # print(original_text)\n",
    "        original_text = sample[0]['original_text'].encode('utf-8').decode('unicode-escape')\n",
    "\n",
    "        item['original_label'] = sample[0]['original_label']\n",
    "        item['original_text'] = original_text\n",
    "        item['original_pred'] = original_pred_ds.iloc[sample[0]['index']]['pred']\n",
    "        item['type'] = sample[0]['type']\n",
    "        items.append(item)\n",
    "    \n",
    "    df_result = pd.DataFrame(data=items)\n",
    "    \n",
    "    \n",
    "    # Save results with filename based on input json\n",
    "    output_filename = f\"results/sa/claude-3-5-sonnet-0shot-{json_file.split('/')[-1].replace('.json', '')}.csv\"\n",
    "    df_result.to_csv(output_filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_files = glob.glob('results/sa/claude-3-5-sonnet-0shot-*_100.csv')\n",
    "\n",
    "aggregated_results = []\n",
    "\n",
    "for file in result_files:\n",
    "    # Extract modification type from filename\n",
    "    print(file)\n",
    "    mod_type = file.split('-')[-1].replace('.csv','')\n",
    "    \n",
    "    # Read results file\n",
    "    df = pd.read_csv(file)\n",
    "\n",
    "    # Calculate accuracies\n",
    "    original_correct = (df['original_pred'] == df['original_label']).sum()\n",
    "    modified_correct = (df['modified_pred'] == df['modified_label']).sum()\n",
    "    total = len(df)\n",
    "\n",
    "    original_acc = original_correct / total\n",
    "    modified_acc = modified_correct / total\n",
    "    \n",
    "    # Calculate the difference between original_res and modified_res\n",
    "    difference = -round(original_acc - modified_acc, 2)\n",
    "    \n",
    "    # Calculate percentage difference with respect to total samples\n",
    "    pct_difference = -round((original_correct - modified_correct) / original_correct * 100, 2)\n",
    "    \n",
    "    # Perform t-test between original and modified predictions\n",
    "    t_stat, p_value = stats.ttest_ind(\n",
    "        (df['original_pred'] == df['original_label']).astype(float),\n",
    "        (df['modified_pred'] == df['modified_label']).astype(float)\n",
    "    )\n",
    "    \n",
    "    aggregated_results.append({\n",
    "        'task': 'dialogue_contradiction_detection',\n",
    "        'modification': mod_type,\n",
    "        'original_res': round(original_acc, 2),\n",
    "        'modified_res': round(modified_acc, 2),\n",
    "        'difference': difference,  # Difference in accuracy\n",
    "        'pct_difference': pct_difference,  # Percentage difference relative to total samples\n",
    "        'p_value': p_value  # Add p-value from t-test\n",
    "    })\n",
    "\n",
    "# Create final results dataframe\n",
    "results_df = pd.DataFrame(aggregated_results)\n",
    "\n",
    "# Sort the results based on modification_name\n",
    "modification_name = ['temporal_bias_100', 'geographical_bias_100','length_bias_100', 'typo_bias_100', 'capitalization_100', 'punctuation_100', 'derivation_100', 'compound_word_100','active_to_passive_100','grammatical_role_100', 'coordinating_conjunction_100', 'concept_replacement_100','negation_100','discourse_100','sentiment_100','casual_100', 'dialectal_100']\n",
    "results_df['modification'] = pd.Categorical(results_df['modification'], categories=modification_name, ordered=True)\n",
    "results_df = results_df.sort_values(by='modification')\n",
    "\n",
    "# Calculate averages across all modifications\n",
    "avg_original = results_df['original_res'].mean()\n",
    "avg_modified = results_df['modified_res'].mean()\n",
    "avg_difference = avg_original - avg_modified\n",
    "avg_pct_difference = results_df['pct_difference'].mean()\n",
    "\n",
    "# Add averages as a new row\n",
    "results_df.loc[len(results_df)] = {\n",
    "    'task': 'dialogue_contradiction_detection',\n",
    "    'modification': 'average',\n",
    "    'original_res': round(avg_original, 2),\n",
    "    'modified_res': round(avg_modified, 2),\n",
    "    'difference': -round(avg_difference, 2),\n",
    "    'pct_difference': round(avg_pct_difference, 2),\n",
    "    'p_value': None  # No p-value for average row\n",
    "}\n",
    "\n",
    "print(\"\\n\")\n",
    "results_df.to_csv('results/sa/claude-3-5-sonnet-DP.csv')\n",
    "\n",
    "# Apply styling to highlight rows where original_res > modified_res and significant p-values\n",
    "def highlight_drops_and_significance(row):\n",
    "    colors = [''] * len(row)\n",
    "    if row['original_res'] > row['modified_res']:\n",
    "        colors = ['background-color: red'] * len(row)\n",
    "        # If p-value < 0.05, add bold text\n",
    "        if 'p_value' in row and row['p_value'] is not None and row['p_value'] < 0.05:\n",
    "            colors = ['background-color: red; font-weight: bold'] * len(row)\n",
    "    return colors\n",
    "\n",
    "results_df.round(2).style.apply(highlight_drops_and_significance, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results from different models\n",
    "gpt4_df = pd.read_csv('results/sa/claude-3-5-sonnet-0shot-sst2.csv')\n",
    "claude_df = pd.read_csv('results/sa/claude-3-5-sonnet-0shot-sst2.csv')\n",
    "mixtral_df = pd.read_csv('results/sa/mixtral-8x22b-sst2.csv')\n",
    "\n",
    "# Calculate accuracy between predictions and labels\n",
    "gpt4_acc = (gpt4_df['pred'] == gpt4_df['label']).mean()\n",
    "claude_acc = (claude_df['pred'] == claude_df['label']).mean()\n",
    "mixtral_acc = (mixtral_df['pred'] == mixtral_df['label']).mean()\n",
    "# Calculate average accuracy for each model\n",
    "print(f\"GPT-4 Average Accuracy: {gpt4_acc:.2%}\")\n",
    "print(f\"Claude-3.5 Average Accuracy: {claude_acc:.2%}\")\n",
    "print(f\"Mixtral Average Accuracy: {mixtral_acc:.2%}\")\n",
    "\n",
    "# Create comparison dataframe\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': ['GPT-4', 'Claude-3.5', 'Mixtral'],\n",
    "    'Accuracy': [gpt4_acc, claude_acc, mixtral_acc]\n",
    "})\n",
    "\n",
    "# Style the dataframe\n",
    "def highlight_max(s):\n",
    "    is_max = s == s.max()\n",
    "    return ['background-color: green' if v else '' for v in is_max]\n",
    "\n",
    "styled_df = comparison_df.style.apply(highlight_max, subset=['Accuracy'])\n",
    "styled_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
