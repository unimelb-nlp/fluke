{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import dspy\n",
    "import openai\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "openai.organization = os.getenv('OPENAI_ORGANIZATION')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = dspy.LM('together_ai/meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo', temperature=0, max_tokens=300)\n",
    "dspy.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = pd.read_json('../preprocessing/train_dev_test_data/coref/test.json')\n",
    "ds = ds.to_dict('records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_space(text):\n",
    "    # Remove multiple spaces\n",
    "    text = ' '.join(text.split())\n",
    "    # lines = text.split('\\n')\n",
    "    # for i,line in enumerate(lines):\n",
    "        \n",
    "    # lines[i] = lines[i].replace('  ', ' ')\n",
    "    # Fix spacing around punctuation\n",
    "    text = re.sub(r'\\s+([.,!?])', r'\\1', text)\n",
    "    text = re.sub(r'([.,!?])\\s+', r'\\1 ', text)\n",
    "    \n",
    "    # Fix contractions\n",
    "    text = re.sub(r'\\s*\\'\\s*s\\b', \"'s\", text)\n",
    "    text = re.sub(r'\\s*n\\s*\\'\\s*t\\b', \"n't\", text)\n",
    "    text = re.sub(r'\\s*\\'\\s*ve\\b', \"'ve\", text)\n",
    "    text = re.sub(r'\\s*\\'\\s*re\\b', \"'re\", text)\n",
    "    text = re.sub(r'\\s*\\'\\s*ll\\b', \"'ll\", text)\n",
    "    text = re.sub(r'\\s*\\'\\s*d\\b', \"'d\", text)\n",
    "    text = re.sub(r'\\s*\\'\\s*m\\b', \"'m\", text)\n",
    "    \n",
    "    # Fix spaces around parentheses\n",
    "    text = re.sub(r'\\(\\s+', '(', text)\n",
    "    text = re.sub(r'\\s+\\)', ')', text)\n",
    "    \n",
    "    # Remove spaces before and after text\n",
    "    text = text.strip()\n",
    "    # text = text.replace('agent 0: ','')\n",
    "    # text = text.replace('agent 1: ','')\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    dspy.Example({ \n",
    "                  \"text\" : remove_space(r[\"text\"]), \n",
    "                  \"pronoun\": r['pronoun'],\n",
    "                  \"candidate\": '0: '+  str(r['candidate'][0]) +  ', 1: ' + str(r['candidate'][1]),\n",
    "                  \"label\": r['label']\n",
    "\n",
    "                }\n",
    "                  ).with_inputs(\"text\", 'pronoun', 'candidate') \n",
    "    \n",
    "    for r in ds\n",
    "    \n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = examples[0]\n",
    "for k, v in example.items():\n",
    "    print(f\"\\n{k.upper()}:\\n\")\n",
    "    print(v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_prediction(text):\n",
    "    matches = re.findall(r'\\b[0-2]\\b', text)\n",
    "    # print(matches)\n",
    "    parsed_answer = matches[-1] if matches else \"\"\n",
    "    return parsed_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_metric(true, prediction, trace=None):\n",
    "    pred = prediction.label\n",
    "    matches = re.findall(r'\\b[0-9]\\b', pred)\n",
    "    # print(matches)\n",
    "    parsed_answer = matches[-1] if matches else \"\"\n",
    "    # print(parsed_answer)\n",
    "    return parsed_answer == str(true.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the original test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.evaluate import Evaluate\n",
    "\n",
    "# evaluate = Evaluate(devset= examples, metric=eval_metric, num_threads=6, display_progress=True, display_table=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Coref(dspy.Signature):\n",
    "    \"\"\"Which candidate does the pronoun refer to? Answer with either 0 or 1.\"\"\"\n",
    "    text = dspy.InputField()\n",
    "    pronoun = dspy.InputField()\n",
    "    candidate = dspy.InputField()\n",
    "    label = dspy.OutputField(desc=\"The index 0 or 1 of the candidates.\", prefix = 'Answer:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCoref(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.prog = dspy.Predict(Coref)\n",
    "\n",
    "    def forward(self, text, pronoun, candidate):\n",
    "\n",
    "        return self.prog(text = text, pronoun = pronoun, candidate = candidate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_coref = SimpleCoref()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = simple_coref(text=example.text, pronoun = example.pronoun, candidate = example.candidate)\n",
    "print(\"\\nQUESTION:\\n\")\n",
    "print(example.text)\n",
    "print(\"\\nPRONOUN:\\n\")\n",
    "print(example.pronoun)\n",
    "print(\"\\nCANDIDATES:\\n\")\n",
    "print(example.candidate)\n",
    "\n",
    "# print(\"\\nANSWER:\\n\")\n",
    "# print(example.label)\n",
    "print(\"\\nPREDICTION:\\n\")\n",
    "print(pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metric(example, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate = Evaluate(devset= examples, metric=eval_metric, num_threads=6, display_progress=True, display_table=10, return_outputs= True, return_all_scores=True)\n",
    "results = evaluate(simple_coref)\n",
    "items = []\n",
    "for sample in results[1]:\n",
    "    item = {}\n",
    "    sentence = sample[0]['text']\n",
    "    pronoun = sample[0]['pronoun']\n",
    "    candidate = sample[0]['candidate']\n",
    "    label = sample[0]['label']\n",
    "    pred = sample[1]['label']\n",
    "    item['text'] = sentence\n",
    "    item['pronoun'] = pronoun\n",
    "    item['candidate'] = candidate\n",
    "    item['label'] = label\n",
    "    item['pred'] = pred\n",
    "    items.append(item)\n",
    "df_result = pd.DataFrame(data = items)\n",
    "df_result.to_csv('results/coref/llama-0shot-coref.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoTCoref(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.prog = dspy.ChainOfThought(Coref)\n",
    "\n",
    "    def forward(self, text, pronoun, candidate):\n",
    "\n",
    "        return self.prog(text = text, pronoun = pronoun, candidate = candidate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cot_coref = CoTCoref()\n",
    "pred = cot_coref(text=example.text, pronoun = example.pronoun, candidate = example.candidate)\n",
    "print(\"\\nQUESTION:\\n\")\n",
    "print(example.text)\n",
    "# print(\"\\nANSWER:\\n\")\n",
    "# print(example.label)\n",
    "print(\"\\nPREDICTION:\\n\")\n",
    "print(pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate = Evaluate(devset= examples, metric=eval_metric, num_threads=6, display_progress=True, display_table=10, return_outputs= True, return_all_scores=True)\n",
    "results = evaluate(cot_coref)\n",
    "items = []\n",
    "for sample in results[1]:\n",
    "    item = {}\n",
    "    sentence = sample[0]['text']\n",
    "    pronoun = sample[0]['pronoun']\n",
    "    candidate = sample[0]['candidate']\n",
    "    label = sample[0]['label']\n",
    "    pred = sample[1]['label']\n",
    "    reasoning = sample[1]['reasoning']\n",
    "    item['text'] = sentence\n",
    "    item['pronoun'] = pronoun\n",
    "    item['candidate'] = candidate\n",
    "    item['rationale'] = reasoning\n",
    "    item['label'] = label\n",
    "    item['pred'] = pred\n",
    "    items.append(item)\n",
    "df_result = pd.DataFrame(data = items)\n",
    "df_result.to_csv('results/coref/llama-0shot-cot-coref.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate by modification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without label change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_modified_set(ds, program):\n",
    "    examples = [\n",
    "    dspy.Example({ \n",
    "                  \"text\" : remove_space(r['modified_text']), \n",
    "                  \"original_text\": remove_space(r['original_text']),\n",
    "                  \"pronoun\": r['modified_pronoun'],\n",
    "                  \"candidate\": '0: '+  str(r['modified_candidates'][0]) +  ', 1: ' + str(r['modified_candidates'][1]),\n",
    "                  \"label\": int(r['modified_label']),\n",
    "                  \"modified_label\": int(r['modified_label'])\n",
    "                }\n",
    "                  ).with_inputs(\"text\", \"pronoun\", \"candidate\") \n",
    "    for r in ds\n",
    "    ]\n",
    "    evaluate = Evaluate(devset= examples, metric=eval_metric, num_threads=6, display_progress=True, display_table=1, return_outputs= True, return_all_scores=True)\n",
    "    results = evaluate(program)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Coref(dspy.Signature):\n",
    "    \"\"\"Which candidate does the pronoun refer to? Answer with either 0 or 1.\"\"\"\n",
    "    text = dspy.InputField()\n",
    "    pronoun = dspy.InputField()\n",
    "    candidate = dspy.InputField()\n",
    "    label = dspy.OutputField(desc=\"The index 0 or 1 of the candidates.\", prefix = 'Answer:')\n",
    "class SimpleCoref(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.prog = dspy.Predict(Coref)\n",
    "\n",
    "    def forward(self, text, pronoun, candidate):\n",
    "\n",
    "        return self.prog(text = text, pronoun = pronoun, candidate = candidate)\n",
    "simple_coref = SimpleCoref()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure GPT-4 as the language model\n",
    "lm = dspy.LM('together_ai/meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo', temperature=0, max_tokens=300)\n",
    "dspy.configure(lm=lm)\n",
    "\n",
    "# Get all json files in the specified directory\n",
    "json_files = glob.glob('../data/modified_data/coref/*_100.json')\n",
    "original_pred_ds = pd.read_csv('results/coref/llama-0shot-coref.csv', index_col=False)\n",
    "original_pred_ds['text'] = original_pred_ds['text'].apply(remove_space)  # Replace 'your_function' with the actual function\n",
    "# print(original_pred_ds['dialog'][1958])\n",
    "for json_file in json_files:\n",
    "    print(json_file)\n",
    "    if not any(x in json_file for x in ['grammatical_role', 'derivation']):\n",
    "        continue\n",
    "    # Load the json file\n",
    "    # with open(json_file, 'r') as f:\n",
    "    #     data = json.load(f)\n",
    "    with open(json_file,'r') as f:\n",
    "        data = json.load(f)\n",
    "        # data = pd.read_json(json_file)\n",
    "        # data = data.to_json(orient = 'records')\n",
    "        # data = ast.literal_eval(data)\n",
    "    # print(data)\n",
    "    results_modified = evaluate_modified_set(data, simple_coref)\n",
    "    items = []\n",
    "    for sample in results_modified[1]:\n",
    "        item = {}\n",
    "        # print(sample[0])\n",
    "        modified_text = sample[0]['text']\n",
    "        original_text = sample[0]['original_text']\n",
    "\n",
    "        pred = sample[1]['label']\n",
    "        # rationale = sample[1]['reasoning']\n",
    "        # original_pred = compare_dialog(original_pred_ds, original_pred_ds['dialog'], original_text)\n",
    "        original_text = remove_space(original_text)\n",
    "        # print(original_text)\n",
    "        pred = extract_prediction(pred)\n",
    "        # print()\n",
    "        original_pred = original_pred_ds.loc[original_pred_ds['text'] == original_text]['pred'].values[0]\n",
    "        item['original_text'] = original_text\n",
    "        item['modified_text'] = modified_text\n",
    "        item['modified_pronoun'] = sample[0]['pronoun']\n",
    "        item['modified_candidates'] = sample[0]['candidate']\n",
    "        item['modified_label'] = sample[0]['modified_label']\n",
    "        item['modified_pred'] = pred\n",
    "        item['original_pred'] = original_pred\n",
    "        item['original_label'] = sample[0]['label']\n",
    "        # item['reasoning'] = rationale\n",
    "        items.append(item)\n",
    "    \n",
    "    df_result = pd.DataFrame(data=items)\n",
    "    \n",
    "    # Save results with filename based on input json\n",
    "    output_filename = f\"results/coref/llama-0shot-{json_file.split('/')[-1].replace('.json', '')}.csv\"\n",
    "    df_result.to_csv(output_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With label change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_modified_set(ds, program):\n",
    "    examples = [\n",
    "    dspy.Example({ \n",
    "                  \"text\" : remove_space(r['modified_text']), \n",
    "                  \"original_text\": remove_space(r['original_text']),\n",
    "                  \"pronoun\": r['modified_pronoun'],\n",
    "                  \"candidate\": '0: '+  str(r['modified_candidates'][0]) +  ', 1: ' + str(r['modified_candidates'][1]),\n",
    "                  \"label\": int(r['modified_label']),\n",
    "                  \"original_label\": int(r['original_label']),\n",
    "                  \"original_pronoun\": r['original_pronoun'],\n",
    "                  \"type\": r['type']\n",
    "                }\n",
    "                  ).with_inputs(\"text\", \"pronoun\", \"candidate\") \n",
    "    for r in ds\n",
    "    ]\n",
    "    evaluate = Evaluate(devset= examples, metric=eval_metric, num_threads=6, display_progress=True, display_table=1, return_outputs= True, return_all_scores=True)\n",
    "    results = evaluate(program)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Coref(dspy.Signature):\n",
    "    \"\"\"Which candidate does the pronoun refer to? Answer with either 0 or 1.\"\"\"\n",
    "    text = dspy.InputField()\n",
    "    pronoun = dspy.InputField()\n",
    "    candidate = dspy.InputField()\n",
    "    label = dspy.OutputField(desc=\"The index 0 or 1 of the candidates.\", prefix = 'Answer:')\n",
    "class SimpleCoref(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.prog = dspy.Predict(Coref)\n",
    "\n",
    "    def forward(self, text, pronoun, candidate):\n",
    "\n",
    "        return self.prog(text = text, pronoun = pronoun, candidate = candidate)\n",
    "simple_coref = SimpleCoref()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = dspy.LM('together_ai/meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo', temperature=0, max_tokens=300)\n",
    "dspy.configure(lm=lm)\n",
    "\n",
    "# Get all json files in the specified directory\n",
    "json_files = glob.glob('../data/modified_data/coref/*_100.json')\n",
    "original_pred_ds = pd.read_csv('results/coref/llama-0shot-coref.csv', index_col=False)\n",
    "original_pred_ds['text'] = original_pred_ds['text'].apply(remove_space)  # Replace 'your_function' with the actual function\n",
    "# print(original_pred_ds['dialog'][1958])\n",
    "for json_file in json_files:\n",
    "    # if not any(x in json_file for x in ['active_to_passive']):\n",
    "    #     continue\n",
    " \n",
    "    # Load the json file\n",
    "    print(json_file)\n",
    "    # with open(json_file, 'r') as f:\n",
    "    #     data = json.load(f)\n",
    "    with open(json_file,'r') as f:\n",
    "        data = json.load(f)\n",
    "        # data = pd.read_json(json_file)\n",
    "        # data = data.to_json(orient = 'records')\n",
    "        # data = ast.literal_eval(data)\n",
    "    # print(data)\n",
    "    results_modified = evaluate_modified_set(data, simple_coref)\n",
    "    items = []\n",
    "    for sample in results_modified[1]:\n",
    "        item = {}\n",
    "        modified_text = sample[0]['text']\n",
    "        original_text = sample[0]['original_text']\n",
    "\n",
    "        label = sample[0]['label']\n",
    "        pred = sample[1]['label']\n",
    "        # rationale = sample[1]['reasoning']\n",
    "        # original_pred = compare_dialog(original_pred_ds, original_pred_ds['dialog'], original_text)\n",
    "        original_text = remove_space(original_text)\n",
    "        # print(original_text)\n",
    "        pred = extract_prediction(pred)\n",
    "\n",
    "        # print()\n",
    "        original_pred = original_pred_ds.loc[original_pred_ds['text'] == original_text]['pred'].values[0]\n",
    "        item['original_text'] = original_text\n",
    "        item['modified_text'] = modified_text\n",
    "        item['modified_label'] = sample[0]['label']\n",
    "        item['modified_pred'] = pred\n",
    "        item['original_pred'] = original_pred\n",
    "        item['modified_pronoun'] = sample[0]['pronoun']\n",
    "        # if sample[0]['pronoun'] != sample[0]['original_pronoun']:\n",
    "            # continue\n",
    "        item['modified_candidates'] = sample[0]['candidate']\n",
    "        item['original_label'] = sample[0]['original_label']\n",
    "        item['type'] = sample[0]['type']\n",
    "        # item['reasoning'] = rationale\n",
    "        items.append(item)\n",
    "    \n",
    "    df_result = pd.DataFrame(data=items)\n",
    "    \n",
    "    # Save results with filename based on input json\n",
    "    output_filename = f\"results/coref/llama-0shot-{json_file.split('/')[-1].replace('.json', '')}.csv\"\n",
    "    print('saved to', output_filename)\n",
    "    df_result.to_csv(output_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_files = glob.glob('results/coref/llama-0shot-*_100.csv')\n",
    "\n",
    "aggregated_results = []\n",
    "\n",
    "for file in result_files:\n",
    "    # Extract modification type from filename\n",
    "    mod_type = file.split('-')[-1].replace('.csv','')\n",
    "    \n",
    "    # Read results file\n",
    "    df = pd.read_csv(file)\n",
    "\n",
    "    # Calculate accuracies\n",
    "    original_correct = (df['original_pred'] == df['original_label']).sum()\n",
    "    modified_correct = (df['modified_pred'] == df['modified_label']).sum()\n",
    "    total = len(df)\n",
    "\n",
    "    original_acc = original_correct / total\n",
    "    modified_acc = modified_correct / total\n",
    "    \n",
    "    # Calculate the difference between original_res and modified_res\n",
    "    difference = -round(original_acc - modified_acc, 2)\n",
    "    \n",
    "    # Calculate percentage difference with respect to total samples\n",
    "    pct_difference = -round((original_correct - modified_correct) / original_correct * 100, 2)\n",
    "    \n",
    "    # Perform t-test between original and modified predictions\n",
    "    t_stat, p_value = stats.ttest_ind(\n",
    "        (df['original_pred'] == df['original_label']).astype(float),\n",
    "        (df['modified_pred'] == df['modified_label']).astype(float)\n",
    "    )\n",
    "    \n",
    "    aggregated_results.append({\n",
    "        'task': 'dialogue_contradiction_detection',\n",
    "        'modification': mod_type,\n",
    "        'original_res': round(original_acc, 2),\n",
    "        'modified_res': round(modified_acc, 2),\n",
    "        'difference': difference,  # Difference in accuracy\n",
    "        'pct_difference': pct_difference,  # Percentage difference relative to total samples\n",
    "        'p_value': p_value  # Add p-value from t-test\n",
    "    })\n",
    "\n",
    "# Create final results dataframe\n",
    "results_df = pd.DataFrame(aggregated_results)\n",
    "\n",
    "# Sort the results based on modification_name\n",
    "modification_name = ['temporal_bias_100', 'geographical_bias_100','length_bias_100', 'typo_bias_100', 'capitalization_100', 'punctuation_100', 'derivation_100', 'compound_word_100','active_to_passive_100','grammatical_role_100', 'coordinating_conjunction_100', 'concept_replacement_100','negation_100','discourse_100','sentiment_100','casual_100', 'dialectal_100']\n",
    "results_df['modification'] = pd.Categorical(results_df['modification'], categories=modification_name, ordered=True)\n",
    "results_df = results_df.sort_values(by='modification')\n",
    "\n",
    "# Calculate averages across all modifications\n",
    "avg_original = results_df['original_res'].mean()\n",
    "avg_modified = results_df['modified_res'].mean()\n",
    "avg_difference = avg_original - avg_modified\n",
    "avg_pct_difference = results_df['pct_difference'].mean()\n",
    "\n",
    "# Add averages as a new row\n",
    "results_df.loc[len(results_df)] = {\n",
    "    'task': 'dialogue_contradiction_detection',\n",
    "    'modification': 'average',\n",
    "    'original_res': round(avg_original, 2),\n",
    "    'modified_res': round(avg_modified, 2),\n",
    "    'difference': -round(avg_difference, 2),\n",
    "    'pct_difference': round(avg_pct_difference, 2),\n",
    "    'p_value': None  # No p-value for average row\n",
    "}\n",
    "\n",
    "print(\"\\n\")\n",
    "results_df.to_csv('results/coref/llama-DP.csv')\n",
    "\n",
    "# Apply styling to highlight rows where original_res > modified_res and significant p-values\n",
    "def highlight_drops_and_significance(row):\n",
    "    colors = [''] * len(row)\n",
    "    if row['original_res'] > row['modified_res']:\n",
    "        colors = ['background-color: red'] * len(row)\n",
    "        # If p-value < 0.05, add bold text\n",
    "        if 'p_value' in row and row['p_value'] is not None and row['p_value'] < 0.05:\n",
    "            colors = ['background-color: red; font-weight: bold'] * len(row)\n",
    "    return colors\n",
    "\n",
    "results_df.round(2).style.apply(highlight_drops_and_significance, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results from different models\n",
    "gpt4_df = pd.read_csv('results/coref/llama-0shot-coref.csv')\n",
    "claude_df = pd.read_csv('results/coref/claude-3-5-sonnet-0shot-coref.csv')\n",
    "mixtral_df = pd.read_csv('results/coref/mixtral-8x22b-0shot-coref.csv')\n",
    "\n",
    "# Calculate accuracy between predictions and labels\n",
    "gpt4_acc = (gpt4_df['pred'] == gpt4_df['label']).mean()\n",
    "claude_acc = (claude_df['pred'] == claude_df['label']).mean()\n",
    "mixtral_acc = (mixtral_df['pred'] == mixtral_df['label']).mean()\n",
    "# Calculate average accuracy for each model\n",
    "print(f\"GPT-4 Average Accuracy: {gpt4_acc:.2%}\")\n",
    "print(f\"Claude-3.5 Average Accuracy: {claude_acc:.2%}\")\n",
    "print(f\"Mixtral Average Accuracy: {mixtral_acc:.2%}\")\n",
    "\n",
    "# Create comparison dataframe\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': ['GPT-4', 'Claude-3.5', 'Mixtral'],\n",
    "    'Accuracy': [gpt4_acc, claude_acc, mixtral_acc]\n",
    "})\n",
    "\n",
    "# Style the dataframe\n",
    "def highlight_max(s):\n",
    "    is_max = s == s.max()\n",
    "    return ['background-color: green' if v else '' for v in is_max]\n",
    "\n",
    "styled_df = comparison_df.style.apply(highlight_max, subset=['Accuracy'])\n",
    "styled_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
