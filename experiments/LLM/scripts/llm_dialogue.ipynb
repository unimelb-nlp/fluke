{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import dspy\n",
    "import openai\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import json\n",
    "import random\n",
    "from dotenv import load_dotenv\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv('/Users/hungcoreft/workspace/ood-modification/eval/.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "openai.organization = os.getenv('OPENAI_ORGANIZATION')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = dspy.LM('openai/gpt-4o', temperature=0, max_tokens=1023)\n",
    "dspy.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = pd.read_json('../preprocessing/train_dev_test_data/dialogue/test.json')\n",
    "ds = ds.to_dict('records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_with_contradiction = []\n",
    "samples_no_contradiction = []\n",
    "for i, x in enumerate(ds):\n",
    "    if x[\"is_contradiction\"]:\n",
    "        samples_with_contradiction.append((i, x))\n",
    "    else:\n",
    "        samples_no_contradiction.append((i, x))\n",
    "print(len(samples_with_contradiction), len(samples_no_contradiction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = samples_with_contradiction + samples_no_contradiction\n",
    "random.shuffle(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {'is_contradiction': 1, 'no_contradiction': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_space(text):\n",
    "    \"\"\"Clean up spacing and formatting in dialogue text.\"\"\"\n",
    "    lines = text.split('\\n')\n",
    "    cleaned_lines = []\n",
    "    \n",
    "    for line in lines:\n",
    "        # Remove multiple spaces\n",
    "        cleaned = ' '.join(line.split())\n",
    "        \n",
    "        # Fix spacing around punctuation\n",
    "        cleaned = re.sub(r'\\s+([.,!?])', r'\\1', cleaned)\n",
    "        cleaned = re.sub(r'([.,!?])\\s+', r'\\1 ', cleaned)\n",
    "        \n",
    "        # Fix contractions\n",
    "        cleaned = re.sub(r'\\s*\\'\\s*s\\b', \"'s\", cleaned)\n",
    "        cleaned = re.sub(r'\\s*n\\s*\\'\\s*t\\b', \"n't\", cleaned)\n",
    "        cleaned = re.sub(r'\\s*\\'\\s*ve\\b', \"'ve\", cleaned)\n",
    "        cleaned = re.sub(r'\\s*\\'\\s*re\\b', \"'re\", cleaned)\n",
    "        cleaned = re.sub(r'\\s*\\'\\s*ll\\b', \"'ll\", cleaned)\n",
    "        cleaned = re.sub(r'\\s*\\'\\s*d\\b', \"'d\", cleaned)\n",
    "        cleaned = re.sub(r'\\s*\\'\\s*m\\b', \"'m\", cleaned)\n",
    "        \n",
    "        # Fix spaces around parentheses\n",
    "        cleaned = re.sub(r'\\(\\s+', '(', cleaned)\n",
    "        cleaned = re.sub(r'\\s+\\)', ')', cleaned)\n",
    "        \n",
    "        # Remove leading/trailing whitespace\n",
    "        cleaned = cleaned.strip()\n",
    "        \n",
    "        cleaned_lines.append(cleaned)\n",
    "        \n",
    "    return '\\n'.join(cleaned_lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    dspy.Example({ \n",
    "                  \"dialogue\" : remove_space('\\n'.join(r[\"dialogue\"])), \n",
    "                  \"label\": label_map[r['label']]\n",
    "                }\n",
    "                  ).with_inputs(\"dialogue\") \n",
    "    for i,r in samples\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = examples[59]\n",
    "for k, v in example.items():\n",
    "    print(f\"\\n{k.upper()}:\\n\")\n",
    "    print(v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_prediction(text):\n",
    "    matches = re.findall(r'\\b[0-2]\\b', text)\n",
    "    # print(matches)\n",
    "    parsed_answer = matches[-1] if matches else \"\"\n",
    "    return parsed_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_metric(true, prediction, trace=None):\n",
    "    pred = prediction.label\n",
    "    # Find all occurrences of 0 or 1 that are bounded by word boundaries (\\b)\n",
    "    # \\b ensures we only match standalone 0s and 1s, not numbers like 10 or 01\n",
    "    # [0-1] matches either 0 or 1\n",
    "    matches = re.findall(r'\\b[0-2]\\b', pred)\n",
    "    # print(matches)\n",
    "    parsed_answer = matches[-1] if matches else \"\"\n",
    "    \n",
    "    # print(parsed_answer)\n",
    "    return parsed_answer == str(true.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the original test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.evaluate import Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dialogue(dspy.Signature):\n",
    "    \"\"\"Does the last utterance contradict the dialogue context? Answer with 1 if contradict, 0 if not contradict\"\"\"\n",
    "    dialogue = dspy.InputField()\n",
    "    label = dspy.OutputField(prefix = 'Answer:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDialogue(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.prog = dspy.Predict(Dialogue)\n",
    "\n",
    "    def forward(self, dialogue):\n",
    "\n",
    "        return self.prog(dialogue = dialogue)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_dialogue = SimpleDialogue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = simple_dialogue(dialogue = example.dialogue)\n",
    "print(\"\\nDIALOGUE:\\n\")\n",
    "print(example.dialogue)\n",
    "\n",
    "print(\"\\nANSWER:\\n\")\n",
    "print(example.label)\n",
    "\n",
    "print(\"\\nPREDICTION:\\n\")\n",
    "print(pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metric(example, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate = Evaluate(devset= examples, metric=eval_metric, num_threads=6, display_progress=True, display_table=10, return_outputs= True, return_all_scores=True)\n",
    "results = evaluate(simple_dialogue)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "for sample in results[1]:\n",
    "    item = {}\n",
    "    print(sample[0])\n",
    "    dialog = sample[0]['dialogue']\n",
    "    label = sample[0]['label']\n",
    "    pred = sample[1]['label']\n",
    "    item['dialog'] = dialog\n",
    "    item['label'] = label\n",
    "    item['pred'] = pred\n",
    "    items.append(item)\n",
    "df_result = pd.DataFrame(data = items)\n",
    "df_result.to_csv('results/dialogue/gpt4o-0shot-dialogue.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoTDialogue(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.prog = dspy.ChainOfThought(Dialogue)\n",
    "\n",
    "    def forward(self, dialogue):\n",
    "\n",
    "        return self.prog(dialogue = dialogue)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cot_dialogue = CoTDialogue()\n",
    "pred = cot_dialogue(dialogue = example.dialogue)\n",
    "print(\"\\nDIALOGUE:\\n\")\n",
    "print(example.dialogue)\n",
    "print(\"\\nANSWER:\\n\")\n",
    "print(example.label)\n",
    "print(\"\\nPREDICTION:\\n\")\n",
    "print(pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate = Evaluate(devset= examples, metric=eval_metric, num_threads=6, display_progress=True, display_table=10, return_outputs= True, return_all_scores=True)\n",
    "results = evaluate(cot_dialogue)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "for sample in results[1]:\n",
    "    item = {}\n",
    "    print(sample[0])\n",
    "    dialog = sample[0]['dialogue']\n",
    "    label = sample[0]['label']\n",
    "    pred = sample[1]['label']\n",
    "    rationale = sample[1]['reasoning']\n",
    "    item['dialog'] = dialog\n",
    "    item['label'] = label\n",
    "    item['pred'] = pred\n",
    "    item['reasoning'] = rationale\n",
    "    items.append(item)\n",
    "df_result = pd.DataFrame(data = items)\n",
    "df_result.to_csv('results/dialogue/gpt4o-0shot-cot-dialogue.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_person(ds):\n",
    "    for i,sample in enumerate(ds):\n",
    "        # print(sample)\n",
    "        modified_dialog = sample['dialog_context'] + [sample['modified_text']]\n",
    "        original_dialog = sample['dialog_context'] + [sample['original_text']]\n",
    "        for j, turn in enumerate(modified_dialog):\n",
    "            turn = 'agent ' + str(j%2) + ': ' + turn\n",
    "            modified_dialog[j] = turn\n",
    "        for j, turn in enumerate(original_dialog):\n",
    "            turn = 'agent ' + str(j%2) + ': ' + turn\n",
    "            original_dialog[j] = turn\n",
    "        ds[i]['original_dialog'] = '\\n'.join(original_dialog)\n",
    "        ds[i]['original_dialog'] = remove_space(ds[i]['original_dialog'])\n",
    "        ds[i]['modified_dialog'] = '\\n'.join(modified_dialog)\n",
    "        ds[i]['modified_dialog'] = remove_space(ds[i]['modified_dialog'])\n",
    "\n",
    "    return ds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate by modification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without label change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_modified_set(ds, program):\n",
    "    examples = [\n",
    "    dspy.Example({ \n",
    "                  \"dialogue\" : r['modified_dialog'], \n",
    "                  \"original_dialogue\": r['original_dialog'],\n",
    "                  \"label\": int(r['label']),\n",
    "                  \"modified_label\": int(r['label'])\n",
    "                }\n",
    "                  ).with_inputs(\"dialogue\") \n",
    "    for r in ds\n",
    "    ]\n",
    "    evaluate = Evaluate(devset= examples, metric=eval_metric, num_threads=6, display_progress=True, display_table=1, return_outputs= True, return_all_scores=True)\n",
    "    results = evaluate(program)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dialogue(dspy.Signature):\n",
    "    \"\"\"Does the last utterance contradict the dialogue context? Answer with 1 if contradict, 0 if not contradict\"\"\"\n",
    "    dialogue = dspy.InputField()\n",
    "    label = dspy.OutputField(prefix = 'Answer:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDialogue(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.prog = dspy.Predict(Dialogue)\n",
    "\n",
    "    def forward(self, dialogue):\n",
    "\n",
    "        return self.prog(dialogue = dialogue)\n",
    "simple_dialogue = SimpleDialogue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure GPT-4 as the language model\n",
    "lm = dspy.LM('openai/gpt-4o', temperature=0, max_tokens=300)\n",
    "dspy.configure(lm=lm)\n",
    "\n",
    "# Get all json files in the specified directory\n",
    "json_files = glob.glob('../data/modified_data/dialogue/*_100.json')\n",
    "original_pred_ds = pd.read_csv('results/dialogue/gpt4o-0shot-dialogue.csv', index_col=False)\n",
    "original_pred_ds['dialog'] = original_pred_ds['dialog'].apply(remove_space)  # Replace 'your_function' with the actual function\n",
    "# print(original_pred_ds['dialog'][1958])\n",
    "for json_file in json_files:\n",
    "    if any(x in json_file for x in ['grammatical_role', 'negation']):\n",
    "        continue\n",
    " \n",
    "    # Load the json file\n",
    "    print(json_file)\n",
    "    # with open(json_file, 'r') as f:\n",
    "    #     data = json.load(f)\n",
    "    if not any(x in json_file for x in ['capitalization', 'typo_bias', 'punctuation', 'grammatical_role', 'negation']):\n",
    "        data = pd.read_json(json_file)[1]\n",
    "        data = list(data)\n",
    "    else:\n",
    "        with open(json_file,'r') as f:\n",
    "            data = json.load(f)\n",
    "        # data = pd.read_json(json_file)\n",
    "        # data = data.to_json(orient = 'records')\n",
    "        # data = ast.literal_eval(data)\n",
    "    # print(data)\n",
    "    data = append_person(data)\n",
    "    results_modified = evaluate_modified_set(data, simple_dialogue)\n",
    "    items = []\n",
    "    for sample in results_modified[1]:\n",
    "        item = {}\n",
    "        # print(sample[0])\n",
    "        modified_dialog = sample[0]['dialogue']\n",
    "        original_dialog = sample[0]['original_dialogue']\n",
    "\n",
    "        label = sample[0]['label']\n",
    "        pred = sample[1]['label']\n",
    "        # rationale = sample[1]['reasoning']\n",
    "        # original_pred = compare_dialog(original_pred_ds, original_pred_ds['dialog'], original_dialog)\n",
    "        original_dialog = remove_space(original_dialog)\n",
    "        # print(original_dialog)\n",
    "        # print()\n",
    "        original_pred = original_pred_ds.loc[original_pred_ds['dialog'] == original_dialog]['pred'].values[0]\n",
    "        item['original_dialog'] = original_dialog\n",
    "        item['modified_dialog'] = modified_dialog\n",
    "        modified_pred = extract_prediction(pred)\n",
    "        item['modified_label'] = label\n",
    "        item['original_label'] = label\n",
    "        item['modified_pred'] = modified_pred\n",
    "        item['original_pred'] = original_pred\n",
    "        # item['reasoning'] = sample[1]['reasoning']\n",
    "        # item['reasoning'] = rationale\n",
    "        items.append(item)\n",
    "    \n",
    "    df_result = pd.DataFrame(data=items)\n",
    "    \n",
    "    # Save results with filename based on input json\n",
    "    output_filename = f\"results/dialogue/gpt4o-0shot-{json_file.split('/')[-1].replace('.json', '')}.csv\"\n",
    "    df_result.to_csv(output_filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With label change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_modified_set(ds, program):\n",
    "    examples = [\n",
    "    dspy.Example({ \n",
    "                  \"dialogue\" : r['modified_dialog'], \n",
    "                  \"original_dialogue\": r['original_dialog'],\n",
    "                  \"label\": int(r['modified_label']) if r.get('modified_label')!= None else int(r['label']),\n",
    "                  \"original_label\": int(r['label']),\n",
    "                  \"index\": r['index'],\n",
    "                  \"type\": r['type'] if r.get('type') != None else None\n",
    "                }\n",
    "                  ).with_inputs(\"dialogue\") \n",
    "    for r in ds\n",
    "    ]\n",
    "    evaluate = Evaluate(devset= examples, metric=eval_metric, num_threads=6, display_progress=True, display_table=1, return_outputs= True, return_all_scores=True)\n",
    "    results = evaluate(program)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dialogue(dspy.Signature):\n",
    "    \"\"\"Does the last utterance contradict the dialogue context? Answer with 1 if contradict, 0 if not contradict\"\"\"\n",
    "    dialogue = dspy.InputField()\n",
    "    label = dspy.OutputField(prefix = 'Answer:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDialogue(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.prog = dspy.Predict(Dialogue)\n",
    "\n",
    "    def forward(self, dialogue):\n",
    "\n",
    "        return self.prog(dialogue = dialogue)\n",
    "simple_dialogue = SimpleDialogue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Configure GPT-4 as the language model\n",
    "lm = dspy.LM('openai/gpt-4o', temperature=0, max_tokens=250)\n",
    "dspy.configure(lm=lm)\n",
    "\n",
    "# Get all json files in the specified directory\n",
    "json_files = glob.glob('../data/modified_data/dialogue/*_100.json')\n",
    "original_pred_ds = pd.read_csv('results/dialogue/gpt4o-0shot-dialogue.csv', index_col=False)\n",
    "original_pred_ds['dialog'] = original_pred_ds['dialog'].apply(remove_space)  # Replace 'your_function' with the actual function\n",
    "# print(original_pred_ds['dialog'][1958])\n",
    "for json_file in json_files:\n",
    "    if not any(x in json_file for x in ['geographical_bias']):\n",
    "        continue\n",
    " \n",
    "    # Load the json file\n",
    "    print(json_file)\n",
    "    # with open(json_file, 'r') as f:\n",
    "    #     data = json.load(f)\n",
    "    with open(json_file,'r') as f:\n",
    "        raw_data = json.load(f)\n",
    "    data = [dict(sample, index=idx) for idx, sample in raw_data]\n",
    "\n",
    "        # data = pd.read_json(json_file)\n",
    "        # data = data.to_json(orient = 'records')\n",
    "        # data = ast.literal_eval(data)\n",
    "    # print(data)\n",
    "    data = append_person(data)\n",
    "    results_modified = evaluate_modified_set(data, simple_dialogue)\n",
    "    items = []\n",
    "    for sample in results_modified[1]:\n",
    "        item = {}\n",
    "        # print(sample[0])\n",
    "        modified_dialog = sample[0]['dialogue']\n",
    "        original_dialog = sample[0]['original_dialogue']\n",
    "\n",
    "        pred = sample[1]['label']\n",
    "        # rationale = sample[1]['reasoning']\n",
    "        # original_pred = compare_dialog(original_pred_ds, original_pred_ds['dialog'], original_dialog)\n",
    "        original_dialog = remove_space(original_dialog)\n",
    "        # print(original_dialog)\n",
    "        # print()\n",
    "        original_pred = original_pred_ds.loc[original_pred_ds['dialog'] == original_dialog]['pred'].values[0]\n",
    "        item['original_dialog'] = original_dialog\n",
    "        item['modified_dialog'] = modified_dialog\n",
    "        modified_pred = extract_prediction(pred)\n",
    "        item['modified_label'] = sample[0]['label']\n",
    "        item['original_label'] = sample[0]['original_label']\n",
    "        item['modified_pred'] = modified_pred\n",
    "        item['original_pred'] = original_pred\n",
    "        item['type'] = sample[0]['type']\n",
    "        # item['reasoning'] = sample[1]['reasoning']\n",
    "        # item['reasoning'] = rationale\n",
    "        items.append(item)\n",
    "    \n",
    "    df_result = pd.DataFrame(data=items)\n",
    "    \n",
    "    # Save results with filename based on input json\n",
    "    output_filename = f\"results/dialogue/gpt4o-0shot-{json_file.split('/')[-1].replace('.json', '')}.csv\"\n",
    "    df_result.to_csv(output_filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_files = glob.glob('results/dialogue/gpt4o-0shot-*_100.csv')\n",
    "\n",
    "aggregated_results = []\n",
    "\n",
    "for file in result_files:\n",
    "    # Extract modification type from filename\n",
    "    mod_type = file.split('-')[-1].replace('.csv','')\n",
    "    \n",
    "    # Read results file\n",
    "    df = pd.read_csv(file)\n",
    "\n",
    "    # Calculate accuracies\n",
    "    original_correct = (df['original_pred'] == df['original_label']).sum()\n",
    "    modified_correct = (df['modified_pred'] == df['modified_label']).sum()\n",
    "    total = len(df)\n",
    "\n",
    "    original_acc = original_correct / total\n",
    "    modified_acc = modified_correct / total\n",
    "    \n",
    "    # Calculate the difference between original_res and modified_res\n",
    "    difference = -round(original_acc - modified_acc, 2)\n",
    "    \n",
    "    # Calculate percentage difference with respect to total samples\n",
    "    pct_difference = -round((original_acc - modified_acc) / original_acc * 100, 2)\n",
    "    \n",
    "    # Perform t-test between original and modified predictions\n",
    "    t_stat, p_value = stats.ttest_ind(\n",
    "        (df['original_pred'] == df['original_label']).astype(float),\n",
    "        (df['modified_pred'] == df['modified_label']).astype(float)\n",
    "    )\n",
    "    \n",
    "    aggregated_results.append({\n",
    "        'task': 'dialogue_contradiction_detection',\n",
    "        'modification': mod_type,\n",
    "        'original_res': round(original_acc, 2),\n",
    "        'modified_res': round(modified_acc, 2),\n",
    "        'difference': difference,  # Difference in accuracy\n",
    "        'pct_difference': pct_difference,  # Percentage difference relative to total samples\n",
    "        'p_value': p_value  # Add p-value from t-test\n",
    "    })\n",
    "\n",
    "# Create final results dataframe\n",
    "results_df = pd.DataFrame(aggregated_results)\n",
    "\n",
    "# Sort the results based on modification_name\n",
    "modification_name = ['temporal_bias_100', 'geographical_bias_100','length_bias_100', 'typo_bias_100', 'capitalization_100', 'punctuation_100', 'derivation_100', 'compound_word_100','active_to_passive_100','grammatical_role_100', 'coordinating_conjunction_100', 'concept_replacement_100','negation_100','discourse_100','sentiment_100','casual_100', 'dialectal_100']\n",
    "results_df['modification'] = pd.Categorical(results_df['modification'], categories=modification_name, ordered=True)\n",
    "results_df = results_df.sort_values(by='modification')\n",
    "\n",
    "# Calculate averages across all modifications\n",
    "avg_original = results_df['original_res'].mean()\n",
    "avg_modified = results_df['modified_res'].mean()\n",
    "avg_difference = avg_original - avg_modified\n",
    "avg_pct_difference = results_df['pct_difference'].mean()\n",
    "\n",
    "# Add averages as a new row\n",
    "results_df.loc[len(results_df)] = {\n",
    "    'task': 'dialogue_contradiction_detection',\n",
    "    'modification': 'average',\n",
    "    'original_res': round(avg_original, 2),\n",
    "    'modified_res': round(avg_modified, 2),\n",
    "    'difference': -round(avg_difference, 2),\n",
    "    'pct_difference': round(avg_pct_difference, 2),\n",
    "    'p_value': None  # No p-value for average row\n",
    "}\n",
    "\n",
    "print(\"\\n\")\n",
    "results_df.to_csv('results/dialogue/gpt4o-DP.csv')\n",
    "\n",
    "# Apply styling to highlight rows where original_res > modified_res and significant p-values\n",
    "def highlight_drops_and_significance(row):\n",
    "    colors = [''] * len(row)\n",
    "    if row['original_res'] > row['modified_res']:\n",
    "        colors = ['background-color: red'] * len(row)\n",
    "        # If p-value < 0.05, add bold text\n",
    "        if 'p_value' in row and row['p_value'] is not None and row['p_value'] < 0.05:\n",
    "            colors = ['background-color: red; font-weight: bold'] * len(row)\n",
    "    return colors\n",
    "\n",
    "results_df.round(2).style.apply(highlight_drops_and_significance, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for negation modification only\n",
    "# negation_results = pd.read_csv('results/dialogue/gpt4o-DP.csv')\n",
    "# negation_df = negation_results[negation_results['modification'] == 'negation_100']\n",
    "\n",
    "# Load the detailed results for negation\n",
    "negation_detailed = pd.read_csv('results/dialogue/gpt4o-0shot-negation_100.csv')\n",
    "\n",
    "# Calculate accuracy for original and modified examples by type\n",
    "type_results = []\n",
    "for type_name in negation_detailed['type'].unique():\n",
    "    type_data = negation_detailed[negation_detailed['type'] == type_name]\n",
    "    \n",
    "    # Calculate original and modified accuracies\n",
    "    original_acc = (type_data['original_pred'] == type_data['original_label']).sum() / len(type_data)\n",
    "    modified_acc = (type_data['modified_pred'] == type_data['modified_label']).sum() / len(type_data)\n",
    "    \n",
    "    # Calculate differences\n",
    "    difference = -(original_acc - modified_acc)\n",
    "    pct_difference = -round((original_acc - modified_acc) / original_acc * 100, 2)\n",
    "\n",
    "    # Perform t-test\n",
    "    original_correct = (type_data['original_pred'] == type_data['original_label']).astype(int)\n",
    "    modified_correct = (type_data['modified_pred'] == type_data['modified_label']).astype(int)\n",
    "    _, p_value = stats.ttest_rel(original_correct, modified_correct)\n",
    "    \n",
    "    type_results.append({\n",
    "        'type': type_name,\n",
    "        'num_samples': len(type_data),\n",
    "        'original_res': round(original_acc * 100, 2),\n",
    "        'modified_res': round(modified_acc * 100, 2),\n",
    "        'difference': round(difference * 100, 2),\n",
    "        'pct_difference': round(pct_difference, 2),\n",
    "        'p_value': p_value\n",
    "    })\n",
    "\n",
    "# Create and display type-based results dataframe\n",
    "type_results_df = pd.DataFrame(type_results)\n",
    "\n",
    "# Calculate averages\n",
    "avg_original = type_results_df['original_res'].mean()\n",
    "avg_modified = type_results_df['modified_res'].mean()\n",
    "avg_difference = avg_original - avg_modified\n",
    "avg_pct_difference = type_results_df['pct_difference'].mean()\n",
    "total_samples = type_results_df['num_samples'].sum()\n",
    "\n",
    "# # Add averages row\n",
    "# type_results_df.loc[len(type_results_df)] = {\n",
    "#     'type': 'average',\n",
    "#     'num_samples': total_samples,\n",
    "#     'original_res': round(avg_original, 2),\n",
    "#     'modified_res': round(avg_modified, 2),\n",
    "#     'difference': round(avg_difference, 2),\n",
    "#     'pct_difference': round(avg_pct_difference, 2),\n",
    "#     'p_value': None\n",
    "# }\n",
    "\n",
    "# Apply the same styling as before\n",
    "styled_type_results = type_results_df.round(2).style.apply(highlight_drops_and_significance, axis=1)\n",
    "styled_type_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results from different models\n",
    "gpt4_df = pd.read_csv('results/dialogue/gpt4o-0shot-dialogue.csv')\n",
    "claude_df = pd.read_csv('results/dialogue/claude-3-5-sonnet-0shot-dialogue.csv')\n",
    "mixtral_df = pd.read_csv('results/dialogue/mixtral-8x22b-0shot-dialogue.csv')\n",
    "\n",
    "# Calculate accuracy between predictions and labels\n",
    "gpt4_acc = (gpt4_df['pred'] == gpt4_df['label']).mean()\n",
    "claude_acc = (claude_df['pred'] == claude_df['label']).mean()\n",
    "mixtral_acc = (mixtral_df['pred'] == mixtral_df['label']).mean()\n",
    "# Calculate average accuracy for each model\n",
    "print(f\"GPT-4 Average Accuracy: {gpt4_acc:.2%}\")\n",
    "print(f\"Claude-3.5 Average Accuracy: {claude_acc:.2%}\")\n",
    "print(f\"Mixtral Average Accuracy: {mixtral_acc:.2%}\")\n",
    "\n",
    "# Create comparison dataframe\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': ['GPT-4', 'Claude-3.5', 'Mixtral'],\n",
    "    'Accuracy': [gpt4_acc, claude_acc, mixtral_acc]\n",
    "})\n",
    "\n",
    "# Style the dataframe\n",
    "def highlight_max(s):\n",
    "    is_max = s == s.max()\n",
    "    return ['background-color: green' if v else '' for v in is_max]\n",
    "\n",
    "styled_df = comparison_df.style.apply(highlight_max, subset=['Accuracy'])\n",
    "styled_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama-3-405B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = dspy.LM('together_ai/meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo')\n",
    "dspy.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
