{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1203, 1292, 1393, 85, 185, 837, 783, 830, 356, 518, 315, 1492, 849, 232, 734, 671, 1486, 1478, 894, 601, 312, 1154, 211, 1225, 438, 817, 1451, 117, 89, 189, 37, 828, 965, 405, 794, 1386, 1339, 989, 1300, 1104, 1400, 27, 55, 534, 1381, 724, 547, 884, 285, 130, 124, 747, 1449, 1012, 114, 1219, 1056, 207, 274, 679, 845, 1433, 2, 553, 131, 1353, 1084, 684, 121, 436, 1232, 821, 1448, 120, 1141, 1191, 1312, 159, 77, 155, 87, 963, 376, 796, 198, 1187, 1308, 887, 902, 602, 964, 659, 871, 800, 1223, 772, 323, 1066]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import decimal\n",
    "\n",
    "# Load derivation data to get indices\n",
    "derivation_path = Path('../preprocessing/data_after_phase2/thinh/derivation_100.json')\n",
    "with open(derivation_path) as f:\n",
    "    derivation_data = json.load(f)\n",
    "\n",
    "# Extract indices from derivation data\n",
    "derivation_indices = [item['index'] for item in derivation_data]\n",
    "print(derivation_indices)\n",
    "# Create mapping between modification types and their filenames\n",
    "modification_mapping = {\n",
    "    'derivation': 'derivation'\n",
    "}\n",
    "\n",
    "# Base path for results\n",
    "base_path = Path('../pretrained_language_models/coreference_resolution/tmp')\n",
    "\n",
    "# Create index mapping for each model and modification\n",
    "index_mappings = {}\n",
    "for model in ['bert-base-cased', 'gpt2', 't5-base']:\n",
    "    index_mappings[model] = {}\n",
    "    model_path = base_path / f'{model}_results'\n",
    "    \n",
    "    for mod_type, filename in modification_mapping.items():\n",
    "        csv_path = model_path / f'{filename}_predictions.csv'\n",
    "        if csv_path.exists():\n",
    "            df = pd.read_csv(csv_path)\n",
    "            # Replace indices with derivation indices\n",
    "            df['index'] = derivation_indices[:len(df)]\n",
    "            # Create new file with replaced indices\n",
    "            output_path = model_path / f'{filename}_predictions.csv'\n",
    "            df.to_csv(output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-10.0, -8.494850021680092]\n",
      "[-10.0, -20.0]\n"
     ]
    }
   ],
   "source": [
    "# Additional test cases: (A, B) = (100, 90) and (50, 40)\n",
    "import math\n",
    "additional_test_cases = [(100, 90), (50, 40)]\n",
    "\n",
    "# Compute scaled log drop for each case using the weighted formula\n",
    "scaled_log_weighted_additional = [(B - A) * math.log(A) / math.log(100) for A, B in additional_test_cases]\n",
    "print(scaled_log_weighted_additional)\n",
    "\n",
    "\n",
    "delta = [(B-A)/A*100 for A, B in additional_test_cases]\n",
    "print(delta)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hungthinh/miniconda3/lib/python3.12/site-packages/scipy/stats/_wilcoxon.py:172: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  z = (r_plus - mn) / se\n",
      "/Users/hungthinh/miniconda3/lib/python3.12/site-packages/scipy/stats/_wilcoxon.py:172: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  z = (r_plus - mn) / se\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results by modification and model:\n",
      "              model              modification original_acc modified_acc  \\\n",
      "0   bert-base-cased                    casual       90.000       82.000   \n",
      "1              gpt2                    casual       93.000       85.000   \n",
      "2           t5-base                    casual       82.000       88.000   \n",
      "3   bert-base-cased  coordinating_conjunction       89.000       88.000   \n",
      "4              gpt2  coordinating_conjunction       90.000       88.000   \n",
      "5           t5-base  coordinating_conjunction       80.000       83.000   \n",
      "6   bert-base-cased                  singlish       84.906       76.415   \n",
      "7              gpt2                  singlish       80.189       77.358   \n",
      "8           t5-base                  singlish       83.962       81.132   \n",
      "9   bert-base-cased            capitalization       92.708       92.708   \n",
      "10             gpt2            capitalization       90.625       86.458   \n",
      "11          t5-base            capitalization       79.167       79.167   \n",
      "12  bert-base-cased             temporal_bias       87.000       87.000   \n",
      "13             gpt2             temporal_bias       85.000       83.000   \n",
      "14          t5-base             temporal_bias       81.000       76.000   \n",
      "15  bert-base-cased         geographical_bias       82.609       72.826   \n",
      "16             gpt2         geographical_bias       82.609       72.826   \n",
      "17          t5-base         geographical_bias       75.000       72.826   \n",
      "18  bert-base-cased             compound_word       89.000       85.000   \n",
      "19             gpt2             compound_word       86.000       86.000   \n",
      "20          t5-base             compound_word       81.000       82.000   \n",
      "21  bert-base-cased                 dialectal       85.000       81.000   \n",
      "22             gpt2                 dialectal       84.000       82.000   \n",
      "23          t5-base                 dialectal       88.000       84.000   \n",
      "24  bert-base-cased         active_to_passive       87.000       90.000   \n",
      "25             gpt2         active_to_passive       84.000       87.000   \n",
      "26          t5-base         active_to_passive       85.000       82.000   \n",
      "27  bert-base-cased               punctuation       86.000       84.000   \n",
      "28             gpt2               punctuation       90.000       88.000   \n",
      "29          t5-base               punctuation       86.000       86.000   \n",
      "30  bert-base-cased       concept_replacement       89.000       86.000   \n",
      "31             gpt2       concept_replacement       90.000       85.000   \n",
      "32          t5-base       concept_replacement       87.000       85.000   \n",
      "33  bert-base-cased          grammatical_role       85.294       76.471   \n",
      "34             gpt2          grammatical_role       83.824       79.412   \n",
      "35          t5-base          grammatical_role       91.176       83.824   \n",
      "36  bert-base-cased                 typo_bias       84.000       88.000   \n",
      "37             gpt2                 typo_bias       89.000       87.000   \n",
      "38          t5-base                 typo_bias       84.000       82.000   \n",
      "39  bert-base-cased                 discourse       88.506       94.253   \n",
      "40             gpt2                 discourse       91.954       89.655   \n",
      "41          t5-base                 discourse       94.253       93.103   \n",
      "42  bert-base-cased                 sentiment       92.000       88.000   \n",
      "43             gpt2                 sentiment       87.000       85.000   \n",
      "44          t5-base                 sentiment       84.000       81.000   \n",
      "45  bert-base-cased                derivation       88.172       88.172   \n",
      "46             gpt2                derivation       82.796       84.946   \n",
      "47          t5-base                derivation       84.946       83.871   \n",
      "48  bert-base-cased                  negation       89.000       67.000   \n",
      "49             gpt2                  negation       88.000       62.000   \n",
      "50          t5-base                  negation       90.000       68.000   \n",
      "51  bert-base-cased               length_bias       84.000       80.000   \n",
      "52             gpt2               length_bias       85.000       83.000   \n",
      "53          t5-base               length_bias       84.000       81.000   \n",
      "\n",
      "   percentage_diff weighted_delta p_value significance significant  \n",
      "0           -8.889         -7.817   0.021            *         Yes  \n",
      "1           -8.602         -7.874   0.011            *         Yes  \n",
      "2            7.317          5.741   0.083            .          No  \n",
      "3           -1.124         -0.975   0.655           ns          No  \n",
      "4           -2.222         -1.954   0.317           ns          No  \n",
      "5            3.750          2.855   0.257           ns          No  \n",
      "6          -10.000         -8.189   0.039            *         Yes  \n",
      "7           -3.529         -2.694   0.467           ns          No  \n",
      "8           -3.371         -2.723   0.366           ns          No  \n",
      "9            0.000          0.000   1.000           ns          No  \n",
      "10          -4.598         -4.078   0.102           ns          No  \n",
      "11           0.000          0.000   1.000           ns          No  \n",
      "12           0.000          0.000   1.000           ns          No  \n",
      "13          -2.353         -1.929   0.414           ns          No  \n",
      "14          -6.173         -4.771   0.059            .          No  \n",
      "15         -11.842         -9.377   0.020            *         Yes  \n",
      "16         -11.842         -9.377   0.020            *         Yes  \n",
      "17          -2.899         -2.038   0.637           ns          No  \n",
      "18          -4.494         -3.899   0.102           ns          No  \n",
      "19           0.000          0.000   1.000           ns          No  \n",
      "20           1.235          0.954   0.564           ns          No  \n",
      "21          -4.706         -3.859   0.317           ns          No  \n",
      "22          -2.381         -1.924   0.480           ns          No  \n",
      "23          -4.545         -3.889   0.157           ns          No  \n",
      "24           3.448          2.909   0.317           ns          No  \n",
      "25           3.571          2.886   0.366           ns          No  \n",
      "26          -3.529         -2.894   0.257           ns          No  \n",
      "27          -2.326         -1.934   0.317           ns          No  \n",
      "28          -2.222         -1.954   0.157           ns          No  \n",
      "29           0.000          0.000   1.000           ns          No  \n",
      "30          -3.371         -2.924   0.257           ns          No  \n",
      "31          -5.556         -4.886   0.025            *         Yes  \n",
      "32          -2.299         -1.940   0.317           ns          No  \n",
      "33         -10.345         -8.519   0.058            .          No  \n",
      "34          -5.263         -4.243   0.317           ns          No  \n",
      "35          -8.065         -7.205   0.025            *         Yes  \n",
      "36           4.762          3.849   0.102           ns          No  \n",
      "37          -2.247         -1.949   0.414           ns          No  \n",
      "38          -2.381         -1.924   0.317           ns          No  \n",
      "39           6.494          5.595   0.025            *         Yes  \n",
      "40          -2.500         -2.257   0.480           ns          No  \n",
      "41          -1.220         -1.135   0.564           ns          No  \n",
      "42          -4.348         -3.928   0.248           ns          No  \n",
      "43          -2.299         -1.940   0.564           ns          No  \n",
      "44          -3.571         -2.886   0.366           ns          No  \n",
      "45           0.000          0.000   1.000           ns          No  \n",
      "46           2.597          2.062   0.157           ns          No  \n",
      "47          -1.266         -1.037   0.317           ns          No  \n",
      "48         -24.719        -21.443   0.000           **         Yes  \n",
      "49         -29.545        -25.278   0.000           **         Yes  \n",
      "50         -24.444        -21.497   0.000           **         Yes  \n",
      "51          -4.762         -3.849   0.206           ns          No  \n",
      "52          -2.353         -1.929   0.414           ns          No  \n",
      "53          -3.571         -2.886   0.180           ns          No  \n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "# Base path for results\n",
    "base_path = Path('../pretrained_language_models/dialogue_contradiction_detection/tmp')\n",
    "\n",
    "# Function to load and convert predictions to binary\n",
    "def load_predictions(filepath):\n",
    "    with open(filepath) as f:\n",
    "        preds = json.load(f)\n",
    "    for pred in preds.items():\n",
    "        if pred[1] == \"contradictory\":\n",
    "            preds[pred[0]] = 1\n",
    "        elif pred[1] == \"not contradictory\":\n",
    "            preds[pred[0]] = 0\n",
    "    return preds\n",
    "\n",
    "# Load original model predictions\n",
    "model_orig_preds = {}\n",
    "model_names = ['bert-base-cased', 'gpt2', 't5-base']\n",
    "\n",
    "for model in model_names:\n",
    "    filepath = base_path / f'{model}_results/{model}_predictions.json'\n",
    "    model_orig_preds[model] = load_predictions(filepath)\n",
    "\n",
    "# Load predictions for each modification\n",
    "modifications = []\n",
    "for model in model_names:\n",
    "    mod_path = base_path / f'{model}_results'\n",
    "    if mod_path.exists():\n",
    "        # Get all CSV files containing predictions\n",
    "        modifications.extend([f.stem for f in mod_path.glob('*_predictions.csv')])\n",
    "modifications = list(set(modifications))  # Remove duplicates\n",
    "modifications = [mod.replace('_predictions', '') for mod in modifications]\n",
    "\n",
    "# Load negation types from GPT4 results\n",
    "gpt4_negation_df = pd.read_csv('../eval/results/rongxin/gpt4o-0shot-negation_100.csv')\n",
    "negation_types = gpt4_negation_df['type'].tolist()\n",
    "\n",
    "# Sanity check negation types\n",
    "valid_types = {'absolute', 'double', 'lexical', 'approximate', 'verbal'}\n",
    "for neg_type in negation_types:\n",
    "    if neg_type not in valid_types:\n",
    "        print(f\"WARNING: Invalid negation type found: {neg_type}\")\n",
    "\n",
    "# Create results list to store accuracy and statistical test results\n",
    "results_rows = []\n",
    "negation_results_rows = []\n",
    "\n",
    "for mod in modifications:\n",
    "    for model in model_names:\n",
    "        # Get original predictions\n",
    "        orig_preds = model_orig_preds[model]\n",
    "        \n",
    "        # Get modified predictions from CSV file\n",
    "        mod_filepath = base_path / f'{model}_results/{mod}_predictions.csv'\n",
    "        if mod_filepath.exists():\n",
    "            mod_df = pd.read_csv(mod_filepath)\n",
    "            # Calculate accuracies\n",
    "            orig_correct = 0\n",
    "            mod_correct = 0\n",
    "            total = 0\n",
    "            orig_list = []\n",
    "            mod_list = []\n",
    "            \n",
    "            # Track results by negation type if this is negation mod\n",
    "            if mod == 'negation':\n",
    "                results_by_type = {\n",
    "                    'absolute': {'orig_correct': 0, 'mod_correct': 0, 'total': 0, 'orig_binary': [], 'mod_binary': []},\n",
    "                    'double': {'orig_correct': 0, 'mod_correct': 0, 'total': 0, 'orig_binary': [], 'mod_binary': []},\n",
    "                    'lexical': {'orig_correct': 0, 'mod_correct': 0, 'total': 0, 'orig_binary': [], 'mod_binary': []},\n",
    "                    'approximate': {'orig_correct': 0, 'mod_correct': 0, 'total': 0, 'orig_binary': [], 'mod_binary': []},\n",
    "                    'verbal': {'orig_correct': 0, 'mod_correct': 0, 'total': 0, 'orig_binary': [], 'mod_binary': []}\n",
    "                }\n",
    "            \n",
    "            # Load labels from eval/results/rongxin CSV\n",
    "            eval_filepath = Path(f'../preprocessing/data_after_phase2/rongxin/{mod}_100.json')\n",
    "            if eval_filepath.exists():\n",
    "                label_df = json.load(open(eval_filepath))\n",
    "            if len(mod_df) != len(label_df):\n",
    "                print(mod, model, len(mod_df), len(label_df))\n",
    "                \n",
    "            for idx, row in mod_df.iterrows():\n",
    "                # Get ground truth labels from JSON in order\n",
    "                orig_label = label_df[idx][1]['original_label'] if 'original_label' in label_df[idx][1] else label_df[idx][1]['label']\n",
    "                mod_label = label_df[idx][1]['modified_label'] if 'modified_label' in label_df[idx][1] else label_df[idx][1]['label']\n",
    "                \n",
    "                # Get predictions from CSV\n",
    "                orig_pred = row['original'] if 'original' in row else row['original_pred']\n",
    "                mod_pred = row['modified'] if 'modified' in row else row['modified_pred']\n",
    "                \n",
    "                # Compare predictions with ground truth\n",
    "                orig_correct_bool = orig_pred == orig_label\n",
    "                mod_correct_bool = mod_pred == mod_label\n",
    "                \n",
    "                if orig_correct_bool:\n",
    "                    orig_correct += 1\n",
    "                    if mod == 'negation':\n",
    "                        if idx >= len(negation_types):\n",
    "                            print(f\"WARNING: Index {idx} exceeds negation types list length\")\n",
    "                            continue\n",
    "                        results_by_type[negation_types[idx]]['orig_correct'] += 1\n",
    "                if mod_correct_bool:\n",
    "                    mod_correct += 1\n",
    "                    if mod == 'negation':\n",
    "                        if idx >= len(negation_types):\n",
    "                            continue\n",
    "                        results_by_type[negation_types[idx]]['mod_correct'] += 1\n",
    "                        \n",
    "                if mod == 'negation':\n",
    "                    if idx >= len(negation_types):\n",
    "                        continue\n",
    "                    results_by_type[negation_types[idx]]['total'] += 1\n",
    "                    results_by_type[negation_types[idx]]['orig_binary'].append(1 if orig_correct_bool else 0)\n",
    "                    results_by_type[negation_types[idx]]['mod_binary'].append(1 if mod_correct_bool else 0)\n",
    "                    \n",
    "                orig_list.append(orig_pred)\n",
    "                mod_list.append(mod_pred)\n",
    "                total += 1\n",
    "                \n",
    "            orig_acc = orig_correct / total * 100 if total > 0 else 0\n",
    "            mod_acc = mod_correct / total * 100 if total > 0 else 0\n",
    "            pct_diff = ((mod_acc - orig_acc) / orig_acc) * 100 if orig_acc > 0 else 0\n",
    "            weighted_delta = (mod_acc - orig_acc) * np.log10(orig_acc) / np.log10(100)  if orig_acc > 0 else 0\n",
    "            \n",
    "            orig_list_binary = [1 if pred == label else 0 for pred, label in zip(orig_list, [label_df[i][1]['original_label'] if 'original_label' in label_df[i][1] else label_df[i][1]['label'] for i in range(len(orig_list))])]\n",
    "            mod_list_binary = [1 if pred == label else 0 for pred, label in zip(mod_list, [label_df[i][1]['modified_label'] if 'modified_label' in label_df[i][1] else label_df[i][1]['label'] for i in range(len(mod_list))])]\n",
    "\n",
    "            # Perform paired t-test on the raw predictions (0/1)\n",
    "            try:\n",
    "                _, p_value_mw = stats.mannwhitneyu(orig_list_binary, mod_list_binary)\n",
    "                _, p_value_w = stats.wilcoxon(orig_list_binary, mod_list_binary)\n",
    "                p_value = min(p_value_mw, p_value_w)  # Use the more conservative p-value\n",
    "            except ValueError:\n",
    "                # If all elements are identical, set p-value to 1.0 since there is no difference\n",
    "                p_value = 1.0\n",
    "                \n",
    "            # Add significance level\n",
    "            if p_value < 0.01:\n",
    "                significance = '**'\n",
    "            elif p_value < 0.05:\n",
    "                significance = '*'\n",
    "            elif p_value < 0.1:\n",
    "                significance = '.'\n",
    "            else:\n",
    "                significance = 'ns'\n",
    "            \n",
    "            row = {\n",
    "                'model': model,\n",
    "                'modification': mod,\n",
    "                'original_acc': decimal.Decimal(orig_acc).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "                'modified_acc': decimal.Decimal(mod_acc).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "                'percentage_diff': decimal.Decimal(pct_diff).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "                'weighted_delta': decimal.Decimal(weighted_delta).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "                'p_value': decimal.Decimal(p_value).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "                'significance': significance,\n",
    "                'significant': 'Yes' if p_value < 0.05 else 'No'\n",
    "            }\n",
    "            results_rows.append(row)\n",
    "            \n",
    "            # Add rows for each negation type if this is negation mod\n",
    "            if mod == 'negation':\n",
    "                for neg_type, results in results_by_type.items():\n",
    "                    type_orig_acc = results['orig_correct'] / results['total'] * 100 if results['total'] > 0 else 0\n",
    "                    type_mod_acc = results['mod_correct'] / results['total'] * 100 if results['total'] > 0 else 0\n",
    "                    type_pct_diff = ((type_mod_acc - type_orig_acc) / type_orig_acc) * 100 if type_orig_acc > 0 else 0\n",
    "                    type_weighted_delta = (type_mod_acc - type_orig_acc) * np.log10(type_orig_acc) / np.log10(100) if type_orig_acc > 0 else 0\n",
    "\n",
    "                    try:\n",
    "                        _, p_value_mw = stats.mannwhitneyu(results['orig_binary'], results['mod_binary'])\n",
    "                        _, p_value_w = stats.wilcoxon(results['orig_binary'], results['mod_binary'])\n",
    "                        p_value = min(p_value_mw, p_value_w)  # Use the more conservative p-value\n",
    "                    except ValueError:\n",
    "                        # If all elements are identical, set p-value to 1.0 since there is no difference\n",
    "                        p_value = 1.0\n",
    "                        \n",
    "                    # Add significance level\n",
    "                    if p_value < 0.01:\n",
    "                        significance = '**'\n",
    "                    elif p_value < 0.05:\n",
    "                        significance = '*'\n",
    "                    elif p_value < 0.1:\n",
    "                        significance = '.'\n",
    "                    else:\n",
    "                        significance = 'ns'\n",
    "\n",
    "                    type_row = {\n",
    "                        'model': model,\n",
    "                        'modification': neg_type,\n",
    "                        'original_acc': decimal.Decimal(type_orig_acc).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "                        'modified_acc': decimal.Decimal(type_mod_acc).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "                        'pct_diff': decimal.Decimal(type_pct_diff).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "                        'weighted_delta': decimal.Decimal(type_weighted_delta).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "                        'wilcoxon_pvalue': decimal.Decimal(p_value_w).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "                        'mannwhitney_pvalue': decimal.Decimal(p_value_mw).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "                        'pvalue': decimal.Decimal(p_value).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "                        'significance': significance,\n",
    "                        'significant': p_value < 0.05\n",
    "                    }\n",
    "                    negation_results_rows.append(type_row)\n",
    "\n",
    "# Convert to dataframes\n",
    "results_df = pd.DataFrame(results_rows)\n",
    "negation_results_df = pd.DataFrame(negation_results_rows)\n",
    "\n",
    "# Save results\n",
    "results_df.to_csv(base_path / 'dialogue_plm_results.csv', index=False)\n",
    "negation_results_df.to_csv(base_path / 'dialogue_plm_negation_results.csv', index=False)\n",
    "\n",
    "# Display results\n",
    "print(\"Results by modification and model:\")\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Results by modification and model:\n",
      "\n",
      "Negation Results by type and model:\n"
     ]
    }
   ],
   "source": [
    "# Also analyze results from eval/results/rongxin/\n",
    "eval_base_path = Path('../eval/results/rongxin')\n",
    "eval_results_rows = []\n",
    "eval_negation_results_rows = []\n",
    "\n",
    "for mod in os.listdir(eval_base_path):\n",
    "    if not mod.endswith('_100.csv'):\n",
    "        continue\n",
    "    model = mod.split('-0shot-')[0]\n",
    "    if model == 'mixtral':\n",
    "        continue\n",
    "    mod = mod.split('-0shot-')[1].replace('_100.csv', '')\n",
    "    # Load predictions from CSV\n",
    "    eval_filepath = eval_base_path / f'{model}-0shot-{mod}_100.csv'\n",
    "    if not eval_filepath.exists():\n",
    "        continue\n",
    "    df = pd.read_csv(eval_filepath)\n",
    "    \n",
    "    compare_file = Path(f'../preprocessing/data_after_phase2/rongxin/{mod}_100.json')\n",
    "    if not compare_file.exists():\n",
    "        continue\n",
    "    compare_df = json.load(open(compare_file))\n",
    "    if len(df) != len(compare_df):\n",
    "        print(f\"Warning: Length mismatch for {mod} {model}\")\n",
    "    \n",
    "    # Calculate accuracies\n",
    "    orig_correct = sum(df['original_pred'] == df['original_label'])\n",
    "    mod_correct = sum(df['modified_pred'] == df['modified_label'])\n",
    "    total = len(df)\n",
    "        \n",
    "    orig_acc = orig_correct / total * 100 if total > 0 else 0\n",
    "    mod_acc = mod_correct / total * 100 if total > 0 else 0\n",
    "    pct_diff = ((mod_acc - orig_acc) / orig_acc) * 100 if orig_acc > 0 else 0\n",
    "    \n",
    "    # Calculate weighted delta\n",
    "    weighted_delta = (mod_acc - orig_acc) * np.log10(orig_acc) / np.log10(100) if orig_acc > 0 else 0\n",
    "    \n",
    "    # Convert predictions to binary (0/1) based on correctness for t-test\n",
    "    orig_binary = (df['original_pred'] == df['original_label']).astype(int)\n",
    "    mod_binary = (df['modified_pred'] == df['modified_label']).astype(int)\n",
    "    \n",
    "    # Perform paired t-test on binary correctness values\n",
    "    try:\n",
    "        _, p_value_mw = stats.mannwhitneyu(orig_binary, mod_binary)\n",
    "        _, p_value_wilc = stats.wilcoxon(orig_binary, mod_binary)\n",
    "        p_value = min(p_value_mw, p_value_wilc)  # Use most conservative p-value\n",
    "    except ValueError:\n",
    "        # If all elements are identical, set p-value to 1.0 since there is no difference\n",
    "        p_value = 1.0\n",
    "        \n",
    "    # Add significance level\n",
    "    if p_value < 0.01:\n",
    "        significance = '**'\n",
    "    elif p_value < 0.05:\n",
    "        significance = '*'\n",
    "    elif p_value < 0.1:\n",
    "        significance = '.'\n",
    "    else:\n",
    "        significance = 'ns'\n",
    "    \n",
    "    row = {\n",
    "        'model': model,\n",
    "        'modification': mod,\n",
    "        'original_acc': decimal.Decimal(orig_acc).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "        'modified_acc': decimal.Decimal(mod_acc).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "        'percentage_diff': decimal.Decimal(pct_diff).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "        'weighted_delta': decimal.Decimal(weighted_delta).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "        'p_value': decimal.Decimal(p_value).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "        'significance': significance,\n",
    "        'significant': p_value < 0.05\n",
    "    }\n",
    "    eval_results_rows.append(row)\n",
    "    \n",
    "    # Add rows for each negation type if this is negation mod\n",
    "    if mod == 'negation':\n",
    "        # Verify negation types match expected values\n",
    "        expected_types = {'absolute', 'double', 'lexical', 'approximate', 'verbal'}\n",
    "        actual_types = set(df['type'].unique())\n",
    "        if actual_types != expected_types:\n",
    "            print(f\"Warning: Unexpected negation types for {model}\")\n",
    "            print(f\"Expected: {expected_types}\")\n",
    "            print(f\"Found: {actual_types}\")\n",
    "            \n",
    "        for neg_type in df['type'].unique():\n",
    "            type_df = df[df['type'] == neg_type]\n",
    "            \n",
    "            type_orig_correct = sum(type_df['original_pred'] == type_df['original_label'])\n",
    "            type_mod_correct = sum(type_df['modified_pred'] == type_df['modified_label'])\n",
    "            type_total = len(type_df)\n",
    "            \n",
    "            type_orig_acc = type_orig_correct / type_total * 100 if type_total > 0 else 0\n",
    "            type_mod_acc = type_mod_correct / type_total * 100 if type_total > 0 else 0\n",
    "            type_pct_diff = ((type_mod_acc - type_orig_acc) / type_orig_acc) * 100 if type_orig_acc > 0 else 0\n",
    "            \n",
    "            # Calculate weighted delta for this type\n",
    "            type_weighted_delta = (type_mod_acc - type_orig_acc) * np.log10(type_orig_acc) / np.log10(100) if type_orig_acc > 0 else 0\n",
    "            \n",
    "            # Statistical tests for this negation type\n",
    "            type_orig_binary = (type_df['original_pred'] == type_df['original_label']).astype(int)\n",
    "            type_mod_binary = (type_df['modified_pred'] == type_df['modified_label']).astype(int)\n",
    "            \n",
    "            try:\n",
    "                _, type_p_value_mw = stats.mannwhitneyu(type_orig_binary, type_mod_binary)\n",
    "                _, type_p_value_wilc = stats.wilcoxon(type_orig_binary, type_mod_binary)\n",
    "                type_p_value = min(type_p_value_mw, type_p_value_wilc)\n",
    "            except ValueError:\n",
    "                type_p_value = 1.0\n",
    "                \n",
    "            # Add significance level\n",
    "            if type_p_value < 0.01:\n",
    "                significance = '**'\n",
    "            elif type_p_value < 0.05:\n",
    "                significance = '*'\n",
    "            elif type_p_value < 0.1:\n",
    "                significance = '.'\n",
    "            else:\n",
    "                significance = 'ns'\n",
    "                \n",
    "            type_row = {\n",
    "                'model': model,\n",
    "                'modification': neg_type,\n",
    "                'original_acc': decimal.Decimal(type_orig_acc).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "                'modified_acc': decimal.Decimal(type_mod_acc).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "                'pct_diff': decimal.Decimal(type_pct_diff).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "                'weighted_delta': decimal.Decimal(type_weighted_delta).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "                'wilcoxon_pvalue': decimal.Decimal(type_p_value_wilc).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "                'mannwhitney_pvalue': decimal.Decimal(type_p_value_mw).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "                'pvalue': decimal.Decimal(type_p_value).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "                'significance': significance,\n",
    "                'significant': type_p_value < 0.05\n",
    "            }\n",
    "            eval_negation_results_rows.append(type_row)\n",
    "\n",
    "# Convert to dataframes\n",
    "eval_results_df = pd.DataFrame(eval_results_rows)\n",
    "eval_negation_results_df = pd.DataFrame(eval_negation_results_rows)\n",
    "\n",
    "# Save results\n",
    "eval_results_df.to_csv(base_path / 'dialogue_llm_results.csv', index=False)\n",
    "eval_negation_results_df.to_csv(base_path / 'dialogue_llm_negation_results.csv', index=False)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nEvaluation Results by modification and model:\")\n",
    "# print(eval_results_df)\n",
    "print(\"\\nNegation Results by type and model:\")\n",
    "# print(eval_negation_results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combined Results:\n",
      "              model         modification  original_acc  modified_acc  \\\n",
      "78  bert-base-cased    active_to_passive        87.000        90.000   \n",
      "63  bert-base-cased       capitalization        92.708        92.708   \n",
      "54  bert-base-cased               casual        90.000        82.000   \n",
      "72  bert-base-cased        compound_word        89.000        85.000   \n",
      "84  bert-base-cased  concept_replacement        89.000        86.000   \n",
      "..              ...                  ...           ...           ...   \n",
      "83          t5-base          punctuation        86.000        86.000   \n",
      "98          t5-base            sentiment        84.000        81.000   \n",
      "62          t5-base             singlish        83.962        81.132   \n",
      "68          t5-base        temporal_bias        81.000        76.000   \n",
      "92          t5-base            typo_bias        84.000        82.000   \n",
      "\n",
      "    percentage_diff  weighted_delta  p_value significance significant  \n",
      "78            3.448           2.909    0.317           ns          No  \n",
      "63            0.000           0.000    1.000           ns          No  \n",
      "54           -8.889          -7.817    0.021            *         Yes  \n",
      "72           -4.494          -3.899    0.102           ns          No  \n",
      "84           -3.371          -2.924    0.257           ns          No  \n",
      "..              ...             ...      ...          ...         ...  \n",
      "83            0.000           0.000    1.000           ns          No  \n",
      "98           -3.571          -2.886    0.366           ns          No  \n",
      "62           -3.371          -2.723    0.366           ns          No  \n",
      "68           -6.173          -4.771    0.059            .          No  \n",
      "92           -2.381          -1.924    0.317           ns          No  \n",
      "\n",
      "[108 rows x 9 columns]\n",
      "\n",
      "Combined Negation Results:\n",
      "                model modification  original_acc  modified_acc  pct_diff  \\\n",
      "15    bert-base-cased     absolute        84.615        61.538   -27.273   \n",
      "18    bert-base-cased  approximate       100.000        73.077   -26.923   \n",
      "16    bert-base-cased       double        63.636        72.727    14.286   \n",
      "17    bert-base-cased      lexical        93.750        65.625   -30.000   \n",
      "19    bert-base-cased       verbal        83.333        61.111   -26.667   \n",
      "8   claude-3-5-sonnet     absolute       100.000        69.231   -30.769   \n",
      "9   claude-3-5-sonnet  approximate       100.000        76.923   -23.077   \n",
      "6   claude-3-5-sonnet       double       100.000        63.636   -36.364   \n",
      "5   claude-3-5-sonnet      lexical        87.500        59.375   -32.143   \n",
      "7   claude-3-5-sonnet       verbal        88.889        83.333    -6.250   \n",
      "20               gpt2     absolute        84.615        53.846   -36.364   \n",
      "23               gpt2  approximate        92.308        69.231   -25.000   \n",
      "21               gpt2       double        90.909        63.636   -30.000   \n",
      "22               gpt2      lexical        90.625        59.375   -34.483   \n",
      "24               gpt2       verbal        77.778        61.111   -21.429   \n",
      "13              gpt4o     absolute        92.308        61.538   -33.333   \n",
      "14              gpt4o  approximate        92.308        76.923   -16.667   \n",
      "11              gpt4o       double        81.818        63.636   -22.222   \n",
      "10              gpt4o      lexical        84.375        53.125   -37.037   \n",
      "12              gpt4o       verbal        94.444       100.000     5.882   \n",
      "3               llama     absolute       100.000        61.538   -38.462   \n",
      "4               llama  approximate        96.154        76.923   -20.000   \n",
      "1               llama       double        90.909        45.455   -50.000   \n",
      "0               llama      lexical        87.500        56.250   -35.714   \n",
      "2               llama       verbal       100.000        72.222   -27.778   \n",
      "25            t5-base     absolute        92.308        53.846   -41.667   \n",
      "28            t5-base  approximate        92.308        73.077   -20.833   \n",
      "26            t5-base       double        81.818        63.636   -22.222   \n",
      "27            t5-base      lexical        93.750        65.625   -30.000   \n",
      "29            t5-base       verbal        83.333        77.778    -6.667   \n",
      "\n",
      "    weighted_delta  wilcoxon_pvalue  mannwhitney_pvalue  pvalue significance  \\\n",
      "15         -22.240            0.375               0.205   0.205           ns   \n",
      "18         -26.923            0.008               0.005   0.005           **   \n",
      "16           8.199            1.000               0.684   0.684           ns   \n",
      "17         -27.731            0.003               0.006   0.003           **   \n",
      "19         -21.342            0.102               0.148   0.102           ns   \n",
      "8          -30.769            0.125               0.037   0.037            *   \n",
      "9          -23.077            0.014               0.010   0.010            *   \n",
      "6          -36.364            0.125               0.035   0.035            *   \n",
      "5          -27.309            0.007               0.012   0.007           **   \n",
      "7           -5.413            0.655               0.654   0.654           ns   \n",
      "20         -29.653            0.219               0.102   0.102           ns   \n",
      "23         -22.676            0.058               0.038   0.038            *   \n",
      "21         -26.708            0.375               0.148   0.148           ns   \n",
      "22         -30.582            0.004               0.004   0.004           **   \n",
      "24         -15.757            0.257               0.294   0.257           ns   \n",
      "13         -30.234            0.219               0.073   0.073            .   \n",
      "14         -15.117            0.102               0.132   0.102           ns   \n",
      "11         -17.390            0.688               0.372   0.372           ns   \n",
      "10         -30.097            0.004               0.008   0.004           **   \n",
      "12           5.487            0.317               0.345   0.317           ns   \n",
      "3          -38.462            0.063               0.016   0.016            *   \n",
      "4          -19.067            0.025               0.046   0.025            *   \n",
      "1          -44.514            0.063               0.028   0.028            *   \n",
      "0          -30.344            0.002               0.006   0.002           **   \n",
      "2          -27.778            0.025               0.019   0.019            *   \n",
      "25         -37.793            0.063               0.033   0.033            *   \n",
      "28         -18.897            0.059               0.072   0.059            .   \n",
      "26         -17.390            0.500               0.372   0.372           ns   \n",
      "27         -27.731            0.007               0.006   0.006           **   \n",
      "29          -5.336            0.705               0.695   0.695           ns   \n",
      "\n",
      "    significant  \n",
      "15        False  \n",
      "18         True  \n",
      "16        False  \n",
      "17         True  \n",
      "19        False  \n",
      "8          True  \n",
      "9          True  \n",
      "6          True  \n",
      "5          True  \n",
      "7         False  \n",
      "20        False  \n",
      "23         True  \n",
      "21        False  \n",
      "22         True  \n",
      "24        False  \n",
      "13        False  \n",
      "14        False  \n",
      "11        False  \n",
      "10         True  \n",
      "12        False  \n",
      "3          True  \n",
      "4          True  \n",
      "1          True  \n",
      "0          True  \n",
      "2          True  \n",
      "25         True  \n",
      "28        False  \n",
      "26        False  \n",
      "27         True  \n",
      "29        False  \n"
     ]
    }
   ],
   "source": [
    "# Load LLM and PLM results\n",
    "base_path = Path('../pretrained_language_models/dialogue_contradiction_detection/tmp')\n",
    "llm_results = pd.read_csv(base_path / 'dialogue_llm_results.csv')\n",
    "plm_results = pd.read_csv(base_path / 'dialogue_plm_results.csv')\n",
    "\n",
    "# Load LLM and PLM negation results\n",
    "llm_neg_results = pd.read_csv(base_path / 'dialogue_llm_negation_results.csv')\n",
    "plm_neg_results = pd.read_csv(base_path / 'dialogue_plm_negation_results.csv')\n",
    "\n",
    "# Combine the regular results\n",
    "combined_results = pd.concat([llm_results, plm_results], ignore_index=True)\n",
    "\n",
    "# Combine the negation results\n",
    "combined_neg_results = pd.concat([llm_neg_results, plm_neg_results], ignore_index=True)\n",
    "\n",
    "# Sort by model and modification\n",
    "combined_results = combined_results.sort_values(['model', 'modification'])\n",
    "combined_neg_results = combined_neg_results.sort_values(['model', 'modification'])\n",
    "\n",
    "# Save combined results\n",
    "combined_results.to_csv(base_path / 'dialogue_combined_results.csv', index=False)\n",
    "combined_neg_results.to_csv(base_path / 'dialogue_combined_negation_results.csv', index=False)\n",
    "\n",
    "print(\"\\nCombined Results:\")\n",
    "print(combined_results)\n",
    "print(\"\\nCombined Negation Results:\")\n",
    "print(combined_neg_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LaTeX table saved to .tex\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "base_path = Path('../pretrained_language_models/dialogue_contradiction_detection/tmp')\n",
    "modification_order = [\n",
    "    \"Temporal\", \"Geographical\", \"Length\",\n",
    "    \"Spelling\", \"Capitalization\", \"Punctuation\", \n",
    "    \"Derivation\", \"Compound\",\n",
    "    \"Voice\", \"Grammar\", \"Conjunction\",\n",
    "    \"Concept\", \"Negation\", \n",
    "    \"Discourse\", \"Sentiment\",\n",
    "    \"Casual\", \"Dialectal\", \"Singlish\"\n",
    "]\n",
    "\n",
    "# Read and process data\n",
    "df = pd.read_csv(base_path / 'dialogue_combined_results.csv')\n",
    "df['modification'] = df['modification'].map({\n",
    "    'temporal_bias': 'Temporal', 'geographical_bias': 'Geographical', 'length_bias': 'Length',\n",
    "    'typo_bias': 'Spelling', 'capitalization': 'Capitalization', 'punctuation': 'Punctuation',\n",
    "    'derivation': 'Derivation', 'compound_word': 'Compound', 'active_to_passive': 'Voice',\n",
    "    'grammatical_role': 'Grammar', 'coordinating_conjunction': 'Conjunction',\n",
    "    'concept_replacement': 'Concept', 'negation': 'Negation', 'discourse': 'Discourse',\n",
    "    'sentiment': 'Sentiment', 'casual': 'Casual', 'dialectal': 'Dialectal',\n",
    "    'singlish': 'Singlish'\n",
    "})\n",
    "\n",
    "models = {\n",
    "    'bert-base-cased': 'BERT', 'gpt2': 'GPT-2', 't5-base': 'T5',\n",
    "    'gpt4o': 'GPT-4o', 'claude-3-5-sonnet': 'Claude 3.5', 'llama': 'Llama 3.1'\n",
    "}\n",
    "\n",
    "# Calculate weighted delta instead of percentage difference\n",
    "\n",
    "# Pivot data\n",
    "pivot_df = df.pivot(index='modification', columns='model', values='weighted_delta')\n",
    "p_values = df.pivot(index='modification', columns='model', values='p_value')\n",
    "pivot_df = pivot_df.reindex(modification_order).reindex(models.keys(), axis=1)\n",
    "p_values = p_values.reindex(modification_order).reindex(models.keys(), axis=1)\n",
    "\n",
    "# Get mean original accuracy for each model\n",
    "model_original_acc = df.groupby('model')['original_acc'].mean()\n",
    "\n",
    "# Generate LaTeX table with adjustbox\n",
    "latex = [\n",
    "    '\\\\begin{table}[h]',\n",
    "    '\\\\centering',\n",
    "    '\\\\footnotesize',\n",
    "    '\\\\begin{adjustbox}{max width=\\\\linewidth}',\n",
    "    '\\\\begin{tabular}{llc' + 'c'*(len(models)) + '}',\n",
    "    '\\\\hline',\n",
    "    'Category & Modification & ' + ' & '.join([f'\\\\textbf{{{models[col]}}}' for col in pivot_df.columns]) + ' \\\\\\\\',\n",
    "    '\\\\hline'\n",
    "]\n",
    "\n",
    "# Add mean original accuracy row with textit and grey background\n",
    "cells = []\n",
    "for col in pivot_df.columns:\n",
    "    val = model_original_acc[col]\n",
    "    cells.append(f'{val:.2f}')\n",
    "latex.append('\\\\rowcolor{gray!20}\\\\textit{Original} & & ' + ' & '.join(cells) + ' \\\\\\\\')\n",
    "\n",
    "categories = {\n",
    "    'Temporal': 'Bias',\n",
    "    'Geographical': 'Bias', \n",
    "    'Length': 'Bias',\n",
    "    'Spelling': 'Orthographic',\n",
    "    'Capitalization': 'Orthographic',\n",
    "    'Punctuation': 'Orthographic',\n",
    "    'Derivation': 'Morphological',\n",
    "    'Compound': 'Morphological',\n",
    "    'Voice': 'Syntactic',\n",
    "    'Grammar': 'Syntactic',\n",
    "    'Conjunction': 'Syntactic',\n",
    "    'Concept': 'Semantic',\n",
    "    'Negation': 'Semantic',\n",
    "    'Discourse': 'Pragmatic',\n",
    "    'Sentiment': 'Pragmatic',\n",
    "    'Casual': 'Genre',\n",
    "    'Dialectal': 'Genre',\n",
    "    'Singlish': 'Genre'\n",
    "}\n",
    "\n",
    "prev_category = None\n",
    "for idx, row in pivot_df.iterrows():\n",
    "    current_category = categories[idx]\n",
    "    if prev_category is not None and current_category != prev_category:\n",
    "        latex.append('\\\\hline')\n",
    "    \n",
    "    cells = []\n",
    "    for col, val in row.items():\n",
    "        if np.isnan(val):\n",
    "            cells.append('')\n",
    "            continue\n",
    "        color = 'green' if val > 0 else 'red'\n",
    "        intensity = min(abs(val)/10, 1) * 30  # Adjusted scale for percentage\n",
    "        val_str = f'{val:+.2f}' if val > 0 else f'{val:.2f}'  # Show as percentage\n",
    "        p = p_values.loc[idx, col]\n",
    "        if p < 0.01: val_str = f'\\\\textbf{{{val_str}}}**'\n",
    "        elif p < 0.05: val_str = f'\\\\textbf{{{val_str}}}*'\n",
    "        elif p < 0.1: val_str = f'\\\\textbf{{{val_str}}}'\n",
    "        cells.append(f'\\\\cellcolor{{{color}!{int(intensity)}}} {val_str}')\n",
    "    \n",
    "    category_text = f'\\\\textbf{{{current_category}}}' if current_category != prev_category else ''\n",
    "    latex.append(f'{category_text} & \\\\textbf{{{idx}}} & ' + ' & '.join(cells) + ' \\\\\\\\')\n",
    "    prev_category = current_category\n",
    "\n",
    "latex.extend([\n",
    "    '\\\\hline',\n",
    "    '\\\\end{tabular}',\n",
    "    '\\\\end{adjustbox}',\n",
    "    '\\\\caption{Percentage Change in Micro F1 Score by Model and Modification Type}',\n",
    "    '\\\\label{tab:dialogue_results_table}',\n",
    "    '\\\\end{table}'\n",
    "])\n",
    "\n",
    "with open(base_path / 'dialogue_results_table.tex', 'w') as f:\n",
    "    f.write('\\n'.join(latex))\n",
    "\n",
    "print(\"LaTeX table saved to .tex\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to dialogue_results_df.csv and dialogue_negation_results_df.csv\n"
     ]
    }
   ],
   "source": [
    "# Define modification mapping\n",
    "mod_mapping = {\n",
    "    'temporal_bias': 'Temporal',\n",
    "    'geographical_bias': 'Geographical', \n",
    "    'length_bias': 'Length',\n",
    "    'spelling': 'Spelling',\n",
    "    'capitalization': 'Capitalization',\n",
    "    'punctuation': 'Punctuation',\n",
    "    'derivation': 'Derivation',\n",
    "    'compound': 'Compound',\n",
    "    'voice': 'Voice',\n",
    "    'grammar': 'Grammar',\n",
    "    'conjunction': 'Conjunction',\n",
    "    'concept': 'Concept',\n",
    "    'negation': 'Negation',\n",
    "    'discourse': 'Discourse',\n",
    "    'sentiment': 'Sentiment',\n",
    "    'casual': 'Casual',\n",
    "    'dialectal': 'Dialectal',\n",
    "    'singlish': 'Singlish'\n",
    "}\n",
    "\n",
    "# Load results from CSV\n",
    "base_path = Path('../pretrained_language_models/dialogue_contradiction_detection/tmp')\n",
    "\n",
    "results_df = pd.read_csv(base_path / 'dialogue_combined_results.csv')\n",
    "\n",
    "# Create lists of unique modifications and models\n",
    "modification_order = list(mod_mapping.keys())\n",
    "negation_order = ['verbal', 'lexical', 'double', 'approximate', 'absolute']\n",
    "model_order = ['bert-base-cased', 'gpt2', 't5-base', 'gpt4o', 'claude-3-5-sonnet', 'llama']\n",
    "\n",
    "# Create empty DataFrame with multi-level columns\n",
    "columns = pd.MultiIndex.from_product([model_order, ['original', 'modified', 'diff']])\n",
    "results_df_pivot = pd.DataFrame(index=modification_order, columns=columns)\n",
    "\n",
    "# Fill DataFrame\n",
    "for mod in modification_order:\n",
    "    for model in model_order:\n",
    "        row = results_df[(results_df['modification'] == mod) & (results_df['model'] == model)]\n",
    "        if not row.empty:\n",
    "            results_df_pivot.loc[mod, (model, 'original')] = row['original_acc'].values[0]\n",
    "            results_df_pivot.loc[mod, (model, 'modified')] = row['modified_acc'].values[0]\n",
    "            results_df_pivot.loc[mod, (model, 'diff')] = row['percentage_diff'].values[0]\n",
    "            if mod == 'temporal_bias' and model == 'bert':\n",
    "                print(row)\n",
    "\n",
    "# Save to CSV\n",
    "results_df_pivot.to_csv(base_path / 'dialogue_results_df.csv')\n",
    "\n",
    "# Save negation results to separate CSV\n",
    "# Load negation results from CSV\n",
    "negation_results_df = pd.read_csv(base_path / 'dialogue_combined_negation_results.csv')\n",
    "\n",
    "# Create empty DataFrame with multi-level columns for negation results\n",
    "negation_df_pivot = pd.DataFrame(index=['negation'], columns=columns)\n",
    "\n",
    "# Fill DataFrame with negation results\n",
    "for mod in negation_order:  \n",
    "    for model in model_order:\n",
    "        row = negation_results_df[(negation_results_df['modification'] == mod) & (negation_results_df['model'] == model)]\n",
    "        if not row.empty:\n",
    "            negation_df_pivot.loc[mod, (model, 'original')] = row['original_acc'].values[0]\n",
    "            negation_df_pivot.loc[mod, (model, 'modified')] = row['modified_acc'].values[0] \n",
    "            negation_df_pivot.loc[mod, (model, 'diff')] = row['pct_diff'].values[0]\n",
    "\n",
    "# Save negation results to CSV\n",
    "negation_df_pivot.to_csv(base_path / 'dialogue_negation_results_df.csv')\n",
    "\n",
    "print(\"Results saved to dialogue_results_df.csv and dialogue_negation_results_df.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from scipy import stats\n",
    "\n",
    "# Base path for results\n",
    "base_path = Path('../pretrained_language_models/coreference_resolution/tmp')\n",
    "\n",
    "# Function to load and convert predictions to binary\n",
    "def load_predictions(filepath):\n",
    "    with open(filepath) as f:\n",
    "        preds = json.load(f)\n",
    "    return preds\n",
    "\n",
    "# Load original model predictions\n",
    "model_orig_preds = {}\n",
    "model_names = ['bert-base-cased', 'gpt2', 't5-base']\n",
    "\n",
    "for model in model_names:\n",
    "    filepath = base_path / f'{model}_results/{model}_predictions.json'\n",
    "    model_orig_preds[model] = load_predictions(filepath)\n",
    "\n",
    "# Load predictions for each modification\n",
    "modifications = []\n",
    "for model in model_names:\n",
    "    mod_path = base_path / f'{model}_results'\n",
    "    if mod_path.exists():\n",
    "        # Get all CSV files containing predictions\n",
    "        modifications.extend([f.stem for f in mod_path.glob('*_predictions.csv')])\n",
    "modifications = list(set(modifications))  # Remove duplicates\n",
    "modifications = [mod.replace('_predictions', '') for mod in modifications]\n",
    "\n",
    "# Load negation types from GPT4 results\n",
    "gpt4_negation_df = pd.read_csv('../eval/results/thinh/gpt4o-0shot-negation_100.csv')\n",
    "negation_types = gpt4_negation_df['type'].tolist()\n",
    "\n",
    "# Sanity check negation types\n",
    "valid_types = {'absolute', 'double', 'lexical', 'approximate', 'verbal'}\n",
    "for neg_type in negation_types:\n",
    "    if neg_type not in valid_types:\n",
    "        print(f\"WARNING: Invalid negation type found: {neg_type}\")\n",
    "\n",
    "# Create results list to store accuracy and statistical test results\n",
    "results_rows = []\n",
    "negation_results_rows = []\n",
    "\n",
    "for mod in modifications:\n",
    "    for model in model_names:\n",
    "        # Get original predictions\n",
    "        orig_preds = model_orig_preds[model]\n",
    "        \n",
    "        # Get modified predictions from CSV file\n",
    "        mod_filepath = base_path / f'{model}_results/{mod}_predictions.csv'\n",
    "        if mod_filepath.exists():\n",
    "            mod_df = pd.read_csv(mod_filepath)\n",
    "            # Calculate accuracies\n",
    "            orig_correct = 0\n",
    "            mod_correct = 0\n",
    "            total = 0\n",
    "            orig_list = []\n",
    "            mod_list = []\n",
    "            \n",
    "            # Track results by negation type if this is negation mod\n",
    "            if mod == 'negation':\n",
    "                results_by_type = {\n",
    "                    'absolute': {'orig_correct': 0, 'mod_correct': 0, 'total': 0, 'orig_binary': [], 'mod_binary': []},\n",
    "                    'double': {'orig_correct': 0, 'mod_correct': 0, 'total': 0, 'orig_binary': [], 'mod_binary': []},\n",
    "                    'lexical': {'orig_correct': 0, 'mod_correct': 0, 'total': 0, 'orig_binary': [], 'mod_binary': []},\n",
    "                    'approximate': {'orig_correct': 0, 'mod_correct': 0, 'total': 0, 'orig_binary': [], 'mod_binary': []},\n",
    "                    'verbal': {'orig_correct': 0, 'mod_correct': 0, 'total': 0, 'orig_binary': [], 'mod_binary': []}\n",
    "                }\n",
    "            \n",
    "            # Load labels from eval/results/rongxin CSV\n",
    "            eval_filepath = Path(f'../preprocessing/data_after_phase2/thinh/{mod}_100.json')\n",
    "            if eval_filepath.exists():\n",
    "                label_df = json.load(open(eval_filepath))\n",
    "            if len(mod_df) != len(label_df):\n",
    "                print(mod, model)\n",
    "                \n",
    "            for idx, row in mod_df.iterrows():\n",
    "                # Get ground truth labels from JSON in order\n",
    "                orig_label = label_df[idx]['original_label'] if 'original_label' in label_df[idx] else label_df[idx]['label']\n",
    "                mod_label = label_df[idx]['modified_label'] if 'modified_label' in label_df[idx] else label_df[idx]['label']\n",
    "                \n",
    "                # Get predictions from CSV\n",
    "                orig_pred = row['original'] if 'original' in row else row['original_pred']\n",
    "                mod_pred = row['modified'] if 'modified' in row else row['modified_pred']\n",
    "                \n",
    "                # Compare predictions with ground truth\n",
    "                orig_correct_bool = orig_pred == orig_label\n",
    "                mod_correct_bool = mod_pred == mod_label\n",
    "                \n",
    "                if orig_correct_bool:\n",
    "                    orig_correct += 1\n",
    "                    if mod == 'negation':\n",
    "                        if idx >= len(negation_types):\n",
    "                            print(f\"WARNING: Index {idx} exceeds negation types list length\")\n",
    "                            continue\n",
    "                        results_by_type[negation_types[idx]]['orig_correct'] += 1\n",
    "                if mod_correct_bool:\n",
    "                    mod_correct += 1\n",
    "                    if mod == 'negation':\n",
    "                        if idx >= len(negation_types):\n",
    "                            continue\n",
    "                        results_by_type[negation_types[idx]]['mod_correct'] += 1\n",
    "                        \n",
    "                if mod == 'negation':\n",
    "                    if idx >= len(negation_types):\n",
    "                        continue\n",
    "                    results_by_type[negation_types[idx]]['total'] += 1\n",
    "                    results_by_type[negation_types[idx]]['orig_binary'].append(1 if orig_correct_bool else 0)\n",
    "                    results_by_type[negation_types[idx]]['mod_binary'].append(1 if mod_correct_bool else 0)\n",
    "                    \n",
    "                orig_list.append(orig_pred)\n",
    "                mod_list.append(mod_pred)\n",
    "                total += 1\n",
    "                \n",
    "            orig_acc = orig_correct / total * 100 if total > 0 else 0\n",
    "            mod_acc = mod_correct / total * 100 if total > 0 else 0\n",
    "            pct_diff = ((mod_acc - orig_acc) / orig_acc) * 100 if orig_acc > 0 else 0\n",
    "            weighted_delta = (mod_acc - orig_acc) * np.log10(orig_acc) / np.log10(100) if orig_acc > 0 else 0\n",
    "            \n",
    "            orig_list_binary = [1 if pred == label else 0 for pred, label in zip(orig_list, [label_df[i]['original_label'] if 'original_label' in label_df[i] else label_df[i]['label'] for i in range(len(orig_list))])]\n",
    "            mod_list_binary = [1 if pred == label else 0 for pred, label in zip(mod_list, [label_df[i]['modified_label'] if 'modified_label' in label_df[i] else label_df[i]['label'] for i in range(len(mod_list))])]\n",
    "\n",
    "            # Perform paired t-test on the raw predictions (0/1)\n",
    "            try:\n",
    "                _, p_value_mw = stats.mannwhitneyu(orig_list_binary, mod_list_binary)\n",
    "                _, p_value_w = stats.wilcoxon(orig_list_binary, mod_list_binary)\n",
    "                p_value = min(p_value_mw, p_value_w)  # Use the more conservative p-value\n",
    "            except ValueError:\n",
    "                # If all elements are identical, set p-value to 1.0 since there is no difference\n",
    "                p_value = 1.0\n",
    "                \n",
    "            # Add significance level\n",
    "            if p_value < 0.01:\n",
    "                significance = '**'\n",
    "            elif p_value < 0.05:\n",
    "                significance = '*'\n",
    "            elif p_value < 0.1:\n",
    "                significance = '.'\n",
    "            else:\n",
    "                significance = 'ns'\n",
    "            \n",
    "            row = {\n",
    "                'model': model,\n",
    "                'modification': mod,\n",
    "                'original_acc': decimal.Decimal(orig_acc).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "                'modified_acc': decimal.Decimal(mod_acc).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "                'percentage_diff': decimal.Decimal(pct_diff).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "                'weighted_delta': decimal.Decimal(weighted_delta).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "                'p_value': decimal.Decimal(p_value).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "                'significance': significance,\n",
    "                'significant': 'Yes' if p_value < 0.05 else 'No'\n",
    "            }\n",
    "            results_rows.append(row)\n",
    "            \n",
    "            # Add rows for each negation type if this is negation mod\n",
    "            if mod == 'negation':\n",
    "                for neg_type, results in results_by_type.items():\n",
    "                    type_orig_acc = results['orig_correct'] / results['total'] * 100 if results['total'] > 0 else 0\n",
    "                    type_mod_acc = results['mod_correct'] / results['total'] * 100 if results['total'] > 0 else 0\n",
    "                    type_pct_diff = ((type_mod_acc - type_orig_acc) / type_orig_acc) * 100 if type_orig_acc > 0 else 0\n",
    "                    type_weighted_delta = (type_mod_acc - type_orig_acc) * np.log10(type_orig_acc) / np.log10(100) if type_orig_acc > 0 else 0\n",
    "\n",
    "                    try:\n",
    "                        _, p_value_mw = stats.mannwhitneyu(results['orig_binary'], results['mod_binary'])\n",
    "                        _, p_value_w = stats.wilcoxon(results['orig_binary'], results['mod_binary'])\n",
    "                        p_value = min(p_value_mw, p_value_w)  # Use the more conservative p-value\n",
    "                    except ValueError:\n",
    "                        # If all elements are identical, set p-value to 1.0 since there is no difference\n",
    "                        p_value = 1.0\n",
    "                        \n",
    "                    # Add significance level\n",
    "                    if p_value < 0.01:\n",
    "                        significance = '**'\n",
    "                    elif p_value < 0.05:\n",
    "                        significance = '*'\n",
    "                    elif p_value < 0.1:\n",
    "                        significance = '.'\n",
    "                    else:\n",
    "                        significance = 'ns'\n",
    "\n",
    "                    type_row = {\n",
    "                        'model': model,\n",
    "                        'modification': neg_type,\n",
    "                        'original_acc': decimal.Decimal(type_orig_acc).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "                        'modified_acc': decimal.Decimal(type_mod_acc).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "                        'pct_diff': decimal.Decimal(type_pct_diff).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "                        'weighted_delta': decimal.Decimal(type_weighted_delta).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "                        'wilcoxon_pvalue': decimal.Decimal(p_value_w).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "                        'mannwhitney_pvalue': decimal.Decimal(p_value_mw).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "                        'pvalue': decimal.Decimal(p_value).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "                        'significance': significance,\n",
    "                        'significant': p_value < 0.05\n",
    "                    }\n",
    "                    negation_results_rows.append(type_row)\n",
    "\n",
    "# Convert to dataframes\n",
    "results_df = pd.DataFrame(results_rows)\n",
    "negation_results_df = pd.DataFrame(negation_results_rows)\n",
    "\n",
    "# Save results\n",
    "results_df.to_csv(base_path / 'coreference_plm_results.csv', index=False)\n",
    "negation_results_df.to_csv(base_path / 'coreference_plm_negation_results.csv', index=False)\n",
    "\n",
    "# Display results\n",
    "# print(\"Results by modification and model:\")\n",
    "# print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Results by modification and model:\n",
      "\n",
      "Negation Results by type and model:\n"
     ]
    }
   ],
   "source": [
    "# Also analyze results from eval/results/rongxin/\n",
    "eval_base_path = Path('../eval/results/thinh')\n",
    "eval_results_rows = []\n",
    "eval_negation_results_rows = []\n",
    "\n",
    "for mod in os.listdir(eval_base_path):\n",
    "    if not mod.endswith('_100.csv'):\n",
    "        continue\n",
    "    model = mod.split('-0shot-')[0]\n",
    "    if model == 'mixtral':\n",
    "        continue\n",
    "    mod = mod.split('-0shot-')[1].replace('_100.csv', '')\n",
    "    # Load predictions from CSV\n",
    "    eval_filepath = eval_base_path / f'{model}-0shot-{mod}_100.csv'\n",
    "    if not eval_filepath.exists():\n",
    "        continue\n",
    "    df = pd.read_csv(eval_filepath)\n",
    "    \n",
    "    compare_file = Path(f'../preprocessing/data_after_phase2/thinh/{mod}_100.json')\n",
    "    if not compare_file.exists():\n",
    "        continue\n",
    "    compare_df = json.load(open(compare_file))\n",
    "    if len(df) != len(compare_df):\n",
    "        print(f\"Warning: Length mismatch for {mod} {model}\")\n",
    "    \n",
    "    # Calculate accuracies\n",
    "    orig_correct = sum(df['original_pred'] == df['original_label'])\n",
    "    mod_correct = sum(df['modified_pred'] == df['modified_label'])\n",
    "    total = len(df)\n",
    "        \n",
    "    orig_acc = orig_correct / total * 100 if total > 0 else 0\n",
    "    mod_acc = mod_correct / total * 100 if total > 0 else 0\n",
    "    pct_diff = ((mod_acc - orig_acc) / orig_acc) * 100 if orig_acc > 0 else 0\n",
    "    weighted_delta = (mod_acc - orig_acc) * np.log10(orig_acc) / np.log10(100) if orig_acc > 0 else 0\n",
    "    \n",
    "    # Convert predictions to binary (0/1) based on correctness for t-test\n",
    "    orig_binary = (df['original_pred'] == df['original_label']).astype(int)\n",
    "    mod_binary = (df['modified_pred'] == df['modified_label']).astype(int)\n",
    "    \n",
    "    # Perform paired t-test on binary correctness values\n",
    "    try:\n",
    "        _, p_value_mw = stats.mannwhitneyu(orig_binary, mod_binary)\n",
    "        _, p_value_wilc = stats.wilcoxon(orig_binary, mod_binary)\n",
    "        p_value = min(p_value_mw, p_value_wilc)  # Use most conservative p-value\n",
    "    except ValueError:\n",
    "        # If all elements are identical, set p-value to 1.0 since there is no difference\n",
    "        p_value = 1.0\n",
    "        \n",
    "    # Add significance level\n",
    "    if p_value < 0.01:\n",
    "        significance = '**'\n",
    "    elif p_value < 0.05:\n",
    "        significance = '*'\n",
    "    elif p_value < 0.1:\n",
    "        significance = '.'\n",
    "    else:\n",
    "        significance = 'ns'\n",
    "    \n",
    "    row = {\n",
    "        'model': model,\n",
    "        'modification': mod,\n",
    "        'original_acc': decimal.Decimal(orig_acc).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "        'modified_acc': decimal.Decimal(mod_acc).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "        'percentage_diff': decimal.Decimal(pct_diff).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "        'weighted_delta': decimal.Decimal(weighted_delta).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "        'p_value': decimal.Decimal(p_value).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "        'significance': significance,\n",
    "        'significant': p_value < 0.05\n",
    "    }\n",
    "    eval_results_rows.append(row)\n",
    "    \n",
    "    # Add rows for each negation type if this is negation mod\n",
    "    if mod == 'negation':\n",
    "        # Verify negation types match expected values\n",
    "        expected_types = {'absolute', 'double', 'lexical', 'approximate', 'verbal'}\n",
    "        actual_types = set(df['type'].unique())\n",
    "        if actual_types != expected_types:\n",
    "            print(f\"Warning: Unexpected negation types for {model}\")\n",
    "            print(f\"Expected: {expected_types}\")\n",
    "            print(f\"Found: {actual_types}\")\n",
    "            \n",
    "        for neg_type in df['type'].unique():\n",
    "            type_df = df[df['type'] == neg_type]\n",
    "            \n",
    "            type_orig_correct = sum(type_df['original_pred'] == type_df['original_label'])\n",
    "            type_mod_correct = sum(type_df['modified_pred'] == type_df['modified_label'])\n",
    "            type_total = len(type_df)\n",
    "            \n",
    "            type_orig_acc = type_orig_correct / type_total * 100 if type_total > 0 else 0\n",
    "            type_mod_acc = type_mod_correct / type_total * 100 if type_total > 0 else 0\n",
    "            type_pct_diff = ((type_mod_acc - type_orig_acc) / type_orig_acc) * 100 if type_orig_acc > 0 else 0\n",
    "            type_weighted_delta = (type_mod_acc - type_orig_acc) * np.log10(type_orig_acc) / np.log10(100) if type_orig_acc > 0 else 0\n",
    "            \n",
    "            # Statistical tests for this negation type\n",
    "            type_orig_binary = (type_df['original_pred'] == type_df['original_label']).astype(int)\n",
    "            type_mod_binary = (type_df['modified_pred'] == type_df['modified_label']).astype(int)\n",
    "            \n",
    "            try:\n",
    "                _, type_p_value_mw = stats.mannwhitneyu(type_orig_binary, type_mod_binary)\n",
    "                _, type_p_value_wilc = stats.wilcoxon(type_orig_binary, type_mod_binary)\n",
    "                type_p_value = min(type_p_value_mw, type_p_value_wilc)\n",
    "            except ValueError:\n",
    "                type_p_value = 1.0\n",
    "                \n",
    "            # Add significance level\n",
    "            if type_p_value < 0.01:\n",
    "                significance = '**'\n",
    "            elif type_p_value < 0.05:\n",
    "                significance = '*'\n",
    "            elif type_p_value < 0.1:\n",
    "                significance = '.'\n",
    "            else:\n",
    "                significance = 'ns'\n",
    "                \n",
    "            type_row = {\n",
    "                'model': model,\n",
    "                'modification': neg_type,\n",
    "                'original_acc': decimal.Decimal(type_orig_acc).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "                'modified_acc': decimal.Decimal(type_mod_acc).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "                'pct_diff': decimal.Decimal(type_pct_diff).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "                'weighted_delta': decimal.Decimal(type_weighted_delta).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "                'wilcoxon_pvalue': decimal.Decimal(type_p_value_wilc).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "                'mannwhitney_pvalue': decimal.Decimal(type_p_value_mw).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "                'pvalue': decimal.Decimal(type_p_value).quantize(decimal.Decimal('0.001'), rounding=decimal.ROUND_HALF_UP),\n",
    "                'significance': significance,\n",
    "                'significant': type_p_value < 0.05\n",
    "            }\n",
    "            eval_negation_results_rows.append(type_row)\n",
    "\n",
    "# Convert to dataframes\n",
    "eval_results_df = pd.DataFrame(eval_results_rows)\n",
    "eval_negation_results_df = pd.DataFrame(eval_negation_results_rows)\n",
    "\n",
    "# Save results\n",
    "eval_results_df.to_csv(base_path / 'coreference_llm_results.csv', index=False)\n",
    "eval_negation_results_df.to_csv(base_path / 'coreference_llm_negation_results.csv', index=False)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nEvaluation Results by modification and model:\")\n",
    "# print(eval_results_df)\n",
    "print(\"\\nNegation Results by type and model:\")\n",
    "# print(eval_negation_results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combined Results:\n",
      "              model         modification  original_acc  modified_acc  \\\n",
      "78  bert-base-cased    active_to_passive        49.474        47.368   \n",
      "63  bert-base-cased       capitalization        53.535        48.485   \n",
      "54  bert-base-cased               casual        55.000        43.000   \n",
      "72  bert-base-cased        compound_word        60.417        60.417   \n",
      "84  bert-base-cased  concept_replacement        44.000        42.000   \n",
      "..              ...                  ...           ...           ...   \n",
      "83          t5-base          punctuation        58.586        59.596   \n",
      "98          t5-base            sentiment        64.000        64.000   \n",
      "62          t5-base             singlish        42.157        44.118   \n",
      "68          t5-base        temporal_bias        60.000        58.000   \n",
      "92          t5-base            typo_bias        63.265        64.286   \n",
      "\n",
      "    percentage_diff  weighted_delta  p_value significance significant  \n",
      "78           -4.255          -1.784    0.732           ns          No  \n",
      "63           -9.434          -4.365    0.197           ns          No  \n",
      "54          -21.818         -10.442    0.003           **         Yes  \n",
      "72            0.000           0.000    1.000           ns          No  \n",
      "84           -4.545          -1.643    0.480           ns          No  \n",
      "..              ...             ...      ...          ...         ...  \n",
      "83            1.724           0.893    0.317           ns          No  \n",
      "98            0.000           0.000    1.000           ns          No  \n",
      "62            4.651           1.593    0.317           ns          No  \n",
      "68           -3.333          -1.778    0.414           ns          No  \n",
      "92            1.613           0.919    0.705           ns          No  \n",
      "\n",
      "[108 rows x 9 columns]\n",
      "\n",
      "Combined Negation Results:\n",
      "                model modification  original_acc  modified_acc  pct_diff  \\\n",
      "15    bert-base-cased     absolute        75.000        62.500   -16.667   \n",
      "18    bert-base-cased  approximate        48.387        41.935   -13.333   \n",
      "16    bert-base-cased       double        60.000        60.000     0.000   \n",
      "17    bert-base-cased      lexical        50.000        45.833    -8.333   \n",
      "19    bert-base-cased       verbal        55.000        60.000     9.091   \n",
      "9   claude-3-5-sonnet     absolute        87.500        87.500     0.000   \n",
      "8   claude-3-5-sonnet  approximate        67.742        77.419    14.286   \n",
      "7   claude-3-5-sonnet       double        66.667        66.667     0.000   \n",
      "6   claude-3-5-sonnet      lexical        91.667        83.333    -9.091   \n",
      "5   claude-3-5-sonnet       verbal        85.000        60.000   -29.412   \n",
      "20               gpt2     absolute        62.500        87.500    40.000   \n",
      "23               gpt2  approximate        51.613        45.161   -12.500   \n",
      "21               gpt2       double        46.667        53.333    14.286   \n",
      "22               gpt2      lexical        54.167        45.833   -15.385   \n",
      "24               gpt2       verbal        45.000        45.000     0.000   \n",
      "14              gpt4o     absolute        87.500       100.000    14.286   \n",
      "13              gpt4o  approximate        77.419        64.516   -16.667   \n",
      "12              gpt4o       double        73.333        66.667    -9.091   \n",
      "11              gpt4o      lexical        83.333        79.167    -5.000   \n",
      "10              gpt4o       verbal        90.000        60.000   -33.333   \n",
      "4               llama     absolute        62.500        87.500    40.000   \n",
      "3               llama  approximate        80.645        77.419    -4.000   \n",
      "2               llama       double        66.667        66.667     0.000   \n",
      "1               llama      lexical        87.500        75.000   -14.286   \n",
      "0               llama       verbal        85.000        55.000   -35.294   \n",
      "25            t5-base     absolute        62.500        87.500    40.000   \n",
      "28            t5-base  approximate        51.613        48.387    -6.250   \n",
      "26            t5-base       double        46.667        60.000    28.571   \n",
      "27            t5-base      lexical        62.500        54.167   -13.333   \n",
      "29            t5-base       verbal        75.000        60.000   -20.000   \n",
      "\n",
      "    weighted_delta  wilcoxon_pvalue  mannwhitney_pvalue  pvalue significance  \\\n",
      "15         -11.719            1.000               0.648   0.648           ns   \n",
      "18          -5.435            0.527               0.618   0.527           ns   \n",
      "16           0.000            1.000               1.000   1.000           ns   \n",
      "17          -3.540            0.655               0.784   0.655           ns   \n",
      "19           4.351            0.655               0.764   0.655           ns   \n",
      "9            0.000            1.000               1.000   1.000           ns   \n",
      "8            8.859            0.317               0.402   0.317           ns   \n",
      "7            0.000            1.000               1.000   1.000           ns   \n",
      "6           -8.176            0.317               0.398   0.317           ns   \n",
      "5          -24.118            0.059               0.083   0.059            .   \n",
      "20          22.449            0.625               0.295   0.295           ns   \n",
      "23          -5.525            0.480               0.620   0.480           ns   \n",
      "21           5.563            0.564               0.738   0.564           ns   \n",
      "22          -7.224            0.317               0.576   0.317           ns   \n",
      "24           0.000            1.000               1.000   1.000           ns   \n",
      "14          12.138            1.000               0.382   0.382           ns   \n",
      "13         -12.186            0.157               0.271   0.157           ns   \n",
      "12          -6.218            0.564               0.715   0.564           ns   \n",
      "11          -4.002            0.564               0.726   0.564           ns   \n",
      "10         -29.314            0.014               0.032   0.014            *   \n",
      "4           22.449            0.500               0.295   0.295           ns   \n",
      "3           -3.075            0.705               0.765   0.705           ns   \n",
      "2            0.000            1.000               1.000   1.000           ns   \n",
      "1          -12.138            0.083               0.279   0.083            .   \n",
      "0          -28.941            0.014               0.043   0.014            *   \n",
      "25          22.449            0.500               0.295   0.295           ns   \n",
      "28          -2.763            0.705               0.807   0.705           ns   \n",
      "26          11.127            0.317               0.487   0.317           ns   \n",
      "27          -7.483            0.414               0.570   0.414           ns   \n",
      "29         -14.063            0.180               0.325   0.180           ns   \n",
      "\n",
      "    significant  \n",
      "15        False  \n",
      "18        False  \n",
      "16        False  \n",
      "17        False  \n",
      "19        False  \n",
      "9         False  \n",
      "8         False  \n",
      "7         False  \n",
      "6         False  \n",
      "5         False  \n",
      "20        False  \n",
      "23        False  \n",
      "21        False  \n",
      "22        False  \n",
      "24        False  \n",
      "14        False  \n",
      "13        False  \n",
      "12        False  \n",
      "11        False  \n",
      "10         True  \n",
      "4         False  \n",
      "3         False  \n",
      "2         False  \n",
      "1         False  \n",
      "0          True  \n",
      "25        False  \n",
      "28        False  \n",
      "26        False  \n",
      "27        False  \n",
      "29        False  \n"
     ]
    }
   ],
   "source": [
    "# Load LLM and PLM results\n",
    "llm_results = pd.read_csv(base_path / 'coreference_llm_results.csv')\n",
    "plm_results = pd.read_csv(base_path / 'coreference_plm_results.csv')\n",
    "\n",
    "# Load LLM and PLM negation results\n",
    "llm_neg_results = pd.read_csv(base_path / 'coreference_llm_negation_results.csv')\n",
    "plm_neg_results = pd.read_csv(base_path / 'coreference_plm_negation_results.csv')\n",
    "\n",
    "# Combine the regular results\n",
    "combined_results = pd.concat([llm_results, plm_results], ignore_index=True)\n",
    "\n",
    "# Combine the negation results\n",
    "combined_neg_results = pd.concat([llm_neg_results, plm_neg_results], ignore_index=True)\n",
    "\n",
    "# Sort by model and modification\n",
    "combined_results = combined_results.sort_values(['model', 'modification'])\n",
    "combined_neg_results = combined_neg_results.sort_values(['model', 'modification'])\n",
    "\n",
    "# Save combined results\n",
    "combined_results.to_csv(base_path / 'coreference_combined_results.csv', index=False)\n",
    "combined_neg_results.to_csv(base_path / 'coreference_combined_negation_results.csv', index=False)\n",
    "\n",
    "print(\"\\nCombined Results:\")\n",
    "print(combined_results)\n",
    "print(\"\\nCombined Negation Results:\")\n",
    "print(combined_neg_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LaTeX table saved to .tex\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "base_path = Path('../pretrained_language_models/coreference_resolution/tmp')\n",
    "modification_order = [\n",
    "    \"Bias\", \"Temporal\", \"Geographical\", \"Length\",\n",
    "    \"Orthographic\", \"Spelling\", \"Capitalization\", \"Punctuation\",\n",
    "    \"Morphological\", \"Derivation\", \"Compound\",\n",
    "    \"Syntactic\", \"Voice\", \"Grammar\", \"Conjunction\",\n",
    "    \"Semantic\", \"Concept\", \"Negation\",\n",
    "    \"Pragmatic\", \"Discourse\", \"Sentiment\",\n",
    "    \"Genre\", \"Casual\", \"Dialectal\", \"Singlish\"\n",
    "]\n",
    "\n",
    "# Read the combined results\n",
    "df = pd.read_csv(base_path / 'coreference_combined_results.csv')\n",
    "\n",
    "# Create mapping from modification names to standardized names\n",
    "mod_mapping = {\n",
    "    'temporal_bias': 'Temporal',\n",
    "    'geographical_bias': 'Geographical', \n",
    "    'length_bias': 'Length',\n",
    "    'typo_bias': 'Spelling',\n",
    "    'capitalization': 'Capitalization',\n",
    "    'punctuation': 'Punctuation',\n",
    "    'derivation': 'Derivation',\n",
    "    'compound_word': 'Compound',\n",
    "    'active_to_passive': 'Voice',\n",
    "    'grammatical_role': 'Grammar',\n",
    "    'coordinating_conjunction': 'Conjunction',\n",
    "    'concept_replacement': 'Concept',\n",
    "    'negation': 'Negation',\n",
    "    'discourse': 'Discourse',\n",
    "    'sentiment': 'Sentiment',\n",
    "    'casual': 'Casual',\n",
    "    'dialectal': 'Dialectal',\n",
    "    'singlish': 'Singlish'\n",
    "}\n",
    "\n",
    "# Map the modification names\n",
    "df['modification'] = df['modification'].map(mod_mapping)\n",
    "\n",
    "# Define model order and display names\n",
    "model_order = ['bert-base-cased', 'gpt2', 't5-base', 'gpt4o', 'claude-3-5-sonnet', 'llama']\n",
    "model_display = ['BERT', 'GPT-2', 'T5', 'GPT-4o', 'Claude 3.5', 'Llama 3.1']\n",
    "\n",
    "# Get original accuracy for each model\n",
    "original_acc = df.groupby('model')['original_acc'].first() * 100\n",
    "\n",
    "# Pivot the data to get modifications as rows and models as columns\n",
    "pivot_df = df.pivot(index='modification', columns='model', values='weighted_delta')\n",
    "p_values = df.pivot(index='modification', columns='model', values='p_value')\n",
    "\n",
    "# Reorder rows and columns\n",
    "pivot_df = pivot_df.reindex(modification_order, axis=0)\n",
    "pivot_df = pivot_df.reindex(model_order, axis=1)\n",
    "p_values = p_values.reindex(modification_order, axis=0)\n",
    "p_values = p_values.reindex(model_order, axis=1)\n",
    "\n",
    "# Function to generate color based on value\n",
    "def get_color(val, p_val):\n",
    "    if np.isnan(val):\n",
    "        return ''\n",
    "    elif val > 0:\n",
    "        # Green gradient for positive values\n",
    "        intensity = min(abs(val)/10, 1)  # Scale to max 10% change\n",
    "        val_str = f'+{val:.2f}'\n",
    "        if p_val < 0.01:\n",
    "            val_str = f'\\\\textbf{{{val_str}}}**'\n",
    "        elif p_val < 0.05:\n",
    "            val_str = f'\\\\textbf{{{val_str}}}*'\n",
    "        elif p_val < 0.1:\n",
    "            val_str = f'\\\\textbf{{{val_str}}}'\n",
    "        return f'\\\\cellcolor{{green!{int(intensity*30)}}} {val_str}'\n",
    "    else:\n",
    "        # Red gradient for negative values\n",
    "        intensity = min(abs(val)/10, 1)  # Scale to max 10% change\n",
    "        val_str = f'{val:.2f}'\n",
    "        if p_val < 0.01:\n",
    "            val_str = f'\\\\textbf{{{val_str}}}**'\n",
    "        elif p_val < 0.05:\n",
    "            val_str = f'\\\\textbf{{{val_str}}}*'\n",
    "        elif p_val < 0.1:\n",
    "            val_str = f'\\\\textbf{{{val_str}}}'\n",
    "        return f'\\\\cellcolor{{red!{int(intensity*30)}}} {val_str}'\n",
    "\n",
    "# Generate LaTeX table\n",
    "latex_table = '\\\\begin{table}[h]\\n\\\\centering\\n\\\\footnotesize\\n\\\\begin{adjustbox}{max width=\\\\linewidth}\\n'\n",
    "latex_table += '\\\\begin{tabular}{ll' + 'c'*len(pivot_df.columns) + '}\\n'\n",
    "latex_table += '\\\\hline\\n'\n",
    "latex_table += 'Category & Modification & ' + ' & '.join([f'\\\\textbf{{{name}}}' for name in model_display]) + ' \\\\\\\\\\n'\n",
    "latex_table += '\\\\hline\\n'\n",
    "\n",
    "# Add original accuracy row\n",
    "latex_table += '\\\\rowcolor{gray!20}\\\\textit{Original} & & ' + ' & '.join([f'{acc:.2f}' for acc in original_acc]) + ' \\\\\\\\\\n'\n",
    "\n",
    "# Track displayed categories\n",
    "displayed_categories = set()\n",
    "\n",
    "# Display all modifications including categories\n",
    "for idx in modification_order:\n",
    "    if idx in ['Bias', 'Orthographic', 'Morphological', 'Syntactic', 'Semantic', 'Pragmatic', 'Genre']:\n",
    "        if idx not in displayed_categories:\n",
    "            latex_table += f'\\\\textbf{{{idx}}} & & & & & & & \\\\\\\\\\n'\n",
    "            displayed_categories.add(idx)\n",
    "    else:\n",
    "        latex_table += f'& \\\\textbf{{{idx}}} & '\n",
    "        if idx in pivot_df.index:\n",
    "            latex_table += ' & '.join([get_color(val, p_values.loc[idx, col]) for col, val in pivot_df.loc[idx].items()]) + ' \\\\\\\\\\n'\n",
    "        else:\n",
    "            latex_table += ' & '.join([''] * len(model_order)) + ' \\\\\\\\\\n'\n",
    "\n",
    "latex_table += '\\\\end{tabular}\\n'\n",
    "latex_table += '\\\\end{adjustbox}\\n'\n",
    "latex_table += '\\\\caption{Percentage Change in Micro F1 Score by Model and Modification Type}\\n'\n",
    "latex_table += '\\\\label{tab:dialogue_results_table}\\n'\n",
    "latex_table += '\\\\end{table}'\n",
    "\n",
    "# Save to file\n",
    "with open(base_path / 'coreference_results_table.tex', 'w') as f:\n",
    "    f.write(latex_table)\n",
    "\n",
    "print(\"LaTeX table saved to .tex\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to coreference_results_df.csv and coreference_negation_results_df.csv\n"
     ]
    }
   ],
   "source": [
    "# Load results from CSV\n",
    "base_path = Path('../pretrained_language_models/coreference_resolution/tmp')\n",
    "\n",
    "results_df = pd.read_csv(base_path / 'coreference_combined_results.csv')\n",
    "\n",
    "# Create lists of unique modifications and models\n",
    "modification_order = list(mod_mapping.keys())\n",
    "model_order = ['bert-base-cased', 'gpt2', 't5-base', 'gpt4o', 'claude-3-5-sonnet', 'llama']\n",
    "negation_order = ['verbal', 'lexical', 'double', 'approximate', 'absolute']\n",
    "# Create empty DataFrame with multi-level columns\n",
    "columns = pd.MultiIndex.from_product([model_order, ['original', 'modified', 'diff']])\n",
    "results_df_pivot = pd.DataFrame(index=modification_order, columns=columns)\n",
    "\n",
    "# Fill DataFrame\n",
    "for mod in modification_order:\n",
    "    for model in model_order:\n",
    "        row = results_df[(results_df['modification'] == mod) & (results_df['model'] == model)]\n",
    "        if not row.empty:\n",
    "            results_df_pivot.loc[mod, (model, 'original')] = row['original_acc'].values[0]\n",
    "            results_df_pivot.loc[mod, (model, 'modified')] = row['modified_acc'].values[0]\n",
    "            results_df_pivot.loc[mod, (model, 'diff')] = row['percentage_diff'].values[0]\n",
    "            if mod == 'temporal_bias' and model == 'bert':\n",
    "                print(row)\n",
    "\n",
    "# Save to CSV\n",
    "results_df_pivot.to_csv(base_path / 'coreference_results_df.csv')\n",
    "\n",
    "# Save negation results to separate CSV\n",
    "# Load negation results from CSV\n",
    "negation_results_df = pd.read_csv(base_path / 'coreference_combined_negation_results.csv')\n",
    "\n",
    "# Create empty DataFrame with multi-level columns for negation results\n",
    "negation_df_pivot = pd.DataFrame(index=['negation'], columns=columns)\n",
    "\n",
    "# Fill DataFrame with negation results\n",
    "for mod in negation_order:\n",
    "    for model in model_order:\n",
    "        row = negation_results_df[(negation_results_df['modification'] == mod) & (negation_results_df['model'] == model)]\n",
    "        if not row.empty:   \n",
    "            negation_df_pivot.loc[mod, (model, 'original')] = row['original_acc'].values[0]\n",
    "            negation_df_pivot.loc[mod, (model, 'modified')] = row['modified_acc'].values[0] \n",
    "            negation_df_pivot.loc[mod, (model, 'diff')] = row['pct_diff'].values[0]\n",
    "\n",
    "# Save negation results to CSV\n",
    "negation_df_pivot.to_csv(base_path / 'coreference_negation_results_df.csv')\n",
    "\n",
    "\n",
    "print(\"Results saved to coreference_results_df.csv and coreference_negation_results_df.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model         bert-base-cased  claude-3-5-sonnet    gpt2   gpt4o   llama  \\\n",
      "modification                                                               \n",
      "Verbal                -21.342             -5.413 -15.757   5.487 -27.778   \n",
      "Lexical               -27.731            -27.309 -30.582 -30.097 -30.344   \n",
      "Double                  8.199            -36.364 -26.708 -17.390 -44.514   \n",
      "Approximate           -26.923            -23.077 -22.676 -15.117 -19.067   \n",
      "Absolute              -22.240            -30.769 -29.653 -30.234 -38.462   \n",
      "\n",
      "model         t5-base  \n",
      "modification           \n",
      "Verbal         -5.336  \n",
      "Lexical       -27.731  \n",
      "Double        -17.390  \n",
      "Approximate   -18.897  \n",
      "Absolute      -37.793  \n",
      "LaTeX table saved to ner_results_table.tex\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Read the combined results\n",
    "base_path = Path('dialogue_contradiction_detection/tmp')\n",
    "df = pd.read_csv(base_path / 'dialogue_combined_negation_results.csv')\n",
    "\n",
    "# Create mapping from modification names to standardized names\n",
    "negation_order= ['Verbal', 'Lexical', 'Double', 'Approximate', 'Absolute']\n",
    "mod_mapping = {\n",
    "    'verbal': 'Verbal',\n",
    "    'lexical': 'Lexical',\n",
    "    'double': 'Double',\n",
    "    'approximate': 'Approximate',\n",
    "    'absolute': 'Absolute',\n",
    "}\n",
    "\n",
    "# Define model order\n",
    "model_order = ['bert-base-cased', 'gpt2', 't5-base', 'gpt4o', 'claude-3-5-sonnet', 'llama']\n",
    "\n",
    "# Map the modification names\n",
    "df['modification'] = df['modification'].map(mod_mapping)\n",
    "\n",
    "# Pivot the data to get modifications as rows and models as columns\n",
    "pivot_df = df.pivot(index='modification', columns='model', values='weighted_delta')\n",
    "p_values = df.pivot(index='modification', columns='model', values='pvalue')\n",
    "significance = df.pivot(index='modification', columns='model', values='significance')\n",
    "\n",
    "# Reorder rows according to modification_order\n",
    "pivot_df = pivot_df.reindex(negation_order)\n",
    "p_values = p_values.reindex(negation_order)\n",
    "significance = significance.reindex(negation_order)\n",
    "\n",
    "print(pivot_df)\n",
    "\n",
    "# Function to generate color based on value\n",
    "def get_color(val, sig):\n",
    "    if np.isnan(val):\n",
    "        return ''\n",
    "    elif val > 0:\n",
    "        # Green gradient for positive values\n",
    "        intensity = min(abs(val)/10, 1)  # Scale to max 10% change\n",
    "        val_str = f'+{val:.1f}'\n",
    "        if sig == '.':  # Just bold for '.'\n",
    "            val_str = f'\\\\textbf{{{val_str}}}'\n",
    "        elif sig == '*':  # One asterisk\n",
    "            val_str = f'\\\\textbf{{{val_str}}}*'\n",
    "        elif sig == '**':  # Two asterisks\n",
    "            val_str = f'\\\\textbf{{{val_str}}}**'\n",
    "        return f'\\\\cellcolor{{green!{int(intensity*30)}}} {val_str}'\n",
    "    else:\n",
    "        # Red gradient for negative values\n",
    "        intensity = min(abs(val)/10, 1)  # Scale to max 10% change\n",
    "        val_str = f'{val:.1f}'\n",
    "        if sig == '.':  # Just bold for '.'\n",
    "            val_str = f'\\\\textbf{{{val_str}}}'\n",
    "        elif sig == '*':  # One asterisk\n",
    "            val_str = f'\\\\textbf{{{val_str}}}*'\n",
    "        elif sig == '**':  # Two asterisks\n",
    "            val_str = f'\\\\textbf{{{val_str}}}**'\n",
    "        return f'\\\\cellcolor{{red!{int(intensity*30)}}} {val_str}'\n",
    "\n",
    "# Generate LaTeX table\n",
    "latex_table = '\\\\begin{table}[h]\\n\\\\centering\\n\\\\begin{tabular}{l' + 'r'*len(model_order) + '}\\n'\n",
    "latex_table += '\\\\hline\\n'\n",
    "latex_table += 'Modification & ' + ' & '.join([f'\\\\textbf{{{col}}}' for col in model_order]) + ' \\\\\\\\\\n'\n",
    "latex_table += '\\\\hline\\n'\n",
    "\n",
    "prev_category = None\n",
    "for idx, row in pivot_df.iterrows():\n",
    "    current_category = idx[0]  # Get first character of modification name\n",
    "    if prev_category is not None and current_category != prev_category:\n",
    "        latex_table += '\\\\hline\\n'\n",
    "    prev_category = current_category\n",
    "    \n",
    "    latex_table += f'\\\\textbf{{{idx}}} & '\n",
    "    latex_table += ' & '.join([get_color(row[col], significance.loc[idx, col]) for col in model_order]) + ' \\\\\\\\\\n'\n",
    "\n",
    "latex_table += '\\\\hline\\n'\n",
    "latex_table += '\\\\end{tabular}\\n'\n",
    "latex_table += '\\\\caption{Percentage Change in Micro F1 Score by Model and Modification Type}\\n'\n",
    "latex_table += '\\\\label{tab:ner_results}\\n'\n",
    "latex_table += '\\\\end{table}'\n",
    "\n",
    "# Save to file\n",
    "with open('dialogue_negation_type_results_table.tex', 'w') as f:\n",
    "    f.write(latex_table)\n",
    "\n",
    "print(\"LaTeX table saved to ner_results_table.tex\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model         bert-base-cased  claude-3-5-sonnet    gpt2   gpt4o   llama  \\\n",
      "modification                                                               \n",
      "Verbal                  4.351            -24.118   0.000 -29.314 -28.941   \n",
      "Lexical                -3.540             -8.176  -7.224  -4.002 -12.138   \n",
      "Double                  0.000              0.000   5.563  -6.218   0.000   \n",
      "Approximate            -5.435              8.859  -5.525 -12.186  -3.075   \n",
      "Absolute              -11.719              0.000  22.449  12.138  22.449   \n",
      "\n",
      "model         t5-base  \n",
      "modification           \n",
      "Verbal        -14.063  \n",
      "Lexical        -7.483  \n",
      "Double         11.127  \n",
      "Approximate    -2.763  \n",
      "Absolute       22.449  \n",
      "LaTeX table saved to coreference_negation_type_results_table.tex\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Read the combined results\n",
    "base_path = Path('coreference_resolution/tmp')\n",
    "df = pd.read_csv(base_path / 'coreference_combined_negation_results.csv')\n",
    "\n",
    "# Create mapping from modification names to standardized names\n",
    "negation_order= ['Verbal', 'Lexical', 'Double', 'Approximate', 'Absolute']\n",
    "mod_mapping = {\n",
    "    'verbal': 'Verbal',\n",
    "    'lexical': 'Lexical',\n",
    "    'double': 'Double',\n",
    "    'approximate': 'Approximate',\n",
    "    'absolute': 'Absolute',\n",
    "}\n",
    "\n",
    "# Define model order\n",
    "model_order = ['bert-base-cased', 'gpt2', 't5-base', 'gpt4o', 'claude-3-5-sonnet', 'llama']\n",
    "\n",
    "# Map the modification names\n",
    "df['modification'] = df['modification'].map(mod_mapping)\n",
    "\n",
    "# Pivot the data to get modifications as rows and models as columns\n",
    "pivot_df = df.pivot(index='modification', columns='model', values='weighted_delta')\n",
    "p_values = df.pivot(index='modification', columns='model', values='pvalue')\n",
    "significance = df.pivot(index='modification', columns='model', values='significance')\n",
    "\n",
    "# Reorder rows according to modification_order\n",
    "pivot_df = pivot_df.reindex(negation_order)\n",
    "p_values = p_values.reindex(negation_order)\n",
    "significance = significance.reindex(negation_order)\n",
    "\n",
    "print(pivot_df)\n",
    "\n",
    "# Function to generate color based on value\n",
    "def get_color(val, sig):\n",
    "    if np.isnan(val):\n",
    "        return ''\n",
    "    elif val > 0:\n",
    "        # Green gradient for positive values\n",
    "        intensity = min(abs(val)/10, 1)  # Scale to max 10% change\n",
    "        val_str = f'+{val:.1f}'\n",
    "        if sig == '.':  # Just bold for '.'\n",
    "            val_str = f'\\\\textbf{{{val_str}}}'\n",
    "        elif sig == '*':  # One asterisk\n",
    "            val_str = f'\\\\textbf{{{val_str}}}*'\n",
    "        elif sig == '**':  # Two asterisks\n",
    "            val_str = f'\\\\textbf{{{val_str}}}**'\n",
    "        return f'\\\\cellcolor{{green!{int(intensity*30)}}} {val_str}'\n",
    "    else:\n",
    "        # Red gradient for negative values\n",
    "        intensity = min(abs(val)/10, 1)  # Scale to max 10% change\n",
    "        val_str = f'{val:.1f}'\n",
    "        if sig == '.':  # Just bold for '.'\n",
    "            val_str = f'\\\\textbf{{{val_str}}}'\n",
    "        elif sig == '*':  # One asterisk\n",
    "            val_str = f'\\\\textbf{{{val_str}}}*'\n",
    "        elif sig == '**':  # Two asterisks\n",
    "            val_str = f'\\\\textbf{{{val_str}}}**'\n",
    "        return f'\\\\cellcolor{{red!{int(intensity*30)}}} {val_str}'\n",
    "\n",
    "# Generate LaTeX table\n",
    "latex_table = '\\\\begin{table}[h]\\n\\\\centering\\n\\\\begin{tabular}{l' + 'r'*len(model_order) + '}\\n'\n",
    "latex_table += '\\\\hline\\n'\n",
    "latex_table += 'Modification & ' + ' & '.join([f'\\\\textbf{{{col}}}' for col in model_order]) + ' \\\\\\\\\\n'\n",
    "latex_table += '\\\\hline\\n'\n",
    "\n",
    "prev_category = None\n",
    "for idx, row in pivot_df.iterrows():\n",
    "    current_category = idx[0]  # Get first character of modification name\n",
    "    if prev_category is not None and current_category != prev_category:\n",
    "        latex_table += '\\\\hline\\n'\n",
    "    prev_category = current_category\n",
    "    \n",
    "    latex_table += f'\\\\textbf{{{idx}}} & '\n",
    "    latex_table += ' & '.join([get_color(row[col], significance.loc[idx, col]) for col in model_order]) + ' \\\\\\\\\\n'\n",
    "\n",
    "latex_table += '\\\\hline\\n'\n",
    "latex_table += '\\\\end{tabular}\\n'\n",
    "latex_table += '\\\\caption{Percentage Change in Micro F1 Score by Model and Modification Type}\\n'\n",
    "latex_table += '\\\\label{tab:ner_results}\\n'\n",
    "latex_table += '\\\\end{table}'\n",
    "\n",
    "# Save to file\n",
    "with open(base_path / 'coreference_negation_type_results_table.tex', 'w') as f:\n",
    "    f.write(latex_table)\n",
    "\n",
    "print(\"LaTeX table saved to coreference_negation_type_results_table.tex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
