{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original predictions saved to: ../../eval/PLM_results/sentiment_analysis/BERT/singlish_ori.txt\n",
      "Modified predictions saved to: ../../eval/PLM_results/sentiment_analysis/BERT/singlish_modif.txt\n",
      "Original predictions saved to: ../../eval/PLM_results/sentiment_analysis/GPT2/singlish_ori.txt\n",
      "Modified predictions saved to: ../../eval/PLM_results/sentiment_analysis/GPT2/singlish_modif.txt\n",
      "Original predictions saved to: ../../eval/PLM_results/sentiment_analysis/T5/singlish_ori.txt\n",
      "Modified predictions saved to: ../../eval/PLM_results/sentiment_analysis/T5/singlish_modif.txt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "for model in ['bert', 'gpt2', 't5']:\n",
    "\n",
    "# Define the input and output file paths\n",
    "    input_csv_path = f'sentiment_{model}/sentiment_singlish_all_predictions.csv'\n",
    "    output_dir = f'../../eval/PLM_results/sentiment_analysis/{model.upper()}/singlish'\n",
    "    ori_output_path = output_dir +  '_ori.txt'\n",
    "    modif_output_path = output_dir + '_modif.txt'\n",
    "\n",
    "    # Create the output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(input_csv_path)\n",
    "\n",
    "    # Extract and save original predictions\n",
    "    original_preds = df['original_pred'].tolist()\n",
    "    with open(ori_output_path, 'w') as f:\n",
    "        for pred in original_preds:\n",
    "            f.write(str(pred) + '\\n')\n",
    "\n",
    "    # Extract and save modified predictions\n",
    "    modified_preds = df['modified_pred'].tolist()\n",
    "    with open(modif_output_path, 'w') as f:\n",
    "        for pred in modified_preds:\n",
    "            f.write(str(pred) + '\\n')\n",
    "\n",
    "    print(f\"Original predictions saved to: {ori_output_path}\")\n",
    "    print(f\"Modified predictions saved to: {modif_output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy import stats\n",
    "\n",
    "# Read all modification files\n",
    "modifications = [f.split('_modif.txt')[0] for f in os.listdir('../../eval/PLM_results/sentiment_analysis/BERT') if f.endswith('_modif.txt')]\n",
    "\n",
    "bert_predictions = {}\n",
    "gpt2_predictions = {}\n",
    "t5_predictions = {}\n",
    "model_predictions = {}\n",
    "\n",
    "# Load BERT predictions\n",
    "for mod in modifications:\n",
    "    bert_predictions[f'{mod}_modif'] = pd.read_csv(f'../../eval/PLM_results/sentiment_analysis/BERT/{mod}_modif.txt', header=None)[0].tolist()\n",
    "    bert_predictions[f'{mod}_ori'] = pd.read_csv(f'../../eval/PLM_results/sentiment_analysis/BERT/{mod}_ori.txt', header=None)[0].tolist()\n",
    "    if len(bert_predictions[f'{mod}_modif']) != len(bert_predictions[f'{mod}_ori']):\n",
    "        print(mod)\n",
    "# Load GPT2 predictions  \n",
    "for mod in modifications:\n",
    "    gpt2_predictions[f'{mod}_modif'] = pd.read_csv(f'../../eval/PLM_results/sentiment_analysis/GPT2/{mod}_modif.txt', header=None)[0].tolist()\n",
    "    gpt2_predictions[f'{mod}_ori'] = pd.read_csv(f'../../eval/PLM_results/sentiment_analysis/GPT2/{mod}_ori.txt', header=None)[0].tolist()\n",
    "    if len(gpt2_predictions[f'{mod}_modif']) != len(gpt2_predictions[f'{mod}_ori']):\n",
    "        print(mod)\n",
    "# Load T5 predictions\n",
    "for mod in modifications:\n",
    "    t5_predictions[f'{mod}_modif'] = pd.read_csv(f'../../eval/PLM_results/sentiment_analysis/T5/{mod}_modif.txt', header=None)[0].tolist()\n",
    "    t5_predictions[f'{mod}_ori'] = pd.read_csv(f'../../eval/PLM_results/sentiment_analysis/T5/{mod}_ori.txt', header=None)[0].tolist()\n",
    "\n",
    "# Read the original labels and model predictions\n",
    "labels = {}\n",
    "model_names = ['gpt4o', 'claude-3-5-sonnet', 'llama']\n",
    "for mod in modifications:\n",
    "    labels[mod] = pd.read_csv(f'../../eval/results/yulia/gpt4o-0shot-{mod}_100.csv')\n",
    "    for model in model_names:\n",
    "        key = f'{model}_{mod}'\n",
    "        model_predictions[key] = pd.read_csv(f'../../eval/results/yulia/{model}-0shot-{mod}_100.csv')\n",
    "        if len(model_predictions[key]) != len(labels[mod]): \n",
    "            print(mod, model)\n",
    "\n",
    "# Create comparison files for BERT, GPT2, T5\n",
    "for mod in modifications:\n",
    "    # BERT comparison\n",
    "    # Create directories if they don't exist\n",
    "    os.makedirs('tmp/bert', exist_ok=True)\n",
    "    os.makedirs('tmp/gpt2', exist_ok=True) \n",
    "    os.makedirs('tmp/t5', exist_ok=True)\n",
    "    # print(mod)\n",
    "    bert_comp = pd.DataFrame({\n",
    "        'original_text': labels[mod]['original_text'],\n",
    "        'modified_text': labels[mod]['text'] if 'text' in labels[mod].columns else labels[mod]['original_text'],\n",
    "        'original_label': labels[mod]['original_label'],\n",
    "        'original_pred': bert_predictions[f'{mod}_ori'],\n",
    "        'modified_label': labels[mod]['modified_label'],\n",
    "        'modified_pred': bert_predictions[f'{mod}_modif'],\n",
    "    })\n",
    "    bert_comp.to_csv(f'tmp/bert/{mod}_comparison.csv', index=False)\n",
    "    \n",
    "    # GPT2 comparison  \n",
    "    gpt2_comp = pd.DataFrame({\n",
    "        'original_text': labels[mod]['original_text'],\n",
    "        'modified_text': labels[mod]['text'] if 'text' in labels[mod].columns else labels[mod]['original_text'],\n",
    "        'original_label': labels[mod]['original_label'],\n",
    "        'original_pred': gpt2_predictions[f'{mod}_ori'],\n",
    "        'modified_label': labels[mod]['modified_label'],\n",
    "        'modified_pred': gpt2_predictions[f'{mod}_modif'],\n",
    "    })\n",
    "    gpt2_comp.to_csv(f'tmp/gpt2/{mod}_comparison.csv', index=False)\n",
    "    \n",
    "    # T5 comparison\n",
    "    t5_comp = pd.DataFrame({\n",
    "        'original_text': labels[mod]['original_text'],\n",
    "        'modified_text': labels[mod]['text'] if 'text' in labels[mod].columns else labels[mod]['original_text'],\n",
    "        'original_label': labels[mod]['original_label'],\n",
    "        'original_pred': t5_predictions[f'{mod}_ori'],\n",
    "        'modified_label': labels[mod]['modified_label'],\n",
    "        'modified_pred': t5_predictions[f'{mod}_modif'],\n",
    "    })\n",
    "    t5_comp.to_csv(f'tmp/t5/{mod}_comparison.csv', index=False)\n",
    "# Calculate results for all models\n",
    "results = []\n",
    "negation_results = []\n",
    "\n",
    "def get_significance_level(pvalue):\n",
    "    if pvalue < 0.01:\n",
    "        return \"**\"   # Very significant\n",
    "    elif pvalue < 0.05:\n",
    "        return \"*\"    # Significant\n",
    "    elif pvalue < 0.1:\n",
    "        return \".\"    # Weakly significant\n",
    "    else:\n",
    "        return \"ns\"   # Not significant\n",
    "\n",
    "# Process BERT\n",
    "for mod in modifications:\n",
    "    bert_orig = np.array([1 if p == l else 0 for p, l in zip(bert_predictions[f'{mod}_ori'], labels[mod]['original_label'])])\n",
    "    bert_mod = np.array([1 if p == l else 0 for p, l in zip(bert_predictions[f'{mod}_modif'], labels[mod]['modified_label'])])\n",
    "    bert_orig_acc = np.round(bert_orig.mean() * 100, decimals=3)\n",
    "    bert_mod_acc = np.round(bert_mod.mean() * 100, decimals=3)\n",
    "    bert_pct_diff = np.round(((bert_mod_acc - bert_orig_acc) / bert_orig_acc) * 100, decimals=1)\n",
    "    bert_weighted_delta = np.round((bert_mod_acc - bert_orig_acc) * np.log10(bert_orig_acc) / np.log10(100), decimals=3)\n",
    "    \n",
    "    if np.array_equal(bert_orig, bert_mod):\n",
    "        wilcoxon_pvalue = 1.0\n",
    "        mannwhitney_pvalue = 1.0\n",
    "    else:\n",
    "        _, wilcoxon_pvalue = stats.wilcoxon(bert_orig, bert_mod)\n",
    "        _, mannwhitney_pvalue = stats.mannwhitneyu(bert_orig, bert_mod, alternative='two-sided')\n",
    "    pvalue = min(wilcoxon_pvalue, mannwhitney_pvalue)\n",
    "    significance = get_significance_level(pvalue)\n",
    "\n",
    "    results.append({\n",
    "        'model': 'bert',\n",
    "        'modification': mod,\n",
    "        'original_acc': bert_orig_acc,\n",
    "        'modified_acc': bert_mod_acc,\n",
    "        'pct_diff': bert_pct_diff,\n",
    "        'weighted_delta': bert_weighted_delta,\n",
    "        'wilcoxon_pvalue': wilcoxon_pvalue,\n",
    "        'mannwhitney_pvalue': mannwhitney_pvalue,\n",
    "        'pvalue': pvalue,\n",
    "        'significance': significance,\n",
    "        'significant': wilcoxon_pvalue < 0.05 or mannwhitney_pvalue < 0.05\n",
    "    })\n",
    "\n",
    "    if mod == 'negation':\n",
    "        # Get negation types from gpt4o data\n",
    "        neg_types = model_predictions['gpt4o_negation']['type'].tolist()\n",
    "        for neg_type in set(neg_types):\n",
    "            type_indices = [i for i, t in enumerate(neg_types) if t == neg_type]\n",
    "            bert_orig_type = bert_orig[type_indices]\n",
    "            bert_mod_type = bert_mod[type_indices]\n",
    "            \n",
    "            bert_orig_acc_type = np.round(bert_orig_type.mean() * 100, decimals=3)\n",
    "            bert_mod_acc_type = np.round(bert_mod_type.mean() * 100, decimals=3)\n",
    "            bert_pct_diff_type = np.round(((bert_mod_acc_type - bert_orig_acc_type) / bert_orig_acc_type) * 100, decimals=1)\n",
    "            bert_weighted_delta_type = np.round((bert_mod_acc_type - bert_orig_acc_type) * np.log10(bert_orig_acc_type) / np.log10(100), decimals=3)\n",
    "            \n",
    "            if np.array_equal(bert_orig_type, bert_mod_type):\n",
    "                wilcoxon_pvalue = 1.0\n",
    "                mannwhitney_pvalue = 1.0\n",
    "            else:\n",
    "                _, wilcoxon_pvalue = stats.wilcoxon(bert_orig_type, bert_mod_type)\n",
    "                _, mannwhitney_pvalue = stats.mannwhitneyu(bert_orig_type, bert_mod_type, alternative='two-sided')\n",
    "            pvalue = min(wilcoxon_pvalue, mannwhitney_pvalue)\n",
    "            significance = get_significance_level(pvalue)\n",
    "            \n",
    "            negation_results.append({\n",
    "                'model': 'bert',\n",
    "                'modification': f'{neg_type}',\n",
    "                'original_acc': bert_orig_acc_type,\n",
    "                'modified_acc': bert_mod_acc_type,\n",
    "                'pct_diff': bert_pct_diff_type,\n",
    "                'weighted_delta': bert_weighted_delta_type,\n",
    "                'wilcoxon_pvalue': wilcoxon_pvalue,\n",
    "                'mannwhitney_pvalue': mannwhitney_pvalue,\n",
    "                'pvalue': pvalue,\n",
    "                'significance': significance,\n",
    "                'significant': pvalue < 0.05\n",
    "            })\n",
    "    \n",
    "\n",
    "# Process GPT2\n",
    "for mod in modifications:\n",
    "    gpt2_orig = np.array([1 if p == l else 0 for p, l in zip(gpt2_predictions[f'{mod}_ori'], labels[mod]['original_label'])])\n",
    "    gpt2_mod = np.array([1 if p == l else 0 for p, l in zip(gpt2_predictions[f'{mod}_modif'], labels[mod]['modified_label'])])\n",
    "    gpt2_orig_acc = np.round(gpt2_orig.mean() * 100, decimals=3)\n",
    "    gpt2_mod_acc = np.round(gpt2_mod.mean() * 100, decimals=3)\n",
    "    gpt2_pct_diff = np.round(((gpt2_mod_acc - gpt2_orig_acc) / gpt2_orig_acc) * 100, decimals=1)\n",
    "    gpt2_weighted_delta = np.round((gpt2_mod_acc - gpt2_orig_acc) * np.log10(gpt2_orig_acc) / np.log10(100), decimals=3)\n",
    "    \n",
    "    if np.array_equal(gpt2_orig, gpt2_mod):\n",
    "        wilcoxon_pvalue = 1.0\n",
    "        mannwhitney_pvalue = 1.0\n",
    "    else:\n",
    "        _, wilcoxon_pvalue = stats.wilcoxon(gpt2_orig, gpt2_mod)\n",
    "        _, mannwhitney_pvalue = stats.mannwhitneyu(gpt2_orig, gpt2_mod, alternative='two-sided')\n",
    "    pvalue = min(wilcoxon_pvalue, mannwhitney_pvalue)\n",
    "    significance = get_significance_level(pvalue)\n",
    "    results.append({\n",
    "        'model': 'gpt2',\n",
    "        'modification': mod,\n",
    "        'original_acc': gpt2_orig_acc,\n",
    "        'modified_acc': gpt2_mod_acc,\n",
    "        'pct_diff': gpt2_pct_diff,\n",
    "        'weighted_delta': gpt2_weighted_delta,\n",
    "        'wilcoxon_pvalue': wilcoxon_pvalue,\n",
    "        'mannwhitney_pvalue': mannwhitney_pvalue,\n",
    "        'pvalue': pvalue,\n",
    "        'significance': significance,\n",
    "        'significant': wilcoxon_pvalue < 0.05 or mannwhitney_pvalue < 0.05\n",
    "    })\n",
    "\n",
    "    if mod == 'negation':\n",
    "        # Get negation types from gpt4o data\n",
    "        neg_types = model_predictions['gpt4o_negation']['type'].tolist()\n",
    "        for neg_type in set(neg_types):\n",
    "            type_indices = [i for i, t in enumerate(neg_types) if t == neg_type]\n",
    "            gpt2_orig_type = gpt2_orig[type_indices]\n",
    "            gpt2_mod_type = gpt2_mod[type_indices]\n",
    "            \n",
    "            gpt2_orig_acc_type = np.round(gpt2_orig_type.mean() * 100, decimals=3)\n",
    "            gpt2_mod_acc_type = np.round(gpt2_mod_type.mean() * 100, decimals=3)\n",
    "            gpt2_pct_diff_type = np.round(((gpt2_mod_acc_type - gpt2_orig_acc_type) / gpt2_orig_acc_type) * 100, decimals=1)\n",
    "            gpt2_weighted_delta_type = np.round((gpt2_mod_acc_type - gpt2_orig_acc_type) * np.log10(gpt2_orig_acc_type) / np.log10(100), decimals=3)\n",
    "            \n",
    "            if np.array_equal(gpt2_orig_type, gpt2_mod_type):\n",
    "                wilcoxon_pvalue = 1.0\n",
    "                mannwhitney_pvalue = 1.0\n",
    "            else:\n",
    "                _, wilcoxon_pvalue = stats.wilcoxon(gpt2_orig_type, gpt2_mod_type)\n",
    "                _, mannwhitney_pvalue = stats.mannwhitneyu(gpt2_orig_type, gpt2_mod_type, alternative='two-sided')\n",
    "            pvalue = min(wilcoxon_pvalue, mannwhitney_pvalue)\n",
    "            significance = get_significance_level(pvalue)\n",
    "            \n",
    "            negation_results.append({\n",
    "                'model': 'gpt2',\n",
    "                'modification': f'{neg_type}',\n",
    "                'original_acc': gpt2_orig_acc_type,\n",
    "                'modified_acc': gpt2_mod_acc_type,\n",
    "                'pct_diff': gpt2_pct_diff_type,\n",
    "                'weighted_delta': gpt2_weighted_delta_type,\n",
    "                'wilcoxon_pvalue': wilcoxon_pvalue,\n",
    "                'mannwhitney_pvalue': mannwhitney_pvalue,\n",
    "                'pvalue': pvalue,\n",
    "                'significance': significance,\n",
    "                'significant': pvalue < 0.05\n",
    "            })\n",
    "    \n",
    "\n",
    "# Process T5\n",
    "for mod in modifications:\n",
    "    t5_orig = np.array([1 if p == l else 0 for p, l in zip(t5_predictions[f'{mod}_ori'], labels[mod]['original_label'])])\n",
    "    t5_mod = np.array([1 if p == l else 0 for p, l in zip(t5_predictions[f'{mod}_modif'], labels[mod]['modified_label'])])\n",
    "    t5_orig_acc = np.round(t5_orig.mean() * 100, decimals=3)\n",
    "    t5_mod_acc = np.round(t5_mod.mean() * 100, decimals=3)\n",
    "    t5_pct_diff = np.round(((t5_mod_acc - t5_orig_acc) / t5_orig_acc) * 100, decimals=1)\n",
    "    t5_weighted_delta = np.round((t5_mod_acc - t5_orig_acc) * np.log10(t5_orig_acc) / np.log10(100), decimals=3)\n",
    "    \n",
    "    if np.array_equal(t5_orig, t5_mod):\n",
    "        wilcoxon_pvalue = 1.0\n",
    "        mannwhitney_pvalue = 1.0\n",
    "    else:\n",
    "        _, wilcoxon_pvalue = stats.wilcoxon(t5_orig, t5_mod)\n",
    "        _, mannwhitney_pvalue = stats.mannwhitneyu(t5_orig, t5_mod, alternative='two-sided')\n",
    "    pvalue = min(wilcoxon_pvalue, mannwhitney_pvalue)\n",
    "    significance = get_significance_level(pvalue)\n",
    "    results.append({\n",
    "        'model': 't5',\n",
    "        'modification': mod,\n",
    "        'original_acc': t5_orig_acc,\n",
    "        'modified_acc': t5_mod_acc,\n",
    "        'pct_diff': t5_pct_diff,\n",
    "        'weighted_delta': t5_weighted_delta,\n",
    "        'wilcoxon_pvalue': wilcoxon_pvalue,\n",
    "        'mannwhitney_pvalue': mannwhitney_pvalue,\n",
    "        'pvalue': pvalue,\n",
    "        'significance': significance,\n",
    "        'significant': wilcoxon_pvalue < 0.05 or mannwhitney_pvalue < 0.05\n",
    "    })\n",
    "\n",
    "    if mod == 'negation':\n",
    "        # Get negation types from gpt4o data\n",
    "        neg_types = model_predictions['gpt4o_negation']['type'].tolist()\n",
    "        for neg_type in set(neg_types):\n",
    "            type_indices = [i for i, t in enumerate(neg_types) if t == neg_type]\n",
    "            t5_orig_type = t5_orig[type_indices]\n",
    "            t5_mod_type = t5_mod[type_indices]\n",
    "            \n",
    "            t5_orig_acc_type = np.round(t5_orig_type.mean() * 100, decimals=3)\n",
    "            t5_mod_acc_type = np.round(t5_mod_type.mean() * 100, decimals=3)\n",
    "            t5_pct_diff_type = np.round(((t5_mod_acc_type - t5_orig_acc_type) / t5_orig_acc_type) * 100, decimals=1)\n",
    "            t5_weighted_delta_type = np.round((t5_mod_acc_type - t5_orig_acc_type) * np.log10(t5_orig_acc_type) / np.log10(100), decimals=3)\n",
    "            \n",
    "            if np.array_equal(t5_orig_type, t5_mod_type):\n",
    "                wilcoxon_pvalue = 1.0\n",
    "                mannwhitney_pvalue = 1.0\n",
    "            else:\n",
    "                _, wilcoxon_pvalue = stats.wilcoxon(t5_orig_type, t5_mod_type)\n",
    "                _, mannwhitney_pvalue = stats.mannwhitneyu(t5_orig_type, t5_mod_type, alternative='two-sided')\n",
    "            pvalue = min(wilcoxon_pvalue, mannwhitney_pvalue)\n",
    "            significance = get_significance_level(pvalue)\n",
    "            \n",
    "            negation_results.append({\n",
    "                'model': 't5',\n",
    "                'modification': f'{neg_type}',\n",
    "                'original_acc': t5_orig_acc_type,\n",
    "                'modified_acc': t5_mod_acc_type,\n",
    "                'pct_diff': t5_pct_diff_type,\n",
    "                'weighted_delta': t5_weighted_delta_type,\n",
    "                'wilcoxon_pvalue': wilcoxon_pvalue,\n",
    "                'mannwhitney_pvalue': mannwhitney_pvalue,\n",
    "                'pvalue': pvalue,\n",
    "                'significance': significance,\n",
    "                'significant': pvalue < 0.05\n",
    "            })\n",
    "    \n",
    "\n",
    "# Process other models\n",
    "for model in model_names:\n",
    "    for mod in modifications:\n",
    "        model_data = model_predictions[f'{model}_{mod}']\n",
    "        model_orig = np.array([1 if p == l else 0 for p, l in zip(model_data['original_pred'], labels[mod]['original_label'])])\n",
    "        model_mod = np.array([1 if p == l else 0 for p, l in zip(model_data['modified_pred'], labels[mod]['modified_label'])])\n",
    "        model_orig_acc = np.round(model_orig.mean() * 100, decimals=3)\n",
    "        model_mod_acc = np.round(model_mod.mean() * 100, decimals=3)\n",
    "        model_pct_diff = np.round(((model_mod_acc - model_orig_acc) / model_orig_acc) * 100, decimals=1)\n",
    "        model_weighted_delta = np.round((model_mod_acc - model_orig_acc) * np.log10(model_orig_acc) / np.log10(100), decimals=3)\n",
    "        \n",
    "        if np.array_equal(model_orig, model_mod):\n",
    "            wilcoxon_pvalue = 1.0\n",
    "            mannwhitney_pvalue = 1.0\n",
    "        else:\n",
    "            _, wilcoxon_pvalue = stats.wilcoxon(model_orig, model_mod)\n",
    "            _, mannwhitney_pvalue = stats.mannwhitneyu(model_orig, model_mod, alternative='two-sided')\n",
    "        pvalue = min(wilcoxon_pvalue, mannwhitney_pvalue)\n",
    "        significance = get_significance_level(pvalue)\n",
    "        results.append({\n",
    "            'model': model,\n",
    "            'modification': mod,\n",
    "            'original_acc': model_orig_acc,\n",
    "            'modified_acc': model_mod_acc,\n",
    "            'pct_diff': model_pct_diff,\n",
    "            'weighted_delta': model_weighted_delta,\n",
    "            'wilcoxon_pvalue': wilcoxon_pvalue,\n",
    "            'mannwhitney_pvalue': mannwhitney_pvalue,\n",
    "            'pvalue': pvalue,\n",
    "            'significance': significance,\n",
    "            'significant': pvalue < 0.05\n",
    "        })\n",
    "\n",
    "        if mod == 'negation':\n",
    "            # Get negation types\n",
    "            neg_types = model_data['type'].tolist()\n",
    "            for neg_type in set(neg_types):\n",
    "                type_indices = [i for i, t in enumerate(neg_types) if t == neg_type]\n",
    "                model_orig_type = model_orig[type_indices]\n",
    "                model_mod_type = model_mod[type_indices]\n",
    "                \n",
    "                model_orig_acc_type = np.round(model_orig_type.mean() * 100, decimals=3)\n",
    "                model_mod_acc_type = np.round(model_mod_type.mean() * 100, decimals=3)\n",
    "                model_pct_diff_type = np.round(((model_mod_acc_type - model_orig_acc_type) / model_orig_acc_type) * 100, decimals=1)\n",
    "                model_weighted_delta_type = np.round((model_mod_acc_type - model_orig_acc_type) * np.log10(model_orig_acc_type) / np.log10(100), decimals=3)\n",
    "                \n",
    "                if np.array_equal(model_orig_type, model_mod_type):\n",
    "                    wilcoxon_pvalue = 1.0\n",
    "                    mannwhitney_pvalue = 1.0\n",
    "                else:\n",
    "                    _, wilcoxon_pvalue = stats.wilcoxon(model_orig_type, model_mod_type)\n",
    "                    _, mannwhitney_pvalue = stats.mannwhitneyu(model_orig_type, model_mod_type, alternative='two-sided')\n",
    "                pvalue = min(wilcoxon_pvalue, mannwhitney_pvalue)\n",
    "                significance = get_significance_level(pvalue)\n",
    "                \n",
    "                negation_results.append({\n",
    "                    'model': model,\n",
    "                    'modification': f'{neg_type}',\n",
    "                    'original_acc': model_orig_acc_type,\n",
    "                    'modified_acc': model_mod_acc_type,\n",
    "                    'pct_diff': model_pct_diff_type,\n",
    "                    'weighted_delta': model_weighted_delta_type,\n",
    "                    'wilcoxon_pvalue': wilcoxon_pvalue,\n",
    "                    'mannwhitney_pvalue': mannwhitney_pvalue,\n",
    "                    'pvalue': pvalue,\n",
    "                    'significance': significance,\n",
    "                    'significant': pvalue < 0.05\n",
    "                })\n",
    "        \n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "negation_results_df = pd.DataFrame(negation_results)\n",
    "\n",
    "results_df.to_csv('sentiment_analysis_results.csv', index=False)\n",
    "negation_results_df.to_csv('sentiment_analysis_negation_results.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LaTeX table saved to ner_results_table.tex\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "modification_order = [\n",
    "    (\"Bias\", \"Temporal\"), (\"Bias\", \"Geographical\"), (\"Bias\", \"Length\"),\n",
    "    (\"Orthographic\", \"Spelling\"), (\"Orthographic\", \"Capitalization\"), (\"Orthographic\", \"Punctuation\"),\n",
    "    (\"Morphological\", \"Derivation\"), (\"Morphological\", \"Compound\"),\n",
    "    (\"Syntactic\", \"Voice\"), (\"Syntactic\", \"Grammar\"), (\"Syntactic\", \"Conjunction\"),\n",
    "    (\"Semantic\", \"Concept\"), (\"Semantic\", \"Negation\"),\n",
    "    (\"Pragmatic\", \"Discourse\"), (\"Pragmatic\", \"Sentiment\"),\n",
    "    (\"Genre\", \"Casual\"), (\"Genre\", \"Dialectal\"), (\"Genre\", \"Singlish\")\n",
    "]\n",
    "\n",
    "# Read the combined results\n",
    "df = pd.read_csv('sentiment_analysis_results.csv')\n",
    "\n",
    "# Create mapping from modification names to standardized names\n",
    "mod_mapping = {\n",
    "    'temporal_bias': ('Bias', 'Temporal'),\n",
    "    'geographical_bias': ('Bias', 'Geographical'),\n",
    "    'length_bias': ('Bias', 'Length'),\n",
    "    'typo_bias': ('Orthographic', 'Spelling'),\n",
    "    'capitalization': ('Orthographic', 'Capitalization'),\n",
    "    'punctuation': ('Orthographic', 'Punctuation'),\n",
    "    'derivation': ('Morphological', 'Derivation'),\n",
    "    'compound_word': ('Morphological', 'Compound'),\n",
    "    'active_to_passive': ('Syntactic', 'Voice'),\n",
    "    'grammatical_role': ('Syntactic', 'Grammar'),\n",
    "    'coordinating_conjunction': ('Syntactic', 'Conjunction'),\n",
    "    'concept_replacement': ('Semantic', 'Concept'),\n",
    "    'negation': ('Semantic', 'Negation'),\n",
    "    'discourse': ('Pragmatic', 'Discourse'),\n",
    "    'sentiment': ('Pragmatic', 'Sentiment'),\n",
    "    'casual': ('Genre', 'Casual'),\n",
    "    'dialectal': ('Genre', 'Dialectal'),\n",
    "    'singlish': ('Genre', 'Singlish')\n",
    "}\n",
    "\n",
    "# Map the modification names\n",
    "df['category'], df['modification'] = zip(*df['modification'].map(mod_mapping))\n",
    "\n",
    "# Define model order\n",
    "model_order = ['BERT', 'GPT-2', 'T5', 'GPT-4o', 'Claude 3.5', 'Llama 3.1']\n",
    "model_name_map = {\n",
    "    'bert': 'BERT',\n",
    "    'gpt2': 'GPT-2', \n",
    "    't5': 'T5',\n",
    "    'gpt4o': 'GPT-4o',\n",
    "    'claude-3-5-sonnet': 'Claude 3.5',\n",
    "    'llama': 'Llama 3.1'\n",
    "}\n",
    "\n",
    "# Map model names\n",
    "df['model'] = df['model'].map(model_name_map)\n",
    "\n",
    "# Pivot the data to get modifications as rows and models as columns\n",
    "pivot_df = df.pivot(index=['category', 'modification'], columns='model', values='weighted_delta')\n",
    "significance_df = df.pivot(index=['category', 'modification'], columns='model', values='significance')\n",
    "\n",
    "# Reorder rows and columns\n",
    "pivot_df = pivot_df.reindex(modification_order, axis=0)\n",
    "pivot_df = pivot_df.reindex(model_order, axis=1)\n",
    "significance_df = significance_df.reindex(modification_order, axis=0)\n",
    "significance_df = significance_df.reindex(model_order, axis=1)\n",
    "\n",
    "# Function to generate color based on value\n",
    "def get_color(val, significance):\n",
    "    if np.isnan(val):\n",
    "        return ''\n",
    "    elif val > 0:\n",
    "        # Green gradient for positive values\n",
    "        intensity = min(abs(val)/5, 1) * 15  # Scale to max 5% change = 15 intensity\n",
    "        val_str = f'+{val:.2f}'\n",
    "        if significance == '**':\n",
    "            val_str = f'\\\\textbf{{{val_str}}}**'\n",
    "        elif significance == '*':\n",
    "            val_str = f'\\\\textbf{{{val_str}}}*'\n",
    "        elif significance == '.':\n",
    "            val_str = f'\\\\textbf{{{val_str}}}'\n",
    "        return f'\\\\cellcolor{{green!{int(intensity)}}} {val_str}'\n",
    "    else:\n",
    "        # Red gradient for negative values\n",
    "        intensity = min(abs(val)/5, 1) * 30  # Scale to max 5% change = 30 intensity\n",
    "        val_str = f'{val:.2f}'\n",
    "        if significance == '**':\n",
    "            val_str = f'\\\\textbf{{{val_str}}}**'\n",
    "        elif significance == '*':\n",
    "            val_str = f'\\\\textbf{{{val_str}}}*'\n",
    "        elif significance == '.':\n",
    "            val_str = f'\\\\textbf{{{val_str}}}'\n",
    "        return f'\\\\cellcolor{{red!{int(intensity)}}} {val_str}'\n",
    "\n",
    "# Generate LaTeX table\n",
    "latex_table = '\\\\begin{table}[h]\\n\\\\centering\\n\\\\resizebox{\\\\linewidth}{!}{\\n'\n",
    "latex_table += '\\\\begin{tabular}{llr' + 'r'*(len(pivot_df.columns)) + '}\\n'\n",
    "latex_table += '\\\\hline\\n'\n",
    "latex_table += 'Category & Modification & ' + ' & '.join([f'\\\\textbf{{{col}}}' for col in pivot_df.columns]) + ' \\\\\\\\\\n'\n",
    "latex_table += '\\\\hline\\n'\n",
    "\n",
    "prev_category = None\n",
    "for (category, modification), row in pivot_df.iterrows():\n",
    "    if category != prev_category:\n",
    "        latex_table += f'\\\\textbf{{{category}}} & \\\\textbf{{{modification}}} & '\n",
    "        prev_category = category\n",
    "    else:\n",
    "        latex_table += f' & \\\\textbf{{{modification}}} & '\n",
    "    \n",
    "    latex_table += ' & '.join([get_color(val, significance_df.loc[(category, modification), col]) for col, val in row.items()]) + ' \\\\\\\\\\n'\n",
    "\n",
    "latex_table += '\\\\hline\\n'\n",
    "latex_table += '\\\\end{tabular}}\\n'\n",
    "latex_table += '\\\\caption{Weighted Delta Score by Model and Modification Type}\\n'\n",
    "latex_table += '\\\\label{tab:ner_results}\\n'\n",
    "latex_table += '\\\\end{table}'\n",
    "\n",
    "# Save to file\n",
    "with open('sa_results_table.tex', 'w') as f:\n",
    "    f.write(latex_table)\n",
    "\n",
    "print(\"LaTeX table saved to ner_results_table.tex\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  model   modification  original_acc  modified_acc  pct_diff  weighted_delta  \\\n",
      "2  bert  temporal_bias          92.0          89.0      -3.3          -2.946   \n",
      "\n",
      "   wilcoxon_pvalue  mannwhitney_pvalue    pvalue significance  significant  \n",
      "2         0.179712            0.471985  0.179712           ns        False  \n",
      "Results saved to sentiment_analysis_results_df.csv and negation_results_df.csv\n"
     ]
    }
   ],
   "source": [
    "# Load results from CSV\n",
    "results_df = pd.read_csv('sentiment_analysis_results.csv')\n",
    "negation_df = pd.read_csv('sentiment_analysis_negation_results.csv')\n",
    "\n",
    "# Create lists of unique modifications and models\n",
    "modification_order = list(mod_mapping.keys())\n",
    "negation_order = ['verbal', 'lexical', 'double', 'approximate', 'absolute']\n",
    "model_order = ['bert', 'gpt2', 't5', 'gpt4o', 'claude-3-5-sonnet', 'llama']\n",
    "\n",
    "# Create empty DataFrame with multi-level columns\n",
    "columns = pd.MultiIndex.from_product([model_order, ['original', 'modified', 'diff']])\n",
    "results_df_pivot = pd.DataFrame(index=modification_order, columns=columns)\n",
    "negation_df_pivot = pd.DataFrame(index=negation_order, columns=columns)\n",
    "\n",
    "# Fill DataFrame for sentiment analysis results\n",
    "for mod in modification_order:\n",
    "    for model in model_order:\n",
    "        row = results_df[(results_df['modification'] == mod) & (results_df['model'] == model)]\n",
    "        if not row.empty:\n",
    "            results_df_pivot.loc[mod, (model, 'original')] = row['original_acc'].values[0]\n",
    "            results_df_pivot.loc[mod, (model, 'modified')] = row['modified_acc'].values[0]\n",
    "            results_df_pivot.loc[mod, (model, 'diff')] = row['pct_diff'].values[0]\n",
    "            if mod == 'temporal_bias' and model == 'bert':\n",
    "                print(row)\n",
    "\n",
    "# Fill DataFrame for negation results\n",
    "for mod in negation_order:\n",
    "    for model in model_order:\n",
    "        row = negation_df[(negation_df['modification'] == mod) & (negation_df['model'] == model)]\n",
    "        if not row.empty:\n",
    "            negation_df_pivot.loc[mod, (model, 'original')] = row['original_acc'].values[0]\n",
    "            negation_df_pivot.loc[mod, (model, 'modified')] = row['modified_acc'].values[0]\n",
    "            negation_df_pivot.loc[mod, (model, 'diff')] = row['pct_diff'].values[0]\n",
    "\n",
    "# Save to CSV\n",
    "results_df_pivot.to_csv('sentiment_analysis_results_df.csv')\n",
    "negation_df_pivot.to_csv('negation_results_df.csv')\n",
    "\n",
    "print(\"Results saved to sentiment_analysis_results_df.csv and negation_results_df.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model           bert  claude-3-5-sonnet    gpt2   gpt4o   llama      t5\n",
      "modification                                                           \n",
      "Verbal       -15.409            -10.273 -20.545 -20.806  -5.201 -31.208\n",
      "Lexical       -6.667              0.000   0.000   6.567   0.000 -20.000\n",
      "Double       -25.000            -34.610  -9.771 -19.542 -10.000 -30.000\n",
      "Approximate   -7.149            -14.155 -10.617  -6.924 -14.155 -21.654\n",
      "Absolute     -33.333              0.000 -20.000  -6.344 -13.133 -20.000\n",
      "LaTeX table saved to sentiment_analysis_negation_type_results_table.tex\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Read the combined results\n",
    "df = pd.read_csv('sentiment_analysis_negation_results.csv')\n",
    "\n",
    "# Create mapping from modification names to standardized names\n",
    "negation_order= ['Verbal', 'Lexical', 'Double', 'Approximate', 'Absolute']\n",
    "mod_mapping = {\n",
    "    'verbal': 'Verbal',\n",
    "    'lexical': 'Lexical',\n",
    "    'double': 'Double',\n",
    "    'approximate': 'Approximate',\n",
    "    'absolute': 'Absolute',\n",
    "}\n",
    "\n",
    "# Define model order\n",
    "model_order = ['bert', 'gpt2', 't5', 'gpt4o', 'claude-3-5-sonnet', 'llama']\n",
    "\n",
    "# Map the modification names\n",
    "df['modification'] = df['modification'].map(mod_mapping)\n",
    "\n",
    "# Count samples for each modification type by counting unique rows\n",
    "sample_counts = df.groupby('modification').size()\n",
    "\n",
    "# Pivot the data to get modifications as rows and models as columns\n",
    "pivot_df = df.pivot(index='modification', columns='model', values='weighted_delta')\n",
    "p_values = df.pivot(index='modification', columns='model', values='pvalue')\n",
    "significance = df.pivot(index='modification', columns='model', values='significance')\n",
    "\n",
    "# Reorder rows according to modification_order\n",
    "pivot_df = pivot_df.reindex(negation_order)\n",
    "p_values = p_values.reindex(negation_order)\n",
    "significance = significance.reindex(negation_order)\n",
    "sample_counts = sample_counts.reindex(negation_order)\n",
    "\n",
    "print(pivot_df)\n",
    "\n",
    "# Function to generate color based on value\n",
    "def get_color(val, sig):\n",
    "    if np.isnan(val):\n",
    "        return ''\n",
    "    elif val > 0:\n",
    "        # Green gradient for positive values\n",
    "        intensity = min(abs(val)/10, 1)  # Scale to max 10% change\n",
    "        val_str = f'+{val:.2f}'\n",
    "        if sig == '.':  # Just bold for '.'\n",
    "            val_str = f'\\\\textbf{{{val_str}}}'\n",
    "        elif sig == '*':  # One asterisk\n",
    "            val_str = f'\\\\textbf{{{val_str}}}*'\n",
    "        elif sig == '**':  # Two asterisks\n",
    "            val_str = f'\\\\textbf{{{val_str}}}**'\n",
    "        return f'\\\\cellcolor{{green!{int(intensity*30)}}} {val_str}'\n",
    "    else:\n",
    "        # Red gradient for negative values\n",
    "        intensity = min(abs(val)/10, 1)  # Scale to max 10% change\n",
    "        val_str = f'{val:.2f}'\n",
    "        if sig == '.':  # Just bold for '.'\n",
    "            val_str = f'\\\\textbf{{{val_str}}}'\n",
    "        elif sig == '*':  # One asterisk\n",
    "            val_str = f'\\\\textbf{{{val_str}}}*'\n",
    "        elif sig == '**':  # Two asterisks\n",
    "            val_str = f'\\\\textbf{{{val_str}}}**'\n",
    "        return f'\\\\cellcolor{{red!{int(intensity*30)}}} {val_str}'\n",
    "\n",
    "# Generate LaTeX table\n",
    "latex_table = '\\\\begin{table}[h]\\n\\\\centering\\n\\\\begin{tabular}{lrr' + 'r'*len(model_order) + '}\\n'\n",
    "latex_table += '\\\\hline\\n'\n",
    "latex_table += 'Modification & N & ' + ' & '.join([f'\\\\textbf{{{col}}}' for col in model_order]) + ' \\\\\\\\\\n'\n",
    "latex_table += '\\\\hline\\n'\n",
    "\n",
    "prev_category = None\n",
    "for idx, row in pivot_df.iterrows():\n",
    "    current_category = idx[0]  # Get first character of modification name\n",
    "    if prev_category is not None and current_category != prev_category:\n",
    "        latex_table += '\\\\hline\\n'\n",
    "    prev_category = current_category\n",
    "    \n",
    "    latex_table += f'\\\\textbf{{{idx}}} & {int(sample_counts[idx])} & '\n",
    "    latex_table += ' & '.join([get_color(row[col], significance.loc[idx, col]) for col in model_order]) + ' \\\\\\\\\\n'\n",
    "\n",
    "latex_table += '\\\\hline\\n'\n",
    "latex_table += '\\\\end{tabular}\\n'\n",
    "latex_table += '\\\\caption{Weighted Delta Score by Model and Modification Type}\\n'\n",
    "latex_table += '\\\\label{tab:ner_results}\\n'\n",
    "latex_table += '\\\\end{table}'\n",
    "\n",
    "# Save to file\n",
    "with open('sentiment_analysis_negation_type_results_table.tex', 'w') as f:\n",
    "    f.write(latex_table)\n",
    "\n",
    "print(\"LaTeX table saved to sentiment_analysis_negation_type_results_table.tex\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
