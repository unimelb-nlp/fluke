{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def parse_ner_predictions(input_file):\n",
    "    \"\"\"Parse NER predictions into structured format\"\"\"\n",
    "    with open(input_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    parsed_data = []\n",
    "    \n",
    "    for item in data:\n",
    "        # Get the original text, tokens and labels\n",
    "        text = item['text']\n",
    "        tokens = item['tokenized_text']\n",
    "        gold_labels = item['gold_label']\n",
    "        pred_labels = item['prediction']\n",
    "        \n",
    "        # Sanity check that lengths match\n",
    "        if len(tokens) != len(gold_labels) or len(tokens) != len(pred_labels):\n",
    "            raise ValueError(f\"Mismatched lengths in {input_file}: tokens={len(tokens)}, gold={len(gold_labels)}, pred={len(pred_labels)}\")\n",
    "        \n",
    "        # Build gold and prediction lists\n",
    "        gold = []\n",
    "        pred = []\n",
    "        \n",
    "        # Track multi-token entities\n",
    "        curr_gold_entity = ''\n",
    "        curr_gold_text = []\n",
    "        curr_pred_entity = ''\n",
    "        curr_pred_text = []\n",
    "        \n",
    "        for token, gold_label, pred_label in zip(tokens, gold_labels, pred_labels):\n",
    "            # Handle gold labels\n",
    "            if gold_label.startswith('B-'):\n",
    "                if curr_gold_entity:\n",
    "                    gold.append({'text': ' '.join(curr_gold_text), 'value': curr_gold_entity.upper()})\n",
    "                curr_gold_entity = gold_label[2:]\n",
    "                curr_gold_text = [token]\n",
    "            elif gold_label.startswith('I-'):\n",
    "                if curr_gold_entity == gold_label[2:]:\n",
    "                    curr_gold_text.append(token)\n",
    "            elif gold_label == 'O':\n",
    "                if curr_gold_entity:\n",
    "                    gold.append({'text': ' '.join(curr_gold_text), 'value': curr_gold_entity.upper()})\n",
    "                    curr_gold_entity = ''\n",
    "                    curr_gold_text = []\n",
    "            \n",
    "            # Handle predicted labels\n",
    "            if pred_label.startswith('B-'):\n",
    "                if curr_pred_entity:\n",
    "                    pred.append({'text': ' '.join(curr_pred_text), 'value': curr_pred_entity.upper()})\n",
    "                curr_pred_entity = pred_label[2:]\n",
    "                curr_pred_text = [token]\n",
    "            elif pred_label.startswith('I-'):\n",
    "                if curr_pred_entity == pred_label[2:]:\n",
    "                    curr_pred_text.append(token)\n",
    "            elif pred_label == 'O':\n",
    "                if curr_pred_entity:\n",
    "                    pred.append({'text': ' '.join(curr_pred_text), 'value': curr_pred_entity.upper()})\n",
    "                    curr_pred_entity = ''\n",
    "                    curr_pred_text = []\n",
    "        \n",
    "        # Add any remaining entities\n",
    "        if curr_gold_entity:\n",
    "            gold.append({'text': ' '.join(curr_gold_text), 'value': curr_gold_entity.upper()})\n",
    "        if curr_pred_entity:\n",
    "            pred.append({'text': ' '.join(curr_pred_text), 'value': curr_pred_entity.upper()})\n",
    "            \n",
    "        parsed_item = {\n",
    "            'text': text,\n",
    "            'gold': gold,\n",
    "            'prediction': pred\n",
    "        }\n",
    "        parsed_data.append(parsed_item)\n",
    "    \n",
    "    return parsed_data\n",
    "\n",
    "def process_ner_files(model_name):\n",
    "    \"\"\"Process all NER files for a given model\"\"\"\n",
    "    input_dir = f'NER_{model_name}'\n",
    "    output_dir = f'parsed_NER_{model_name}'\n",
    "    \n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        \n",
    "    for filename in os.listdir(input_dir):\n",
    "        if 'casual_100' not in filename:\n",
    "            continue\n",
    "        if filename.endswith('.json'):\n",
    "            input_path = os.path.join(input_dir, filename)\n",
    "            output_path = os.path.join(output_dir, filename)\n",
    "            \n",
    "            parsed_data = parse_ner_predictions(input_path)\n",
    "            \n",
    "            # Sanity check number of entities matches between input and parsed\n",
    "            with open(input_path, 'r') as f:\n",
    "                input_data = json.load(f)\n",
    "            total_gold_count_input = 0    \n",
    "            total_pred_count_input = 0\n",
    "            total_gold_count_parsed = 0\n",
    "            total_pred_count_parsed = 0\n",
    "            for i, (input_item, parsed_item) in enumerate(zip(input_data, parsed_data)):\n",
    "                gold_count = sum(1 for label in input_item['gold_label'] if label.startswith('B-'))\n",
    "                pred_count = sum(1 for label in input_item['prediction'] if label.startswith('B-'))\n",
    "                total_gold_count_input += gold_count\n",
    "                total_pred_count_input += pred_count\n",
    "                total_gold_count_parsed += len(parsed_item['gold'])\n",
    "                total_pred_count_parsed += len(parsed_item['prediction'])\n",
    "                if len(parsed_item['gold']) != gold_count:\n",
    "                    raise ValueError(f\"Mismatch in gold entities for item {i} in {filename}: {len(parsed_item['gold'])} vs {gold_count}\")\n",
    "                if len(parsed_item['prediction']) != pred_count:\n",
    "                    raise ValueError(f\"Mismatch in predicted entities for item {i} in {filename}: {len(parsed_item['prediction'])} vs {pred_count}\")\n",
    "            if filename == 'bert_active_to_passive_ori.json':\n",
    "                # Count entities by class for gold labels\n",
    "                input_gold_class_counts = {}\n",
    "                input_pred_class_counts = {}\n",
    "                parsed_gold_class_counts = {}\n",
    "                parsed_pred_class_counts = {}\n",
    "                \n",
    "                # Count from input data\n",
    "                for item in input_data:\n",
    "                    for label in item['gold_label']:\n",
    "                        if label.startswith('B-'):\n",
    "                            entity_class = label[2:]\n",
    "                            input_gold_class_counts[entity_class] = input_gold_class_counts.get(entity_class, 0) + 1\n",
    "                    for label in item['prediction']:\n",
    "                        if label.startswith('B-'):\n",
    "                            entity_class = label[2:]\n",
    "                            input_pred_class_counts[entity_class] = input_pred_class_counts.get(entity_class, 0) + 1\n",
    "                \n",
    "                # Count from parsed data            \n",
    "                for item in parsed_data:\n",
    "                    for entity in item['gold']:\n",
    "                        entity_class = entity['value']\n",
    "                        parsed_gold_class_counts[entity_class] = parsed_gold_class_counts.get(entity_class, 0) + 1\n",
    "                    for entity in item['prediction']:\n",
    "                        entity_class = entity['value'] \n",
    "                        parsed_pred_class_counts[entity_class] = parsed_pred_class_counts.get(entity_class, 0) + 1\n",
    "                \n",
    "                print(\"Input gold entity counts by class:\")\n",
    "                for entity_class, count in input_gold_class_counts.items():\n",
    "                    print(f\"{entity_class}: {count}\")\n",
    "                print(\"\\nInput predicted entity counts by class:\")\n",
    "                for entity_class, count in input_pred_class_counts.items():\n",
    "                    print(f\"{entity_class}: {count}\")\n",
    "                print(\"\\nParsed gold entity counts by class:\")\n",
    "                for entity_class, count in parsed_gold_class_counts.items():\n",
    "                    print(f\"{entity_class}: {count}\")\n",
    "                print(\"\\nParsed predicted entity counts by class:\")\n",
    "                for entity_class, count in parsed_pred_class_counts.items():\n",
    "                    print(f\"{entity_class}: {count}\")\n",
    "            with open(output_path, 'w') as f:\n",
    "                json.dump(parsed_data, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_ner_files('BERT')\n",
    "process_ner_files('GPT2')\n",
    "process_ner_files('T5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import ast\n",
    "import difflib\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_example_f1_and_counts(example):\n",
    "    true_entities = []\n",
    "    pred_entities = []\n",
    "    \n",
    "    # Get entities from appropriate field names\n",
    "    gold_entities = example['gold']\n",
    "    pred_entities_raw = example['prediction']\n",
    "    \n",
    "    # Handle empty case\n",
    "    if not gold_entities and not pred_entities_raw:\n",
    "        return 0.0, {}\n",
    "\n",
    "    # Process gold entities\n",
    "    for entity in gold_entities:\n",
    "        if isinstance(entity, str):\n",
    "            entity = ast.literal_eval(entity)\n",
    "        # Store as tuple of (text, value, class) to handle duplicates\n",
    "        if entity.get('text') is not None and entity.get('value') is not None:\n",
    "            true_entities.append((entity['text'], entity['value'], entity['value']))\n",
    "        elif entity.get('text') is not None and entity.get('class') is not None:\n",
    "            true_entities.append((entity['text'], entity['class'], entity['class']))\n",
    "        else:\n",
    "            # Handle dictionary format entities\n",
    "            for key, value in entity.items():\n",
    "                if isinstance(value, str):\n",
    "                    true_entities.append((key, value, value))\n",
    "\n",
    "    # Process predicted entities\n",
    "    for entity in pred_entities_raw:\n",
    "        if isinstance(entity, str):\n",
    "            entity = ast.literal_eval(entity)\n",
    "        if entity.get('text') is not None:\n",
    "            pred_entities.append((entity['text'], entity['value'], entity['value']))\n",
    "        else:\n",
    "            for key, value in entity.items():\n",
    "                if isinstance(value, str):\n",
    "                    pred_entities.append((key, value, value))\n",
    "    \n",
    "    # Calculate per-class counts\n",
    "    class_counts = {}\n",
    "    # Get unique classes from both true and predicted entities to ensure complete coverage\n",
    "    classes = set(e[2] for e in true_entities) | set(e[2] for e in pred_entities)\n",
    "    \n",
    "    for cls in classes:\n",
    "        # Get entities for this class\n",
    "        true_cls = [e for e in true_entities if e[2] == cls]\n",
    "        pred_cls = [e for e in pred_entities if e[2] == cls]\n",
    "        \n",
    "        # Calculate counts for this class allowing for duplicates\n",
    "        tp = sum(1 for t in true_cls if t in pred_cls)\n",
    "        # Count false positives - predictions that don't match any gold entity\n",
    "        fp = len(pred_cls) - tp\n",
    "        # Count false negatives - gold entities that weren't predicted\n",
    "        fn = len(true_cls) - tp\n",
    "        \n",
    "        class_counts[cls] = (tp, fp, fn)\n",
    "    \n",
    "    # Calculate overall F1 for the example\n",
    "    total_tp = sum(counts[0] for counts in class_counts.values())\n",
    "    total_fp = sum(counts[1] for counts in class_counts.values())\n",
    "    total_fn = sum(counts[2] for counts in class_counts.values())\n",
    "    \n",
    "    # Handle edge case where no true positives\n",
    "    if total_tp == 0:\n",
    "        return 0.0, class_counts\n",
    "        \n",
    "    precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0\n",
    "    recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0\n",
    "    \n",
    "    if precision + recall == 0:\n",
    "        return 0.0, class_counts\n",
    "        \n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    return f1, class_counts\n",
    "\n",
    "def get_f1_scores_and_counts(data):\n",
    "    if not data:\n",
    "        return [], {}\n",
    "        \n",
    "    scores_and_counts = [get_example_f1_and_counts(example) for example in data]\n",
    "    f1_scores = [score for score, _ in scores_and_counts]\n",
    "    \n",
    "    # Combine per-class counts across all examples\n",
    "    class_counts = {}\n",
    "    for _, example_counts in scores_and_counts:\n",
    "        for cls, (tp, fp, fn) in example_counts.items():\n",
    "            if cls not in class_counts:\n",
    "                class_counts[cls] = [0, 0, 0]\n",
    "            class_counts[cls][0] += tp  # Add true positives\n",
    "            class_counts[cls][1] += fp  # Add false positives\n",
    "            class_counts[cls][2] += fn  # Add false negatives\n",
    "            \n",
    "    return f1_scores, class_counts\n",
    "\n",
    "def calculate_micro_f1(counts):\n",
    "    if isinstance(counts, tuple):\n",
    "        tp, fp, fn = counts\n",
    "        if tp == 0:\n",
    "            return 0.0\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        if precision + recall == 0:\n",
    "            return 0.0\n",
    "        return 2 * (precision * recall) / (precision + recall)\n",
    "    else:\n",
    "        # Calculate micro F1 across all classes\n",
    "        total_tp = sum(counts[cls][0] for cls in counts)\n",
    "        total_fp = sum(counts[cls][1] for cls in counts)\n",
    "        total_fn = sum(counts[cls][2] for cls in counts)\n",
    "        \n",
    "        if total_tp == 0:\n",
    "            return {'micro_f1': 0.0, 'support': 0}\n",
    "            \n",
    "        precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0\n",
    "        recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0\n",
    "        \n",
    "        if precision + recall == 0:\n",
    "            micro_f1 = 0.0\n",
    "        else:\n",
    "            micro_f1 = 2 * (precision * recall) / (precision + recall)\n",
    "            \n",
    "        # Calculate per-class metrics\n",
    "        class_f1s = {'micro_f1': micro_f1}\n",
    "        total_support = 0\n",
    "        \n",
    "        for cls, (tp, fp, fn) in counts.items():\n",
    "            support = tp + fn  # Support is true positives + false negatives\n",
    "            total_support += support\n",
    "            \n",
    "            if tp == 0:\n",
    "                class_f1s[cls] = {'f1': 0.0, 'support': support}\n",
    "                continue\n",
    "                \n",
    "            precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "            recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "            \n",
    "            if precision + recall == 0:\n",
    "                class_f1s[cls] = {'f1': 0.0, 'support': support}\n",
    "            else:\n",
    "                f1 = 2 * (precision * recall) / (precision + recall)\n",
    "                class_f1s[cls] = {'f1': f1, 'support': support}\n",
    "        \n",
    "        class_f1s['support'] = total_support\n",
    "        return class_f1s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2 casual\n",
      "GPT2 dialectal\n",
      "GPT2 derivation\n",
      "GPT2 grammatical_role\n",
      "GPT2 discourse\n",
      "GPT2 punctuation\n",
      "GPT2 concept_replacement\n",
      "GPT2 geographical_bias\n",
      "GPT2 length_bias\n",
      "GPT2 coordinating_conjunction\n",
      "GPT2 capitalization\n",
      "GPT2 negation\n",
      "GPT2 active_to_passive\n",
      "GPT2 sentiment\n",
      "GPT2 typo_bias\n",
      "GPT2 singlish\n",
      "GPT2 temporal_bias\n",
      "GPT2 compound_word\n",
      "BERT casual\n",
      "BERT dialectal\n",
      "BERT derivation\n",
      "BERT grammatical_role\n",
      "BERT discourse\n",
      "BERT punctuation\n",
      "BERT concept_replacement\n",
      "BERT geographical_bias\n",
      "BERT length_bias\n",
      "BERT coordinating_conjunction\n",
      "BERT capitalization\n",
      "BERT negation\n",
      "BERT active_to_passive\n",
      "BERT sentiment\n",
      "BERT typo_bias\n",
      "BERT singlish\n",
      "BERT temporal_bias\n",
      "BERT compound_word\n",
      "T5 casual\n",
      "T5 dialectal\n",
      "T5 derivation\n",
      "T5 grammatical_role\n",
      "T5 discourse\n",
      "T5 punctuation\n",
      "T5 concept_replacement\n",
      "T5 geographical_bias\n",
      "T5 length_bias\n",
      "T5 coordinating_conjunction\n",
      "T5 capitalization\n",
      "T5 negation\n",
      "T5 active_to_passive\n",
      "T5 sentiment\n",
      "T5 typo_bias\n",
      "T5 singlish\n",
      "T5 temporal_bias\n",
      "T5 compound_word\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "import ast\n",
    "def load_json_file(filepath):\n",
    "    with open(filepath, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Get list of all modifications from filenames\n",
    "modifications = set()\n",
    "models = ['GPT2', 'BERT', 'T5']\n",
    "for model in models:\n",
    "    model_dir = f'parsed_NER_{model}'\n",
    "    for filename in os.listdir(model_dir):\n",
    "        if filename.startswith(f'{model.lower()}_') and filename.endswith('_ori.json'):\n",
    "            mod = filename.replace(f'{model.lower()}_', '').replace('_ori.json', '')\n",
    "            modifications.add(mod)\n",
    "modifications = list(modifications)\n",
    "\n",
    "# Load original and modified files for each model\n",
    "ori_files = {model: {} for model in models}\n",
    "modif_files = {model: {} for model in models}\n",
    "\n",
    "for model in models:\n",
    "    model_dir = f'parsed_NER_{model}'\n",
    "    for modification in modifications:\n",
    "        # Load original files\n",
    "        ori_filepath = f'{model_dir}/{model.lower()}_{modification}_ori.json'\n",
    "        ori_files[model][modification] = load_json_file(ori_filepath)\n",
    "        \n",
    "        # Load modified files\n",
    "        modif_filepath = f'{model_dir}/{model.lower()}_{modification}_modif.json'\n",
    "        modif_files[model][modification] = load_json_file(modif_filepath)\n",
    "\n",
    "\n",
    "# Calculate and store F1 scores for each model\n",
    "results = []\n",
    "negation_type_results = []  # For storing negation type breakdown\n",
    "\n",
    "for model in models:\n",
    "    for modification in modifications:\n",
    "        compare_file = Path(f'../../preprocessing/data_after_phase2/rahmad/{modification}_100.json')\n",
    "        print(model, modification)\n",
    "        if not compare_file.exists():\n",
    "            continue\n",
    "        compare_df = json.load(open(compare_file))\n",
    "        if len(compare_df) != len(ori_files[model][modification]):\n",
    "            print('mismatch',modification, model)\n",
    "            print(len(compare_df), len(ori_files[model][modification]))\n",
    "        ori_f1_scores, ori_counts = get_f1_scores_and_counts(ori_files[model][modification])\n",
    "        modif_f1_scores, modif_counts = get_f1_scores_and_counts(modif_files[model][modification])\n",
    "        \n",
    "        # Multiply F1 scores by 100\n",
    "        ori_f1_scores = [f1 * 100 for f1 in ori_f1_scores]\n",
    "        modif_f1_scores = [f1 * 100 for f1 in modif_f1_scores]\n",
    "        \n",
    "        # Calculate mean F1 scores\n",
    "        ori_mean_f1 = np.mean(ori_f1_scores)\n",
    "        modif_mean_f1 = np.mean(modif_f1_scores)\n",
    "        # Calculate micro F1 scores\n",
    "        ori_micro_f1 = calculate_micro_f1(ori_counts)\n",
    "        modif_micro_f1 = calculate_micro_f1(modif_counts)\n",
    "        \n",
    "        # Multiply micro F1 scores by 100\n",
    "        ori_micro_f1['micro_f1'] *= 100\n",
    "        modif_micro_f1['micro_f1'] *= 100\n",
    "        \n",
    "        # Calculate percentage change\n",
    "        mean_f1_pct_change = ((modif_mean_f1 - ori_mean_f1) / ori_mean_f1) * 100\n",
    "        micro_f1_pct_change = ((modif_micro_f1['micro_f1'] - ori_micro_f1['micro_f1']) / ori_micro_f1['micro_f1']) * 100\n",
    "        # Calculate weighted delta\n",
    "        weighted_delta = (modif_mean_f1 - ori_mean_f1) * np.log10(ori_mean_f1) / np.log10(100)\n",
    "        # print(ori_micro_f1, modif_micro_f1)\n",
    "        # Perform paired t-test on per-example F1 scores\n",
    "        try:\n",
    "            _, p_wilcoxon = stats.wilcoxon(ori_f1_scores, modif_f1_scores)\n",
    "        except ValueError:\n",
    "            p_wilcoxon = 1.0\n",
    "        try:\n",
    "            _, p_mannwhitney = stats.mannwhitneyu(ori_f1_scores, modif_f1_scores)\n",
    "        except ValueError:\n",
    "            p_mannwhitney = 1.0\n",
    "        p_value = min(p_wilcoxon, p_mannwhitney)\n",
    "        \n",
    "        # Determine significance level\n",
    "        if p_value < 0.01:\n",
    "            significance = \"**\"\n",
    "        elif p_value < 0.05:\n",
    "            significance = \"*\"\n",
    "        elif p_value < 0.1:\n",
    "            significance = \".\"\n",
    "        else:\n",
    "            significance = \"ns\"\n",
    "        \n",
    "        results.append({\n",
    "            'model': model,\n",
    "            'modification': modification,\n",
    "            'original_mean_f1': ori_mean_f1,\n",
    "            'modified_mean_f1': modif_mean_f1,\n",
    "            'mean_f1_pct_change': mean_f1_pct_change,\n",
    "            'original_micro_f1': ori_micro_f1,\n",
    "            'modified_micro_f1': modif_micro_f1,\n",
    "            'micro_f1_pct_change': micro_f1_pct_change,\n",
    "            'weighted_delta': weighted_delta,\n",
    "            'p_value': p_value,\n",
    "            'significance': significance\n",
    "        })\n",
    "        \n",
    "        # Additional analysis for negation types\n",
    "        if modification == 'negation':\n",
    "            # Load negation type information\n",
    "            negation_file = Path(f'../../preprocessing/data_after_phase2/rahmad/negation_100.json')\n",
    "            negation_data = json.load(open(negation_file))\n",
    "            \n",
    "            # Group examples by negation type\n",
    "            type_results = {}\n",
    "            for idx, (ori_f1, mod_f1) in enumerate(zip(ori_f1_scores, modif_f1_scores)):\n",
    "                neg_type = negation_data[idx].get('subtype', 'unknown')\n",
    "                if neg_type not in type_results:\n",
    "                    type_results[neg_type] = {'ori_f1s': [], 'mod_f1s': []}\n",
    "                type_results[neg_type]['ori_f1s'].append(ori_f1)\n",
    "                type_results[neg_type]['mod_f1s'].append(mod_f1)\n",
    "            \n",
    "            # Calculate metrics for each negation type\n",
    "            for neg_type, scores in type_results.items():\n",
    "                ori_mean = np.mean(scores['ori_f1s'])\n",
    "                mod_mean = np.mean(scores['mod_f1s'])\n",
    "                pct_change = ((mod_mean - ori_mean) / ori_mean) * 100 if ori_mean > 0 else 0\n",
    "                weighted_delta = (mod_mean - ori_mean) * np.log10(ori_mean) / np.log10(100)\n",
    "                \n",
    "                # Statistical tests\n",
    "                if len(scores['ori_f1s']) > 1:  # Only if we have enough samples\n",
    "                    try:    \n",
    "                        _, p_wilcoxon = stats.wilcoxon(scores['ori_f1s'], scores['mod_f1s'])\n",
    "                    except ValueError:\n",
    "                        p_wilcoxon = 1.0\n",
    "                    try:\n",
    "                        _, p_mannwhitney = stats.mannwhitneyu(scores['ori_f1s'], scores['mod_f1s'])\n",
    "                    except ValueError:\n",
    "                        p_mannwhitney = 1.0\n",
    "                    p_value = min(p_wilcoxon, p_mannwhitney)\n",
    "                else:\n",
    "                    p_value = 1.0\n",
    "                \n",
    "                # Determine significance level for negation types\n",
    "                if p_value < 0.01:\n",
    "                    significance = \"**\"\n",
    "                elif p_value < 0.05:\n",
    "                    significance = \"*\"\n",
    "                elif p_value < 0.1:\n",
    "                    significance = \".\"\n",
    "                else:\n",
    "                    significance = \"ns\"\n",
    "                \n",
    "                negation_type_results.append({\n",
    "                    'model': model,\n",
    "                    'negation_type': neg_type,\n",
    "                    'original_mean_f1': ori_mean,\n",
    "                    'modified_mean_f1': mod_mean,\n",
    "                    'mean_f1_pct_change': pct_change,\n",
    "                    'weighted_delta': weighted_delta,\n",
    "                    'sample_size': len(scores['ori_f1s']),\n",
    "                    'p_value': p_value,\n",
    "                    'significance': significance\n",
    "                })\n",
    "\n",
    "# Create DataFrame and save to CSV\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv('ner_modification_results_plm.csv', index=False)\n",
    "\n",
    "# Save negation type results\n",
    "if negation_type_results:\n",
    "    df_negation = pd.DataFrame(negation_type_results)\n",
    "    df_negation.to_csv('ner_negation_type_results_plm.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_example_f1_and_counts_list(gold, pred):\n",
    "    # Convert string labels to lists if needed\n",
    "    if isinstance(gold, str):\n",
    "        gold = ast.literal_eval(gold)\n",
    "    if isinstance(pred, str):\n",
    "        pred = ast.literal_eval(pred)\n",
    "\n",
    "    # Standardize format to list of dicts with 'text' and 'value' keys\n",
    "    def standardize_format(data):\n",
    "        if isinstance(data, dict):\n",
    "            return [{'text': k, 'value': v} for k, v in data.items()]\n",
    "        elif isinstance(data, list) and len(data) > 0:\n",
    "            if isinstance(data[0], dict):\n",
    "                standardized = []\n",
    "                for item in data:\n",
    "                    if 'text' not in item:\n",
    "                        for text, value in item.items():\n",
    "                            standardized.append({'text': text, 'value': value})\n",
    "                    else:\n",
    "                        standardized.append(item)\n",
    "                return standardized\n",
    "        return data\n",
    "\n",
    "    gold = standardize_format(gold)\n",
    "    pred = standardize_format(pred)\n",
    "\n",
    "    # Calculate metrics by comparing each prediction against gold\n",
    "    tp = 0\n",
    "    gold_matched = [False] * len(gold)\n",
    "    pred_matched = [False] * len(pred)\n",
    "\n",
    "    # First pass - find exact matches\n",
    "    for i, p in enumerate(pred):\n",
    "        for j, g in enumerate(gold):\n",
    "            if not gold_matched[j] and not pred_matched[i]:\n",
    "                if p['text'] == g['text'] and p['value'] == g['value']:\n",
    "                    tp += 1\n",
    "                    gold_matched[j] = True\n",
    "                    pred_matched[i] = True\n",
    "\n",
    "    # Calculate false positives and false negatives\n",
    "    fp = len(pred) - tp  # Predictions that didn't match any gold\n",
    "    fn = len(gold) - tp  # Gold entities that weren't matched\n",
    "\n",
    "    # Calculate F1 score for this example\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    return f1, (tp, fp, fn)\n",
    "\n",
    "def calculate_micro_f1_list(counts):\n",
    "    # Sum up all true positives, false positives, and false negatives\n",
    "    tp = sum(count[0] for count in counts)\n",
    "    fp = sum(count[1] for count in counts)\n",
    "    fn = sum(count[2] for count in counts)\n",
    "\n",
    "    # Calculate micro-averaged precision and recall\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "\n",
    "    # Calculate micro F1\n",
    "    micro_f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    return micro_f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing results from rahmad directory:\n",
      "--------------------------------------------------\n",
      "llama-0shot-concept_replacement_100.csv\n",
      "concept_replacement_100.csv\n",
      "\n",
      "Results from llama-0shot-concept_replacement_100.csv:\n",
      "==================================================\n",
      "claude-0shot-capitalization_100_new.csv\n",
      "capitalization\n",
      "\n",
      "Results from claude-0shot-capitalization_100_new.csv:\n",
      "==================================================\n",
      "original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:00, 24184.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:00, 30856.35it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done calculating f1 scores\n",
      "\n",
      "claude - CAPITALIZATION Modification:\n",
      "Original Mean F1: 60.677\n",
      "Modified Mean F1: 56.444\n",
      "Weighted Delta: -3.774\n",
      "P-value: 0.0792\n",
      "Significance: .\n",
      "gpt4o-0shot-sentiment_100_new.csv\n",
      "sentiment\n",
      "\n",
      "Results from gpt4o-0shot-sentiment_100_new.csv:\n",
      "==================================================\n",
      "original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "123it [00:00, 36039.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "123it [00:00, 38342.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done calculating f1 scores\n",
      "\n",
      "gpt4o - SENTIMENT Modification:\n",
      "Original Mean F1: 57.025\n",
      "Modified Mean F1: 57.081\n",
      "Weighted Delta: 0.049\n",
      "P-value: 0.9547\n",
      "Significance: ns\n",
      "llama-0shot-negation_100.csv\n",
      "negation_100.csv\n",
      "\n",
      "Results from llama-0shot-negation_100.csv:\n",
      "==================================================\n",
      "llama-0shot-temporal_bias_100.csv\n",
      "temporal_bias_100.csv\n",
      "\n",
      "Results from llama-0shot-temporal_bias_100.csv:\n",
      "==================================================\n",
      "llama-0shot-discourse_100_compare.csv\n",
      "discourse_100_compare.csv\n",
      "\n",
      "Results from llama-0shot-discourse_100_compare.csv:\n",
      "==================================================\n",
      "gpt4o-0shot-coordinating_conjunction_100.csv\n",
      "coordinating_conjunction_100.csv\n",
      "\n",
      "Results from gpt4o-0shot-coordinating_conjunction_100.csv:\n",
      "==================================================\n",
      "llama-0shot-sentiment_100_compare.csv\n",
      "sentiment_100_compare.csv\n",
      "\n",
      "Results from llama-0shot-sentiment_100_compare.csv:\n",
      "==================================================\n",
      "llama-0shot-grammatical_role_100.csv\n",
      "grammatical_role_100.csv\n",
      "\n",
      "Results from llama-0shot-grammatical_role_100.csv:\n",
      "==================================================\n",
      "gpt4o-0shot-grammatical_role_100.csv\n",
      "grammatical_role_100.csv\n",
      "\n",
      "Results from gpt4o-0shot-grammatical_role_100.csv:\n",
      "==================================================\n",
      "claude-0shot-compound_word_100_compare.csv\n",
      "compound_word_100_compare.csv\n",
      "\n",
      "Results from claude-0shot-compound_word_100_compare.csv:\n",
      "==================================================\n",
      "llama-0shot-discourse_100.csv\n",
      "discourse_100.csv\n",
      "\n",
      "Results from llama-0shot-discourse_100.csv:\n",
      "==================================================\n",
      "gpt4o-0shot-discourse_100_compare.csv\n",
      "discourse_100_compare.csv\n",
      "\n",
      "Results from gpt4o-0shot-discourse_100_compare.csv:\n",
      "==================================================\n",
      "gpt4o-0shot-sentiment_100_compare.csv\n",
      "sentiment_100_compare.csv\n",
      "\n",
      "Results from gpt4o-0shot-sentiment_100_compare.csv:\n",
      "==================================================\n",
      "llama-0shot-derivation_100_compare.csv\n",
      "derivation_100_compare.csv\n",
      "\n",
      "Results from llama-0shot-derivation_100_compare.csv:\n",
      "==================================================\n",
      "claude-DP.csv\n",
      "llama-0shot-coordinating_conjunction_100.csv\n",
      "coordinating_conjunction_100.csv\n",
      "\n",
      "Results from llama-0shot-coordinating_conjunction_100.csv:\n",
      "==================================================\n",
      "gpt4o-0shot-ner.csv\n",
      "claude-0shot-punctuation_100.csv\n",
      "punctuation_100.csv\n",
      "\n",
      "Results from claude-0shot-punctuation_100.csv:\n",
      "==================================================\n",
      "llama-0shot-length_bias_100_compare.csv\n",
      "length_bias_100_compare.csv\n",
      "\n",
      "Results from llama-0shot-length_bias_100_compare.csv:\n",
      "==================================================\n",
      "claude-0shot-sentiment_100.csv\n",
      "sentiment_100.csv\n",
      "\n",
      "Results from claude-0shot-sentiment_100.csv:\n",
      "==================================================\n",
      "claude-0shot-geographical_bias_100.csv\n",
      "geographical_bias_100.csv\n",
      "\n",
      "Results from claude-0shot-geographical_bias_100.csv:\n",
      "==================================================\n",
      "claude-0shot-length_bias_100.csv\n",
      "length_bias_100.csv\n",
      "\n",
      "Results from claude-0shot-length_bias_100.csv:\n",
      "==================================================\n",
      "gpt4o-0shot-concept_replacement_100_compare.csv\n",
      "concept_replacement_100_compare.csv\n",
      "\n",
      "Results from gpt4o-0shot-concept_replacement_100_compare.csv:\n",
      "==================================================\n",
      "claude-0shot-grammatical_role_100.csv\n",
      "grammatical_role_100.csv\n",
      "\n",
      "Results from claude-0shot-grammatical_role_100.csv:\n",
      "==================================================\n",
      "gpt4o-0shot-punctuation_100_new.csv\n",
      "punctuation\n",
      "\n",
      "Results from gpt4o-0shot-punctuation_100_new.csv:\n",
      "==================================================\n",
      "original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:00, 42629.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:00, 44131.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done calculating f1 scores\n",
      "\n",
      "gpt4o - PUNCTUATION Modification:\n",
      "Original Mean F1: 50.585\n",
      "Modified Mean F1: 47.106\n",
      "Weighted Delta: -2.965\n",
      "P-value: 0.0978\n",
      "Significance: .\n",
      "claude-0shot-compound_word_100.csv\n",
      "compound_word_100.csv\n",
      "\n",
      "Results from claude-0shot-compound_word_100.csv:\n",
      "==================================================\n",
      "llama-0shot-geographical_bias_100.csv\n",
      "geographical_bias_100.csv\n",
      "\n",
      "Results from llama-0shot-geographical_bias_100.csv:\n",
      "==================================================\n",
      "gpt4o-0shot-coordinating_conjunction_100_compare.csv\n",
      "coordinating_conjunction_100_compare.csv\n",
      "\n",
      "Results from gpt4o-0shot-coordinating_conjunction_100_compare.csv:\n",
      "==================================================\n",
      "llama-0shot-singlish_100_new.csv\n",
      "singlish\n",
      "\n",
      "Results from llama-0shot-singlish_100_new.csv:\n",
      "==================================================\n",
      "original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "96it [00:00, 43534.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "96it [00:00, 54700.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done calculating f1 scores\n",
      "\n",
      "llama - SINGLISH Modification:\n",
      "Original Mean F1: 59.550\n",
      "Modified Mean F1: 56.677\n",
      "Weighted Delta: -2.550\n",
      "P-value: 0.0927\n",
      "Significance: .\n",
      "gpt4o-0shot-geographical_bias_100.csv\n",
      "geographical_bias_100.csv\n",
      "\n",
      "Results from gpt4o-0shot-geographical_bias_100.csv:\n",
      "==================================================\n",
      "gpt4o-0shot-capitalization_100.csv\n",
      "capitalization_100.csv\n",
      "\n",
      "Results from gpt4o-0shot-capitalization_100.csv:\n",
      "==================================================\n",
      "claude-0shot-capitalization_100.csv\n",
      "capitalization_100.csv\n",
      "\n",
      "Results from claude-0shot-capitalization_100.csv:\n",
      "==================================================\n",
      "llama-0shot-punctuation_100.csv\n",
      "punctuation_100.csv\n",
      "\n",
      "Results from llama-0shot-punctuation_100.csv:\n",
      "==================================================\n",
      "gpt4o-0shot-derivation_100_new.csv\n",
      "derivation\n",
      "\n",
      "Results from gpt4o-0shot-derivation_100_new.csv:\n",
      "==================================================\n",
      "original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "69it [00:00, 41196.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "69it [00:00, 40836.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done calculating f1 scores\n",
      "\n",
      "gpt4o - DERIVATION Modification:\n",
      "Original Mean F1: 55.509\n",
      "Modified Mean F1: 55.502\n",
      "Weighted Delta: -0.006\n",
      "P-value: 0.9396\n",
      "Significance: ns\n",
      "claude-0shot-derivation_100_new.csv\n",
      "derivation\n",
      "\n",
      "Results from claude-0shot-derivation_100_new.csv:\n",
      "==================================================\n",
      "original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "69it [00:00, 42938.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "69it [00:00, 43532.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done calculating f1 scores\n",
      "\n",
      "claude - DERIVATION Modification:\n",
      "Original Mean F1: 56.304\n",
      "Modified Mean F1: 54.351\n",
      "Weighted Delta: -1.709\n",
      "P-value: 0.0679\n",
      "Significance: .\n",
      "claude-0shot-discourse_100_compare.csv\n",
      "discourse_100_compare.csv\n",
      "\n",
      "Results from claude-0shot-discourse_100_compare.csv:\n",
      "==================================================\n",
      "llama-0shot-compound_word_100_compare.csv\n",
      "compound_word_100_compare.csv\n",
      "\n",
      "Results from llama-0shot-compound_word_100_compare.csv:\n",
      "==================================================\n",
      "claude-0shot-sentiment_100_compare.csv\n",
      "sentiment_100_compare.csv\n",
      "\n",
      "Results from claude-0shot-sentiment_100_compare.csv:\n",
      "==================================================\n",
      "gpt4o-0shot-casual_100_compare.csv\n",
      "casual_100_compare.csv\n",
      "\n",
      "Results from gpt4o-0shot-casual_100_compare.csv:\n",
      "==================================================\n",
      "claude-0shot-punctuation_100_new.csv\n",
      "punctuation\n",
      "\n",
      "Results from claude-0shot-punctuation_100_new.csv:\n",
      "==================================================\n",
      "original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:00, 45704.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:00, 50430.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done calculating f1 scores\n",
      "\n",
      "claude - PUNCTUATION Modification:\n",
      "Original Mean F1: 45.389\n",
      "Modified Mean F1: 37.331\n",
      "Weighted Delta: -6.676\n",
      "P-value: 0.0007\n",
      "Significance: **\n",
      "llama-0shot-length_bias_100.csv\n",
      "length_bias_100.csv\n",
      "\n",
      "Results from llama-0shot-length_bias_100.csv:\n",
      "==================================================\n",
      "claude-0shot-casual_100_compare.csv\n",
      "casual_100_compare.csv\n",
      "\n",
      "Results from claude-0shot-casual_100_compare.csv:\n",
      "==================================================\n",
      "gpt4o-0shot-discourse_100_new.csv\n",
      "discourse\n",
      "\n",
      "Results from gpt4o-0shot-discourse_100_new.csv:\n",
      "==================================================\n",
      "original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "72it [00:00, 40313.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "72it [00:00, 33821.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done calculating f1 scores\n",
      "\n",
      "gpt4o - DISCOURSE Modification:\n",
      "Original Mean F1: 54.912\n",
      "Modified Mean F1: 55.007\n",
      "Weighted Delta: 0.083\n",
      "P-value: 0.8125\n",
      "Significance: ns\n",
      "llama-0shot-capitalization_100.csv\n",
      "capitalization_100.csv\n",
      "\n",
      "Results from llama-0shot-capitalization_100.csv:\n",
      "==================================================\n",
      "claude-0shot-temporal_bias_100_new.csv\n",
      "temporal_bias\n",
      "\n",
      "Results from claude-0shot-temporal_bias_100_new.csv:\n",
      "==================================================\n",
      "original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "91it [00:00, 47668.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "91it [00:00, 44866.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done calculating f1 scores\n",
      "\n",
      "claude - TEMPORAL_BIAS Modification:\n",
      "Original Mean F1: 53.267\n",
      "Modified Mean F1: 48.995\n",
      "Weighted Delta: -3.688\n",
      "P-value: 0.0180\n",
      "Significance: *\n",
      "claude-0shot-negation_100_new.csv\n",
      "negation\n",
      "\n",
      "Results from claude-0shot-negation_100_new.csv:\n",
      "==================================================\n",
      "original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "110it [00:00, 44754.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "110it [00:00, 42950.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done calculating f1 scores\n",
      "\n",
      "claude - NEGATION Modification:\n",
      "Original Mean F1: 46.497\n",
      "Modified Mean F1: 46.388\n",
      "Weighted Delta: -0.091\n",
      "P-value: 0.6768\n",
      "Significance: ns\n",
      "gpt4o-0shot-dialectal_100_new.csv\n",
      "dialectal\n",
      "\n",
      "Results from gpt4o-0shot-dialectal_100_new.csv:\n",
      "==================================================\n",
      "original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "99it [00:00, 47596.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "99it [00:00, 43380.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done calculating f1 scores\n",
      "\n",
      "gpt4o - DIALECTAL Modification:\n",
      "Original Mean F1: 51.646\n",
      "Modified Mean F1: 54.935\n",
      "Weighted Delta: 2.817\n",
      "P-value: 0.2043\n",
      "Significance: ns\n",
      "llama-0shot-coordinating_conjunction_100_new.csv\n",
      "coordinating_conjunction\n",
      "\n",
      "Results from llama-0shot-coordinating_conjunction_100_new.csv:\n",
      "==================================================\n",
      "original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "61it [00:00, 33223.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "61it [00:00, 30455.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done calculating f1 scores\n",
      "\n",
      "llama - COORDINATING_CONJUNCTION Modification:\n",
      "Original Mean F1: 73.382\n",
      "Modified Mean F1: 73.627\n",
      "Weighted Delta: 0.229\n",
      "P-value: 0.6528\n",
      "Significance: ns\n",
      "llama-0shot-casual_100_compare.csv\n",
      "casual_100_compare.csv\n",
      "\n",
      "Results from llama-0shot-casual_100_compare.csv:\n",
      "==================================================\n",
      "claude-0shot-active_to_passive_100.csv\n",
      "active_to_passive_100.csv\n",
      "\n",
      "Results from claude-0shot-active_to_passive_100.csv:\n",
      "==================================================\n",
      "llama-0shot-derivation_100_new.csv\n",
      "derivation\n",
      "\n",
      "Results from llama-0shot-derivation_100_new.csv:\n",
      "==================================================\n",
      "original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "69it [00:00, 43756.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "69it [00:00, 39204.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done calculating f1 scores\n",
      "\n",
      "llama - DERIVATION Modification:\n",
      "Original Mean F1: 60.106\n",
      "Modified Mean F1: 58.870\n",
      "Weighted Delta: -1.099\n",
      "P-value: 0.1441\n",
      "Significance: ns\n",
      "gpt4o-0shot-discourse_100.csv\n",
      "discourse_100.csv\n",
      "\n",
      "Results from gpt4o-0shot-discourse_100.csv:\n",
      "==================================================\n",
      "gpt4o-0shot-typo_bias_100_new.csv\n",
      "typo_bias\n",
      "\n",
      "Results from gpt4o-0shot-typo_bias_100_new.csv:\n",
      "==================================================\n",
      "original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:00, 44897.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:00, 41634.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done calculating f1 scores\n",
      "\n",
      "gpt4o - TYPO_BIAS Modification:\n",
      "Original Mean F1: 49.703\n",
      "Modified Mean F1: 50.250\n",
      "Weighted Delta: 0.464\n",
      "P-value: 0.7088\n",
      "Significance: ns\n",
      "claude-0shot-temporal_bias_100.csv\n",
      "temporal_bias_100.csv\n",
      "\n",
      "Results from claude-0shot-temporal_bias_100.csv:\n",
      "==================================================\n",
      "claude-0shot-typo_bias_100.csv\n",
      "typo_bias_100.csv\n",
      "\n",
      "Results from claude-0shot-typo_bias_100.csv:\n",
      "==================================================\n",
      "claude-0shot-coordinating_conjunction_100.csv\n",
      "coordinating_conjunction_100.csv\n",
      "\n",
      "Results from claude-0shot-coordinating_conjunction_100.csv:\n",
      "==================================================\n",
      "gpt4o-0shot-compound_word_100_compare.csv\n",
      "compound_word_100_compare.csv\n",
      "\n",
      "Results from gpt4o-0shot-compound_word_100_compare.csv:\n",
      "==================================================\n",
      "llama-0shot-punctuation_100_new.csv\n",
      "punctuation\n",
      "\n",
      "Results from llama-0shot-punctuation_100_new.csv:\n",
      "==================================================\n",
      "original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:00, 43500.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:00, 41577.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done calculating f1 scores\n",
      "\n",
      "llama - PUNCTUATION Modification:\n",
      "Original Mean F1: 53.950\n",
      "Modified Mean F1: 47.381\n",
      "Weighted Delta: -5.689\n",
      "P-value: 0.0030\n",
      "Significance: **\n",
      "claude-0shot-active_to_passive_100_compare.csv\n",
      "active_to_passive_100_compare.csv\n",
      "\n",
      "Results from claude-0shot-active_to_passive_100_compare.csv:\n",
      "==================================================\n",
      "llama-0shot-singlish_100.csv\n",
      "singlish_100.csv\n",
      "\n",
      "Results from llama-0shot-singlish_100.csv:\n",
      "==================================================\n",
      "claude-0shot-dialectal_100.csv\n",
      "dialectal_100.csv\n",
      "\n",
      "Results from claude-0shot-dialectal_100.csv:\n",
      "==================================================\n",
      "claude-0shot-capitalization_100_compare.csv\n",
      "capitalization_100_compare.csv\n",
      "\n",
      "Results from claude-0shot-capitalization_100_compare.csv:\n",
      "==================================================\n",
      "claude-0shot-concept_replacement_100_new.csv\n",
      "concept_replacement\n",
      "\n",
      "Results from claude-0shot-concept_replacement_100_new.csv:\n",
      "==================================================\n",
      "original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "85it [00:00, 46067.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "85it [00:00, 39758.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done calculating f1 scores\n",
      "\n",
      "claude - CONCEPT_REPLACEMENT Modification:\n",
      "Original Mean F1: 49.185\n",
      "Modified Mean F1: 49.104\n",
      "Weighted Delta: -0.069\n",
      "P-value: 0.9161\n",
      "Significance: ns\n",
      "gpt4o-0shot-derivation_100_compare.csv\n",
      "derivation_100_compare.csv\n",
      "\n",
      "Results from gpt4o-0shot-derivation_100_compare.csv:\n",
      "==================================================\n",
      "claude-0shot-length_bias_100_compare.csv\n",
      "length_bias_100_compare.csv\n",
      "\n",
      "Results from claude-0shot-length_bias_100_compare.csv:\n",
      "==================================================\n",
      "gpt4o-0shot-singlish_100_new.csv\n",
      "singlish\n",
      "\n",
      "Results from gpt4o-0shot-singlish_100_new.csv:\n",
      "==================================================\n",
      "original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "96it [00:00, 39902.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "96it [00:00, 53036.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done calculating f1 scores\n",
      "\n",
      "gpt4o - SINGLISH Modification:\n",
      "Original Mean F1: 54.252\n",
      "Modified Mean F1: 53.290\n",
      "Weighted Delta: -0.835\n",
      "P-value: 0.8925\n",
      "Significance: ns\n",
      "llama-0shot-active_to_passive_100.csv\n",
      "active_to_passive_100.csv\n",
      "\n",
      "Results from llama-0shot-active_to_passive_100.csv:\n",
      "==================================================\n",
      "gpt4o-0shot-temporal_bias_100_new.csv\n",
      "temporal_bias\n",
      "\n",
      "Results from gpt4o-0shot-temporal_bias_100_new.csv:\n",
      "==================================================\n",
      "original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "91it [00:00, 42823.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "91it [00:00, 40321.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done calculating f1 scores\n",
      "\n",
      "gpt4o - TEMPORAL_BIAS Modification:\n",
      "Original Mean F1: 57.036\n",
      "Modified Mean F1: 53.207\n",
      "Weighted Delta: -3.362\n",
      "P-value: 0.0275\n",
      "Significance: *\n",
      "gpt4o-0shot-active_to_passive_100_compare.csv\n",
      "active_to_passive_100_compare.csv\n",
      "\n",
      "Results from gpt4o-0shot-active_to_passive_100_compare.csv:\n",
      "==================================================\n",
      "claude-0shot-concept_replacement_100.csv\n",
      "concept_replacement_100.csv\n",
      "\n",
      "Results from claude-0shot-concept_replacement_100.csv:\n",
      "==================================================\n",
      "gpt4o-0shot-capitalization_100_compare.csv\n",
      "capitalization_100_compare.csv\n",
      "\n",
      "Results from gpt4o-0shot-capitalization_100_compare.csv:\n",
      "==================================================\n",
      "llama-0shot-compound_word_100.csv\n",
      "compound_word_100.csv\n",
      "\n",
      "Results from llama-0shot-compound_word_100.csv:\n",
      "==================================================\n",
      "gpt4o-0shot-active_to_passive_100.csv\n",
      "active_to_passive_100.csv\n",
      "\n",
      "Results from gpt4o-0shot-active_to_passive_100.csv:\n",
      "==================================================\n",
      "llama-0shot-temporal_bias_100_new.csv\n",
      "temporal_bias\n",
      "\n",
      "Results from llama-0shot-temporal_bias_100_new.csv:\n",
      "==================================================\n",
      "original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "91it [00:00, 36402.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "91it [00:00, 37099.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done calculating f1 scores\n",
      "\n",
      "llama - TEMPORAL_BIAS Modification:\n",
      "Original Mean F1: 57.115\n",
      "Modified Mean F1: 55.802\n",
      "Weighted Delta: -1.154\n",
      "P-value: 0.1730\n",
      "Significance: ns\n",
      "llama-0shot-geographical_bias_100_compare.csv\n",
      "geographical_bias_100_compare.csv\n",
      "\n",
      "Results from llama-0shot-geographical_bias_100_compare.csv:\n",
      "==================================================\n",
      "gpt4o-0shot-punctuation_100_compare.csv\n",
      "punctuation_100_compare.csv\n",
      "\n",
      "Results from gpt4o-0shot-punctuation_100_compare.csv:\n",
      "==================================================\n",
      "claude-0shot-grammatical_role_100_new.csv\n",
      "grammatical_role\n",
      "\n",
      "Results from claude-0shot-grammatical_role_100_new.csv:\n",
      "==================================================\n",
      "original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "83it [00:00, 32395.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "83it [00:00, 38125.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done calculating f1 scores\n",
      "\n",
      "claude - GRAMMATICAL_ROLE Modification:\n",
      "Original Mean F1: 60.329\n",
      "Modified Mean F1: 57.999\n",
      "Weighted Delta: -2.075\n",
      "P-value: 0.0853\n",
      "Significance: .\n",
      "llama-0shot-ner.csv\n",
      "llama-0shot-capitalization_100_compare.csv\n",
      "capitalization_100_compare.csv\n",
      "\n",
      "Results from llama-0shot-capitalization_100_compare.csv:\n",
      "==================================================\n",
      "gpt4o-0shot-derivation_100.csv\n",
      "derivation_100.csv\n",
      "\n",
      "Results from gpt4o-0shot-derivation_100.csv:\n",
      "==================================================\n",
      "llama-0shot-dialectal_100.csv\n",
      "dialectal_100.csv\n",
      "\n",
      "Results from llama-0shot-dialectal_100.csv:\n",
      "==================================================\n",
      "claude-0shot-length_bias_100_new.csv\n",
      "length_bias\n",
      "\n",
      "Results from claude-0shot-length_bias_100_new.csv:\n",
      "==================================================\n",
      "original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "92it [00:00, 40764.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "92it [00:00, 45343.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done calculating f1 scores\n",
      "\n",
      "claude - LENGTH_BIAS Modification:\n",
      "Original Mean F1: 48.274\n",
      "Modified Mean F1: 43.352\n",
      "Weighted Delta: -4.144\n",
      "P-value: 0.2576\n",
      "Significance: ns\n",
      "llama-0shot-sentiment_100_new.csv\n",
      "sentiment\n",
      "\n",
      "Results from llama-0shot-sentiment_100_new.csv:\n",
      "==================================================\n",
      "original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "123it [00:00, 37775.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "123it [00:00, 42210.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done calculating f1 scores\n",
      "\n",
      "llama - SENTIMENT Modification:\n",
      "Original Mean F1: 59.942\n",
      "Modified Mean F1: 58.206\n",
      "Weighted Delta: -1.543\n",
      "P-value: 0.1921\n",
      "Significance: ns\n",
      "llama-0shot-concept_replacement_100_new.csv\n",
      "concept_replacement\n",
      "\n",
      "Results from llama-0shot-concept_replacement_100_new.csv:\n",
      "==================================================\n",
      "original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "85it [00:00, 35417.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "85it [00:00, 29048.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done calculating f1 scores\n",
      "\n",
      "llama - CONCEPT_REPLACEMENT Modification:\n",
      "Original Mean F1: 56.100\n",
      "Modified Mean F1: 53.173\n",
      "Weighted Delta: -2.559\n",
      "P-value: 0.1257\n",
      "Significance: ns\n",
      "llama-0shot-casual_100_new.csv\n",
      "casual\n",
      "\n",
      "Results from llama-0shot-casual_100_new.csv:\n",
      "==================================================\n",
      "original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98it [00:00, 37634.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98it [00:00, 37562.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done calculating f1 scores\n",
      "\n",
      "llama - CASUAL Modification:\n",
      "Original Mean F1: 57.339\n",
      "Modified Mean F1: 54.858\n",
      "Weighted Delta: -2.181\n",
      "P-value: 0.3201\n",
      "Significance: ns\n",
      "claude-0shot-typo_bias_100_new.csv\n",
      "typo_bias\n",
      "\n",
      "Results from claude-0shot-typo_bias_100_new.csv:\n",
      "==================================================\n",
      "original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:00, 46402.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:00, 40563.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done calculating f1 scores\n",
      "\n",
      "claude - TYPO_BIAS Modification:\n",
      "Original Mean F1: 46.693\n",
      "Modified Mean F1: 45.667\n",
      "Weighted Delta: -0.857\n",
      "P-value: 0.2367\n",
      "Significance: ns\n",
      "llama-0shot-concept_replacement_100_compare.csv\n",
      "concept_replacement_100_compare.csv\n",
      "\n",
      "Results from llama-0shot-concept_replacement_100_compare.csv:\n",
      "==================================================\n",
      "llama-0shot-typo_bias_100.csv\n",
      "typo_bias_100.csv\n",
      "\n",
      "Results from llama-0shot-typo_bias_100.csv:\n",
      "==================================================\n",
      "gpt4o-0shot-temporal_bias_100.csv\n",
      "temporal_bias_100.csv\n",
      "\n",
      "Results from gpt4o-0shot-temporal_bias_100.csv:\n",
      "==================================================\n",
      "llama-0shot-punctuation_100_compare.csv\n",
      "punctuation_100_compare.csv\n",
      "\n",
      "Results from llama-0shot-punctuation_100_compare.csv:\n",
      "==================================================\n",
      "claude-0shot-dialectal_100_new.csv\n",
      "dialectal\n",
      "\n",
      "Results from claude-0shot-dialectal_100_new.csv:\n",
      "==================================================\n",
      "original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "99it [00:00, 10144.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "99it [00:00, 10297.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done calculating f1 scores\n",
      "\n",
      "claude - DIALECTAL Modification:\n",
      "Original Mean F1: 49.416\n",
      "Modified Mean F1: 47.940\n",
      "Weighted Delta: -1.250\n",
      "P-value: 0.5723\n",
      "Significance: ns\n",
      "claude-0shot-temporal_bias_100_compare.csv\n",
      "temporal_bias_100_compare.csv\n",
      "\n",
      "Results from claude-0shot-temporal_bias_100_compare.csv:\n",
      "==================================================\n",
      "gpt4o-0shot-dialectal_100_compare.csv\n",
      "dialectal_100_compare.csv\n",
      "\n",
      "Results from gpt4o-0shot-dialectal_100_compare.csv:\n",
      "==================================================\n",
      "claude-0shot-discourse_100_new.csv\n",
      "discourse\n",
      "\n",
      "Results from claude-0shot-discourse_100_new.csv:\n",
      "==================================================\n",
      "original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "72it [00:00, 11189.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "72it [00:00, 38421.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done calculating f1 scores\n",
      "\n",
      "claude - DISCOURSE Modification:\n",
      "Original Mean F1: 53.172\n",
      "Modified Mean F1: 52.152\n",
      "Weighted Delta: -0.880\n",
      "P-value: 0.2785\n",
      "Significance: ns\n",
      "claude-0shot-compound_word_100_new.csv\n",
      "compound_word\n",
      "\n",
      "Results from claude-0shot-compound_word_100_new.csv:\n",
      "==================================================\n",
      "original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "86it [00:00, 35802.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "86it [00:00, 24048.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done calculating f1 scores\n",
      "\n",
      "claude - COMPOUND_WORD Modification:\n",
      "Original Mean F1: 51.450\n",
      "Modified Mean F1: 49.718\n",
      "Weighted Delta: -1.483\n",
      "P-value: 0.1088\n",
      "Significance: ns\n",
      "gpt4o-0shot-length_bias_100_new.csv\n",
      "length_bias\n",
      "\n",
      "Results from gpt4o-0shot-length_bias_100_new.csv:\n",
      "==================================================\n",
      "original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "92it [00:00, 19246.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "92it [00:00, 31176.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done calculating f1 scores\n",
      "\n",
      "gpt4o - LENGTH_BIAS Modification:\n",
      "Original Mean F1: 50.422\n",
      "Modified Mean F1: 55.338\n",
      "Weighted Delta: 4.185\n",
      "P-value: 0.0750\n",
      "Significance: .\n",
      "llama-0shot-dialectal_100_compare.csv\n",
      "dialectal_100_compare.csv\n",
      "\n",
      "Results from llama-0shot-dialectal_100_compare.csv:\n",
      "==================================================\n",
      "gpt4o-0shot-grammatical_role_100_new.csv\n",
      "grammatical_role\n",
      "\n",
      "Results from gpt4o-0shot-grammatical_role_100_new.csv:\n",
      "==================================================\n",
      "original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "83it [00:00, 27686.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "83it [00:00, 32845.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done calculating f1 scores\n",
      "\n",
      "gpt4o - GRAMMATICAL_ROLE Modification:\n",
      "Original Mean F1: 59.695\n",
      "Modified Mean F1: 61.743\n",
      "Weighted Delta: 1.818\n",
      "P-value: 0.1935\n",
      "Significance: ns\n",
      "claude-0shot-active_to_passive_100_new.csv\n",
      "active_to_passive\n",
      "\n",
      "Results from claude-0shot-active_to_passive_100_new.csv:\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "81it [00:00, 48478.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "81it [00:00, 47396.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done calculating f1 scores\n",
      "\n",
      "claude - ACTIVE_TO_PASSIVE Modification:\n",
      "Original Mean F1: 51.052\n",
      "Modified Mean F1: 52.909\n",
      "Weighted Delta: 1.586\n",
      "P-value: 0.3326\n",
      "Significance: ns\n",
      "llama-0shot-geographical_bias_100_new.csv\n",
      "geographical_bias\n",
      "\n",
      "Results from llama-0shot-geographical_bias_100_new.csv:\n",
      "==================================================\n",
      "original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "102it [00:00, 29623.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "102it [00:00, 34390.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done calculating f1 scores\n",
      "\n",
      "llama - GEOGRAPHICAL_BIAS Modification:\n",
      "Original Mean F1: 62.562\n",
      "Modified Mean F1: 63.299\n",
      "Weighted Delta: 0.662\n",
      "P-value: 0.8502\n",
      "Significance: ns\n",
      "llama-0shot-grammatical_role_100_new.csv\n",
      "grammatical_role\n",
      "\n",
      "Results from llama-0shot-grammatical_role_100_new.csv:\n",
      "==================================================\n",
      "original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "83it [00:00, 30029.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "83it [00:00, 38655.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done calculating f1 scores\n",
      "\n",
      "llama - GRAMMATICAL_ROLE Modification:\n",
      "Original Mean F1: 64.082\n",
      "Modified Mean F1: 63.701\n",
      "Weighted Delta: -0.344\n",
      "P-value: 0.7554\n",
      "Significance: ns\n",
      "llama-0shot-temporal_bias_100_compare.csv\n",
      "temporal_bias_100_compare.csv\n",
      "\n",
      "Results from llama-0shot-temporal_bias_100_compare.csv:\n",
      "==================================================\n",
      "llama-0shot-coordinating_conjunction_100_compare.csv\n",
      "coordinating_conjunction_100_compare.csv\n",
      "\n",
      "Results from llama-0shot-coordinating_conjunction_100_compare.csv:\n",
      "==================================================\n",
      "claude-0shot-grammatical_role_100_compare.csv\n",
      "grammatical_role_100_compare.csv\n",
      "\n",
      "Results from claude-0shot-grammatical_role_100_compare.csv:\n",
      "==================================================\n",
      "claude-0shot-dialectal_100_compare.csv\n",
      "dialectal_100_compare.csv\n",
      "\n",
      "Results from claude-0shot-dialectal_100_compare.csv:\n",
      "==================================================\n",
      "gpt4o-0shot-compound_word_100_new.csv\n",
      "compound_word\n",
      "\n",
      "Results from gpt4o-0shot-compound_word_100_new.csv:\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "86it [00:00, 44466.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "86it [00:00, 42788.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done calculating f1 scores\n",
      "\n",
      "gpt4o - COMPOUND_WORD Modification:\n",
      "Original Mean F1: 54.489\n",
      "Modified Mean F1: 52.890\n",
      "Weighted Delta: -1.388\n",
      "P-value: 0.5281\n",
      "Significance: ns\n",
      "gpt4o-DP.csv\n",
      "claude-0shot-geographical_bias_100_new.csv\n",
      "geographical_bias\n",
      "\n",
      "Results from claude-0shot-geographical_bias_100_new.csv:\n",
      "==================================================\n",
      "original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "102it [00:00, 31706.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "102it [00:00, 39675.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done calculating f1 scores\n",
      "\n",
      "claude - GEOGRAPHICAL_BIAS Modification:\n",
      "Original Mean F1: 65.997\n",
      "Modified Mean F1: 61.513\n",
      "Weighted Delta: -4.079\n",
      "P-value: 0.2847\n",
      "Significance: ns\n",
      "claude-0shot-singlish_100.csv\n",
      "singlish_100.csv\n",
      "\n",
      "Results from claude-0shot-singlish_100.csv:\n",
      "==================================================\n",
      "llama-0shot-active_to_passive_100_new.csv\n",
      "active_to_passive\n",
      "\n",
      "Results from llama-0shot-active_to_passive_100_new.csv:\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "81it [00:00, 35792.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "81it [00:00, 42994.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done calculating f1 scores\n",
      "\n",
      "llama - ACTIVE_TO_PASSIVE Modification:\n",
      "Original Mean F1: 54.761\n",
      "Modified Mean F1: 56.830\n",
      "Weighted Delta: 1.799\n",
      "P-value: 0.2198\n",
      "Significance: ns\n",
      "llama-0shot-compound_word_100_new.csv\n",
      "compound_word\n",
      "\n",
      "Results from llama-0shot-compound_word_100_new.csv:\n",
      "==================================================\n",
      "original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "86it [00:00, 43438.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "86it [00:00, 41513.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done calculating f1 scores\n",
      "\n",
      "llama - COMPOUND_WORD Modification:\n",
      "Original Mean F1: 58.061\n",
      "Modified Mean F1: 57.092\n",
      "Weighted Delta: -0.855\n",
      "P-value: 0.1797\n",
      "Significance: ns\n",
      "claude-0shot-coordinating_conjunction_100_compare.csv\n",
      "coordinating_conjunction_100_compare.csv\n",
      "\n",
      "Results from claude-0shot-coordinating_conjunction_100_compare.csv:\n",
      "==================================================\n",
      "gpt4o-0shot-capitalization_100_new.csv\n",
      "capitalization\n",
      "\n",
      "Results from gpt4o-0shot-capitalization_100_new.csv:\n",
      "==================================================\n",
      "original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:00, 47820.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:00, 46065.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done calculating f1 scores\n",
      "\n",
      "gpt4o - CAPITALIZATION Modification:\n",
      "Original Mean F1: 55.648\n",
      "Modified Mean F1: 55.061\n",
      "Weighted Delta: -0.512\n",
      "P-value: 0.7260\n",
      "Significance: ns\n",
      "gpt4o-0shot-sentiment_100.csv\n",
      "sentiment_100.csv\n",
      "\n",
      "Results from gpt4o-0shot-sentiment_100.csv:\n",
      "==================================================\n",
      "claude-0shot-derivation_100.csv\n",
      "derivation_100.csv\n",
      "\n",
      "Results from claude-0shot-derivation_100.csv:\n",
      "==================================================\n",
      "llama-0shot-negation_100_new.csv\n",
      "negation\n",
      "\n",
      "Results from llama-0shot-negation_100_new.csv:\n",
      "==================================================\n",
      "original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "110it [00:00, 32079.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "110it [00:00, 36457.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done calculating f1 scores\n",
      "\n",
      "llama - NEGATION Modification:\n",
      "Original Mean F1: 50.035\n",
      "Modified Mean F1: 49.231\n",
      "Weighted Delta: -0.683\n",
      "P-value: 0.6871\n",
      "Significance: ns\n",
      "gpt4o-0shot-punctuation_100.csv\n",
      "punctuation_100.csv\n",
      "\n",
      "Results from gpt4o-0shot-punctuation_100.csv:\n",
      "==================================================\n",
      "gpt4o-0shot-length_bias_100.csv\n",
      "length_bias_100.csv\n",
      "\n",
      "Results from gpt4o-0shot-length_bias_100.csv:\n",
      "==================================================\n",
      "llama-0shot-active_to_passive_100_compare.csv\n",
      "active_to_passive_100_compare.csv\n",
      "\n",
      "Results from llama-0shot-active_to_passive_100_compare.csv:\n",
      "==================================================\n",
      "gpt4o-0shot-typo_bias_100_compare.csv\n",
      "typo_bias_100_compare.csv\n",
      "\n",
      "Results from gpt4o-0shot-typo_bias_100_compare.csv:\n",
      "==================================================\n",
      "gpt4o-0shot-dialectal_100.csv\n",
      "dialectal_100.csv\n",
      "\n",
      "Results from gpt4o-0shot-dialectal_100.csv:\n",
      "==================================================\n",
      "gpt4o-0shot-geographical_bias_100_new.csv\n",
      "geographical_bias\n",
      "\n",
      "Results from gpt4o-0shot-geographical_bias_100_new.csv:\n",
      "==================================================\n",
      "original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "102it [00:00, 29990.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "102it [00:00, 34776.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done calculating f1 scores\n",
      "\n",
      "gpt4o - GEOGRAPHICAL_BIAS Modification:\n",
      "Original Mean F1: 64.277\n",
      "Modified Mean F1: 69.178\n",
      "Weighted Delta: 4.430\n",
      "P-value: 0.0651\n",
      "Significance: .\n",
      "claude-0shot-negation_100_compare.csv\n",
      "negation_100_compare.csv\n",
      "\n",
      "Results from claude-0shot-negation_100_compare.csv:\n",
      "==================================================\n",
      "gpt4o-0shot-grammatical_role_100_compare.csv\n",
      "grammatical_role_100_compare.csv\n",
      "\n",
      "Results from gpt4o-0shot-grammatical_role_100_compare.csv:\n",
      "==================================================\n",
      "llama-0shot-typo_bias_100_compare.csv\n",
      "typo_bias_100_compare.csv\n",
      "\n",
      "Results from llama-0shot-typo_bias_100_compare.csv:\n",
      "==================================================\n",
      "gpt4o-0shot-typo_bias_100.csv\n",
      "typo_bias_100.csv\n",
      "\n",
      "Results from gpt4o-0shot-typo_bias_100.csv:\n",
      "==================================================\n",
      "llama-0shot-discourse_100_new.csv\n",
      "discourse\n",
      "\n",
      "Results from llama-0shot-discourse_100_new.csv:\n",
      "==================================================\n",
      "original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "72it [00:00, 37938.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "72it [00:00, 40886.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done calculating f1 scores\n",
      "\n",
      "llama - DISCOURSE Modification:\n",
      "Original Mean F1: 58.555\n",
      "Modified Mean F1: 57.366\n",
      "Weighted Delta: -1.050\n",
      "P-value: 0.3946\n",
      "Significance: ns\n",
      "gpt4o-0shot-geographical_bias_100_compare.csv\n",
      "geographical_bias_100_compare.csv\n",
      "\n",
      "Results from gpt4o-0shot-geographical_bias_100_compare.csv:\n",
      "==================================================\n",
      "claude-0shot-negation_100.csv\n",
      "negation_100.csv\n",
      "\n",
      "Results from claude-0shot-negation_100.csv:\n",
      "==================================================\n",
      "llama-0shot-dialectal_100_new.csv\n",
      "dialectal\n",
      "\n",
      "Results from llama-0shot-dialectal_100_new.csv:\n",
      "==================================================\n",
      "original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "99it [00:00, 44103.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "99it [00:00, 47832.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done calculating f1 scores\n",
      "\n",
      "llama - DIALECTAL Modification:\n",
      "Original Mean F1: 57.443\n",
      "Modified Mean F1: 56.364\n",
      "Weighted Delta: -0.949\n",
      "P-value: 0.6460\n",
      "Significance: ns\n",
      "llama-0shot-capitalization_100_new.csv\n",
      "capitalization\n",
      "\n",
      "Results from llama-0shot-capitalization_100_new.csv:\n",
      "==================================================\n",
      "original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100it [00:00, 47760.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:00, 48782.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done calculating f1 scores\n",
      "\n",
      "llama - CAPITALIZATION Modification:\n",
      "Original Mean F1: 62.331\n",
      "Modified Mean F1: 55.166\n",
      "Weighted Delta: -6.430\n",
      "P-value: 0.0410\n",
      "Significance: *\n",
      "llama-0shot-derivation_100.csv\n",
      "derivation_100.csv\n",
      "\n",
      "Results from llama-0shot-derivation_100.csv:\n",
      "==================================================\n",
      "claude-0shot-concept_replacement_100_compare.csv\n",
      "concept_replacement_100_compare.csv\n",
      "\n",
      "Results from claude-0shot-concept_replacement_100_compare.csv:\n",
      "==================================================\n",
      "llama-0shot-typo_bias_100_new.csv\n",
      "typo_bias\n",
      "\n",
      "Results from llama-0shot-typo_bias_100_new.csv:\n",
      "==================================================\n",
      "original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:00, 45104.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:00, 37409.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done calculating f1 scores\n",
      "\n",
      "llama - TYPO_BIAS Modification:\n",
      "Original Mean F1: 53.180\n",
      "Modified Mean F1: 54.728\n",
      "Weighted Delta: 1.336\n",
      "P-value: 0.2289\n",
      "Significance: ns\n",
      "llama-0shot-grammatical_role_100_compare.csv\n",
      "grammatical_role_100_compare.csv\n",
      "\n",
      "Results from llama-0shot-grammatical_role_100_compare.csv:\n",
      "==================================================\n",
      "claude-0shot-sentiment_100_new.csv\n",
      "sentiment\n",
      "\n",
      "Results from claude-0shot-sentiment_100_new.csv:\n",
      "==================================================\n",
      "original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "123it [00:00, 48278.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "123it [00:00, 49058.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done calculating f1 scores\n",
      "\n",
      "claude - SENTIMENT Modification:\n",
      "Original Mean F1: 54.543\n",
      "Modified Mean F1: 51.578\n",
      "Weighted Delta: -2.575\n",
      "P-value: 0.0172\n",
      "Significance: *\n",
      "llama-DP.csv\n",
      "claude-0shot-singlish_100_new.csv\n",
      "singlish\n",
      "\n",
      "Results from claude-0shot-singlish_100_new.csv:\n",
      "==================================================\n",
      "original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "96it [00:00, 48629.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "96it [00:00, 60842.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done calculating f1 scores\n",
      "\n",
      "claude - SINGLISH Modification:\n",
      "Original Mean F1: 52.187\n",
      "Modified Mean F1: 47.124\n",
      "Weighted Delta: -4.348\n",
      "P-value: 0.0493\n",
      "Significance: *\n",
      "claude-0shot-geographical_bias_100_compare.csv\n",
      "geographical_bias_100_compare.csv\n",
      "\n",
      "Results from claude-0shot-geographical_bias_100_compare.csv:\n",
      "==================================================\n",
      "llama-0shot-length_bias_100_new.csv\n",
      "length_bias\n",
      "\n",
      "Results from llama-0shot-length_bias_100_new.csv:\n",
      "==================================================\n",
      "original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "92it [00:00, 43114.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "92it [00:00, 42898.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done calculating f1 scores\n",
      "\n",
      "llama - LENGTH_BIAS Modification:\n",
      "Original Mean F1: 54.248\n",
      "Modified Mean F1: 55.622\n",
      "Weighted Delta: 1.192\n",
      "P-value: 0.5196\n",
      "Significance: ns\n",
      "claude-0shot-ner.csv\n",
      "gpt4o-0shot-casual_100_new.csv\n",
      "casual\n",
      "\n",
      "Results from gpt4o-0shot-casual_100_new.csv:\n",
      "==================================================\n",
      "original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98it [00:00, 41141.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98it [00:00, 43345.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done calculating f1 scores\n",
      "\n",
      "gpt4o - CASUAL Modification:\n",
      "Original Mean F1: 56.961\n",
      "Modified Mean F1: 57.416\n",
      "Weighted Delta: 0.399\n",
      "P-value: 0.3318\n",
      "Significance: ns\n",
      "claude-0shot-derivation_100_compare.csv\n",
      "derivation_100_compare.csv\n",
      "\n",
      "Results from claude-0shot-derivation_100_compare.csv:\n",
      "==================================================\n",
      "llama-0shot-negation_100_compare.csv\n",
      "negation_100_compare.csv\n",
      "\n",
      "Results from llama-0shot-negation_100_compare.csv:\n",
      "==================================================\n",
      "claude-0shot-discourse_100.csv\n",
      "discourse_100.csv\n",
      "\n",
      "Results from claude-0shot-discourse_100.csv:\n",
      "==================================================\n",
      "claude-0shot-coordinating_conjunction_100_new.csv\n",
      "coordinating_conjunction\n",
      "\n",
      "Results from claude-0shot-coordinating_conjunction_100_new.csv:\n",
      "==================================================\n",
      "original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "61it [00:00, 36903.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "61it [00:00, 30214.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done calculating f1 scores\n",
      "\n",
      "claude - COORDINATING_CONJUNCTION Modification:\n",
      "Original Mean F1: 69.680\n",
      "Modified Mean F1: 69.382\n",
      "Weighted Delta: -0.275\n",
      "P-value: 0.7167\n",
      "Significance: ns\n",
      "gpt4o-0shot-length_bias_100_compare.csv\n",
      "length_bias_100_compare.csv\n",
      "\n",
      "Results from gpt4o-0shot-length_bias_100_compare.csv:\n",
      "==================================================\n",
      "gpt4o-0shot-coordinating_conjunction_100_new.csv\n",
      "coordinating_conjunction\n",
      "\n",
      "Results from gpt4o-0shot-coordinating_conjunction_100_new.csv:\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "61it [00:00, 31091.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "61it [00:00, 31102.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done calculating f1 scores\n",
      "\n",
      "gpt4o - COORDINATING_CONJUNCTION Modification:\n",
      "Original Mean F1: 64.273\n",
      "Modified Mean F1: 68.988\n",
      "Weighted Delta: 4.263\n",
      "P-value: 0.0411\n",
      "Significance: *\n",
      "claude-0shot-casual_100_new.csv\n",
      "casual\n",
      "\n",
      "Results from claude-0shot-casual_100_new.csv:\n",
      "==================================================\n",
      "original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98it [00:00, 49003.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98it [00:00, 42349.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done calculating f1 scores\n",
      "\n",
      "claude - CASUAL Modification:\n",
      "Original Mean F1: 50.492\n",
      "Modified Mean F1: 49.287\n",
      "Weighted Delta: -1.027\n",
      "P-value: 0.5316\n",
      "Significance: ns\n",
      "gpt4o-0shot-negation_100.csv\n",
      "negation_100.csv\n",
      "\n",
      "Results from gpt4o-0shot-negation_100.csv:\n",
      "==================================================\n",
      "gpt4o-0shot-negation_100_compare.csv\n",
      "negation_100_compare.csv\n",
      "\n",
      "Results from gpt4o-0shot-negation_100_compare.csv:\n",
      "==================================================\n",
      "claude-0shot-typo_bias_100_compare.csv\n",
      "typo_bias_100_compare.csv\n",
      "\n",
      "Results from claude-0shot-typo_bias_100_compare.csv:\n",
      "==================================================\n",
      "gpt4o-0shot-concept_replacement_100.csv\n",
      "concept_replacement_100.csv\n",
      "\n",
      "Results from gpt4o-0shot-concept_replacement_100.csv:\n",
      "==================================================\n",
      "gpt4o-0shot-compound_word_100.csv\n",
      "compound_word_100.csv\n",
      "\n",
      "Results from gpt4o-0shot-compound_word_100.csv:\n",
      "==================================================\n",
      "gpt4o-0shot-negation_100_new.csv\n",
      "negation\n",
      "\n",
      "Results from gpt4o-0shot-negation_100_new.csv:\n",
      "==================================================\n",
      "original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "110it [00:00, 37324.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "110it [00:00, 36317.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done calculating f1 scores\n",
      "\n",
      "gpt4o - NEGATION Modification:\n",
      "Original Mean F1: 50.776\n",
      "Modified Mean F1: 50.885\n",
      "Weighted Delta: 0.093\n",
      "P-value: 0.4870\n",
      "Significance: ns\n",
      "llama-0shot-sentiment_100.csv\n",
      "sentiment_100.csv\n",
      "\n",
      "Results from llama-0shot-sentiment_100.csv:\n",
      "==================================================\n",
      "gpt4o-0shot-active_to_passive_100_new.csv\n",
      "active_to_passive\n",
      "\n",
      "Results from gpt4o-0shot-active_to_passive_100_new.csv:\n",
      "==================================================\n",
      "original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "81it [00:00, 39001.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "81it [00:00, 42999.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done calculating f1 scores\n",
      "\n",
      "gpt4o - ACTIVE_TO_PASSIVE Modification:\n",
      "Original Mean F1: 52.995\n",
      "Modified Mean F1: 55.889\n",
      "Weighted Delta: 2.495\n",
      "P-value: 0.3258\n",
      "Significance: ns\n",
      "gpt4o-0shot-concept_replacement_100_new.csv\n",
      "concept_replacement\n",
      "\n",
      "Results from gpt4o-0shot-concept_replacement_100_new.csv:\n",
      "==================================================\n",
      "original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "85it [00:00, 29951.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "85it [00:00, 38688.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done calculating f1 scores\n",
      "\n",
      "gpt4o - CONCEPT_REPLACEMENT Modification:\n",
      "Original Mean F1: 52.962\n",
      "Modified Mean F1: 57.130\n",
      "Weighted Delta: 3.593\n",
      "P-value: 0.0464\n",
      "Significance: *\n",
      "claude-0shot-punctuation_100_compare.csv\n",
      "punctuation_100_compare.csv\n",
      "\n",
      "Results from claude-0shot-punctuation_100_compare.csv:\n",
      "==================================================\n",
      "gpt4o-0shot-temporal_bias_100_compare.csv\n",
      "temporal_bias_100_compare.csv\n",
      "\n",
      "Results from gpt4o-0shot-temporal_bias_100_compare.csv:\n",
      "==================================================\n",
      "\n",
      "Results saved to ner_modification_results_llm.csv\n",
      "\n",
      "Negation results saved to ner_negation_type_results_llm.csv\n"
     ]
    }
   ],
   "source": [
    "# Read and analyze modification results from rahmad directory\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from pathlib import Path\n",
    "import json\n",
    "rahmad_results_dir = '../../eval/results/rahmad/'\n",
    "rahmad_results_files = glob.glob(os.path.join(rahmad_results_dir, '*.csv'))\n",
    "\n",
    "print(\"\\nAnalyzing results from rahmad directory:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Create list to store results\n",
    "results_data = []\n",
    "negation_results_data = []\n",
    "\n",
    "for results_file in rahmad_results_files:\n",
    "    # Extract model and modification from filename\n",
    "    filename = os.path.basename(results_file)\n",
    "    print(filename)\n",
    "    if 'DP' in filename or 'ner' in filename:\n",
    "        continue\n",
    "    model = filename.split('-')[0]\n",
    "    modification = filename.split('-')[2].replace('_100_new.csv', '')\n",
    "    print(modification)\n",
    "    \n",
    "    print(f\"\\nResults from {filename}:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(results_file)\n",
    "    compare_file = Path(f'../../preprocessing/data_after_phase2/rahmad/{modification}_100.json')\n",
    "    if not compare_file.exists():\n",
    "        continue\n",
    "    compare_df = json.load(open(compare_file))\n",
    "    if len(compare_df) != len(df):\n",
    "        print('mismatch',modification, model)\n",
    "    # Calculate macro F1 scores\n",
    "    # Get labels and predictions\n",
    "    # Get original and modified labels/predictions\n",
    "    ori_labels = df['original_label'].values\n",
    "    ori_preds = df['original_pred'].values\n",
    "    mod_labels = df['modified_label'].values\n",
    "    mod_preds = df['modified_pred'].values\n",
    "    # Calculate F1 scores using helper functions\n",
    "    ori_f1_scores = []\n",
    "    modif_f1_scores = []\n",
    "    print('original')\n",
    "    for l, p in tqdm(zip(ori_labels, ori_preds)):\n",
    "        f1, _ = get_example_f1_and_counts_list(l, p)\n",
    "        ori_f1_scores.append(f1 * 100)\n",
    "    print('modified')\n",
    "    for l, p in tqdm(zip(mod_labels, mod_preds)):\n",
    "        f1, _ = get_example_f1_and_counts_list(l, p)\n",
    "        modif_f1_scores.append(f1 * 100)\n",
    "    print('done calculating f1 scores')\n",
    "    # Calculate mean F1 scores\n",
    "    ori_mean_f1 = np.mean(ori_f1_scores)\n",
    "    modif_mean_f1 = np.mean(modif_f1_scores)\n",
    "    # Calculate weighted delta\n",
    "    weighted_delta = (modif_mean_f1 - ori_mean_f1) * np.log10(ori_mean_f1) / np.log10(100)\n",
    "    # Perform t-test\n",
    "    _, p_value_mw = stats.mannwhitneyu(ori_f1_scores, modif_f1_scores, alternative='two-sided')\n",
    "    _, p_value_w = stats.wilcoxon(ori_f1_scores, modif_f1_scores)\n",
    "    p_value = min(p_value_mw, p_value_w)\n",
    "    \n",
    "    # Determine significance level\n",
    "    if p_value < 0.01:\n",
    "        significance = \"**\"\n",
    "    elif p_value < 0.05:\n",
    "        significance = \"*\"\n",
    "    elif p_value < 0.1:\n",
    "        significance = \".\"\n",
    "    else:\n",
    "        significance = \"ns\"\n",
    "        \n",
    "    print(f\"\\n{model} - {modification.upper()} Modification:\")\n",
    "    print(f\"Original Mean F1: {ori_mean_f1:.3f}\")\n",
    "    print(f\"Modified Mean F1: {modif_mean_f1:.3f}\")\n",
    "    print(f\"Weighted Delta: {weighted_delta:.3f}\")\n",
    "    print(f\"P-value: {p_value:.4f}\")\n",
    "    print(f\"Significance: {significance}\")\n",
    "    \n",
    "    # For negation, get subtype results\n",
    "    if modification == 'negation':\n",
    "        for subtype in ['verbal', 'lexical', 'double', 'approximate', 'absolute']:\n",
    "            subtype_df = df[df['type'] == subtype]\n",
    "            if len(subtype_df) == 0:\n",
    "                continue\n",
    "                \n",
    "            # Calculate F1 scores for subtype\n",
    "            ori_subtype_f1 = []\n",
    "            mod_subtype_f1 = []\n",
    "            \n",
    "            for l, p in zip(subtype_df['original_label'], subtype_df['original_pred']):\n",
    "                f1, _ = get_example_f1_and_counts_list(l, p)\n",
    "                ori_subtype_f1.append(f1 * 100)\n",
    "                \n",
    "            for l, p in zip(subtype_df['modified_label'], subtype_df['modified_pred']):\n",
    "                f1, _ = get_example_f1_and_counts_list(l, p)\n",
    "                mod_subtype_f1.append(f1 * 100)\n",
    "                \n",
    "            # Calculate stats\n",
    "            ori_mean = np.mean(ori_subtype_f1)\n",
    "            mod_mean = np.mean(mod_subtype_f1)\n",
    "            weighted_delta = (mod_mean - ori_mean) * np.log10(ori_mean) / np.log10(100)\n",
    "            \n",
    "            # Statistical tests\n",
    "            _, p_mw = stats.mannwhitneyu(ori_subtype_f1, mod_subtype_f1, alternative='two-sided')\n",
    "            _, p_w = stats.wilcoxon(ori_subtype_f1, mod_subtype_f1)\n",
    "            p_val = min(p_mw, p_w)\n",
    "            \n",
    "            # Determine significance\n",
    "            if p_val < 0.001:\n",
    "                sig = '***'\n",
    "            elif p_val < 0.01:\n",
    "                sig = '**'\n",
    "            elif p_val < 0.05:\n",
    "                sig = '*'\n",
    "            elif p_val < 0.1:\n",
    "                sig = '.'\n",
    "            else:\n",
    "                sig = 'ns'\n",
    "                \n",
    "            # Store subtype results in negation results\n",
    "            negation_results_data.append({\n",
    "                'model': model,\n",
    "                'negation_type': subtype,\n",
    "                'original_mean_f1': ori_mean,\n",
    "                'modified_mean_f1': mod_mean,\n",
    "                'weighted_delta': weighted_delta,\n",
    "                'sample_size': len(subtype_df),\n",
    "                'p_value': p_val,\n",
    "                'significance': sig\n",
    "            })\n",
    "    \n",
    "    # Store main results\n",
    "    results_data.append({\n",
    "        'model': model,\n",
    "        'modification': modification,\n",
    "        'original_mean_f1': ori_mean_f1,\n",
    "        'modified_mean_f1': modif_mean_f1,\n",
    "        'weighted_delta': weighted_delta,\n",
    "        'p_value': p_value,\n",
    "        'significance': significance\n",
    "    })\n",
    "\n",
    "# Create DataFrame and save to CSV\n",
    "results_df = pd.DataFrame(results_data)\n",
    "results_df.to_csv('ner_modification_results_llm.csv', index=False)\n",
    "print(\"\\nResults saved to ner_modification_results_llm.csv\")\n",
    "\n",
    "# Save negation results\n",
    "negation_results_df = pd.DataFrame(negation_results_data)\n",
    "negation_results_df.to_csv('ner_negation_type_results_llm.csv', index=False)\n",
    "print(\"\\nNegation results saved to ner_negation_type_results_llm.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combined results saved to ner_modification_results_combined.csv\n",
      "\n",
      "Combined negation results saved to ner_negation_type_results_combined.csv\n"
     ]
    }
   ],
   "source": [
    "# Read both CSV files\n",
    "llm_results = pd.read_csv('ner_modification_results_llm.csv')\n",
    "plm_results = pd.read_csv('ner_modification_results_plm.csv')\n",
    "\n",
    "# Combine the dataframes\n",
    "combined_results = pd.concat([llm_results, plm_results], ignore_index=True)\n",
    "\n",
    "# Save combined results\n",
    "combined_results.to_csv('ner_modification_results_combined.csv', index=False)\n",
    "print(\"\\nCombined results saved to ner_modification_results_combined.csv\")\n",
    "\n",
    "negation_results_llm = pd.read_csv('ner_negation_type_results_llm.csv')\n",
    "negation_results_plm = pd.read_csv('ner_negation_type_results_plm.csv')\n",
    "\n",
    "combined_negation_results = pd.concat([negation_results_llm, negation_results_plm], ignore_index=True)\n",
    "\n",
    "combined_negation_results.to_csv('ner_negation_type_results_combined.csv', index=False)\n",
    "print(\"\\nCombined negation results saved to ner_negation_type_results_combined.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "modification_order =[\"B: Tem\", \"B: Geo\", \"B: Len\", \"O: Spell\",\"O: Cap\",\"O: Punc\",\n",
    "\"M: Deri\",\n",
    "\"M: Com\",\n",
    "\"Sx: Voice\",\n",
    "\"Sx: Gra\",\n",
    "\"Sx: Conj\",\n",
    "\"Sm: Con\",\n",
    "\"P: Neg\",\n",
    "\"P: Disc\",\n",
    "\"P: Senti\",\n",
    "\"G: Cas\",\n",
    "\"G: Dial\",\n",
    "\"G: Sing\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model                              BERT  Claude 3.5      GPT-2    GPT-4o  \\\n",
      "category      modification                                                 \n",
      "Bias          Geographical     7.246421   -4.078650   1.042956  4.430418   \n",
      "              Length          -0.433732   -4.143969   0.244144  4.185142   \n",
      "              Temporal        -4.977408   -3.687997  -2.310513 -3.361895   \n",
      "Genre         Casual          -2.337139   -1.026684  -2.196722  0.399290   \n",
      "              Dialectal       -6.829746   -1.250404  -3.255341  2.816613   \n",
      "              Singlish       -11.513399   -4.347931  -4.296672 -0.834512   \n",
      "Morphological Compound        -0.257036   -1.482735  -0.314691 -1.388483   \n",
      "              Derivation      -0.193705   -1.709467  -2.493454 -0.006384   \n",
      "Orthographic  Capitalization -12.276385   -3.773813 -16.853842 -0.512300   \n",
      "              Punctuation     -4.466147   -6.675797   1.221348 -2.964893   \n",
      "              Spelling        -3.247760   -0.856916  -0.873977  0.464285   \n",
      "Pragmatic     Discourse       -0.532594   -0.879770  -1.227179  0.082582   \n",
      "              Sentiment        0.224206   -2.575448  -1.175567  0.048929   \n",
      "Semantic      Concept         -1.863668   -0.068948  -3.486979  3.592807   \n",
      "              Negation        -2.036705   -0.084966  -0.178747 -4.571956   \n",
      "Syntactic     Conjunction     -1.938696   -0.275008   0.514074  4.262550   \n",
      "              Grammar         -2.630346   -2.075111  -2.759425  1.818423   \n",
      "              Voice           -0.040733    1.585972   0.167325  2.494836   \n",
      "\n",
      "model                         Llama 3.1         T5  \n",
      "category      modification                          \n",
      "Bias          Geographical     0.662083   1.530873  \n",
      "              Length           1.192207   0.555912  \n",
      "              Temporal        -1.153841   0.343940  \n",
      "Genre         Casual          -2.180688  -6.958744  \n",
      "              Dialectal       -0.948762  -3.779432  \n",
      "              Singlish        -2.550289  -6.828671  \n",
      "Morphological Compound        -0.854596   1.944204  \n",
      "              Derivation      -1.098965   1.027755  \n",
      "Orthographic  Capitalization  -6.429610 -10.692719  \n",
      "              Punctuation     -5.688901  -1.893465  \n",
      "              Spelling         1.335980  -0.841268  \n",
      "Pragmatic     Discourse       -1.050172  -1.035027  \n",
      "              Sentiment       -1.543387  -2.554572  \n",
      "Semantic      Concept         -2.559461  -0.648489  \n",
      "              Negation        -0.338853  -0.028816  \n",
      "Syntactic     Conjunction      0.228603   1.322581  \n",
      "              Grammar         -0.343559  -0.713036  \n",
      "              Voice            1.798770   1.068799  \n",
      "LaTeX table saved to ner_results_table.tex\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Read the combined results\n",
    "df = pd.read_csv('ner_modification_results_combined.csv')\n",
    "\n",
    "# Create mapping from modification names to standardized names with categories\n",
    "mod_mapping = {\n",
    "    'temporal_bias': ('Bias', 'Temporal'),\n",
    "    'geographical_bias': ('Bias', 'Geographical'), \n",
    "    'length_bias': ('Bias', 'Length'),\n",
    "    'typo_bias': ('Orthographic', 'Spelling'),\n",
    "    'capitalization': ('Orthographic', 'Capitalization'),\n",
    "    'punctuation': ('Orthographic', 'Punctuation'),\n",
    "    'derivation': ('Morphological', 'Derivation'),\n",
    "    'compound_word': ('Morphological', 'Compound'),\n",
    "    'active_to_passive': ('Syntactic', 'Voice'),\n",
    "    'grammatical_role': ('Syntactic', 'Grammar'),\n",
    "    'coordinating_conjunction': ('Syntactic', 'Conjunction'),\n",
    "    'concept_replacement': ('Semantic', 'Concept'),\n",
    "    'negation': ('Semantic', 'Negation'),\n",
    "    'discourse': ('Pragmatic', 'Discourse'),\n",
    "    'sentiment': ('Pragmatic', 'Sentiment'),\n",
    "    'casual': ('Genre', 'Casual'),\n",
    "    'dialectal': ('Genre', 'Dialectal'),\n",
    "    'singlish': ('Genre', 'Singlish')\n",
    "}\n",
    "\n",
    "# Define model order and normalize names\n",
    "model_order = ['BERT', 'GPT-2', 'T5', 'GPT-4o', 'Claude 3.5', 'Llama 3.1']\n",
    "model_map = {'gpt4o': 'GPT-4o', 'claude': 'Claude 3.5', 'llama': 'Llama 3.1', 'GPT2': 'GPT-2'}\n",
    "df['model'] = df['model'].replace(model_map)\n",
    "\n",
    "# Add category and modification columns\n",
    "df['category'] = df['modification'].map(lambda x: mod_mapping[x][0])\n",
    "df['modification'] = df['modification'].map(lambda x: mod_mapping[x][1])\n",
    "\n",
    "# Pivot the data to get modifications as rows and models as columns\n",
    "pivot_df = df.pivot(index=['category', 'modification'], columns='model', values='weighted_delta')\n",
    "p_values = df.pivot(index=['category', 'modification'], columns='model', values='p_value')\n",
    "significance = df.pivot(index=['category', 'modification'], columns='model', values='significance')\n",
    "\n",
    "print(pivot_df)\n",
    "\n",
    "# Function to generate color based on value\n",
    "def get_color(val, sig):\n",
    "    if np.isnan(val):\n",
    "        return ''\n",
    "    elif val > 0:\n",
    "        # Green gradient for positive values\n",
    "        intensity = min(abs(val)/10, 1)  # Scale to max 10% change\n",
    "        val_str = f'+{val:.2f}'\n",
    "        if sig == '.':  # Just bold for '.'\n",
    "            val_str = f'\\\\textbf{{{val_str}}}'\n",
    "        elif sig == '*':  # One asterisk\n",
    "            val_str = f'\\\\textbf{{{val_str}}}*'\n",
    "        elif sig == '**':  # Two asterisks\n",
    "            val_str = f'\\\\textbf{{{val_str}}}**'\n",
    "        return f'\\\\cellcolor{{green!{int(intensity*30)}}} {val_str}'\n",
    "    else:\n",
    "        # Red gradient for negative values\n",
    "        intensity = min(abs(val)/10, 1)  # Scale to max 10% change\n",
    "        val_str = f'{val:.2f}'\n",
    "        if sig == '.':  # Just bold for '.'\n",
    "            val_str = f'\\\\textbf{{{val_str}}}'\n",
    "        elif sig == '*':  # One asterisk\n",
    "            val_str = f'\\\\textbf{{{val_str}}}*'\n",
    "        elif sig == '**':  # Two asterisks\n",
    "            val_str = f'\\\\textbf{{{val_str}}}**'\n",
    "        return f'\\\\cellcolor{{red!{int(intensity*30)}}} {val_str}'\n",
    "\n",
    "# Generate LaTeX table\n",
    "latex_table = '\\\\begin{table}[h]\\n\\\\centering\\n\\\\resizebox{\\\\linewidth}{!}{\\n\\\\begin{tabular}{llr' + 'r'*len(model_order) + '}\\n'\n",
    "latex_table += '\\\\hline\\n'\n",
    "latex_table += 'Category & Modification & ' + ' & '.join([f'\\\\textbf{{{col}}}' for col in model_order]) + ' \\\\\\\\\\n'\n",
    "latex_table += '\\\\hline\\n'\n",
    "\n",
    "# Keep original order from mod_mapping\n",
    "categories_seen = []\n",
    "for mod, (category, modification) in mod_mapping.items():\n",
    "    if category not in categories_seen:\n",
    "        if categories_seen:  # Add hline between categories except before first\n",
    "            latex_table += '\\\\hline\\n'\n",
    "        categories_seen.append(category)\n",
    "        row_start = f'\\\\textbf{{{category}}}'\n",
    "    else:\n",
    "        row_start = ' '\n",
    "    \n",
    "    latex_table += f'{row_start} & \\\\textbf{{{modification}}} & '\n",
    "    latex_table += ' & '.join([get_color(pivot_df.loc[(category, modification), col], \n",
    "                                       significance.loc[(category, modification), col]) for col in model_order]) + ' \\\\\\\\\\n'\n",
    "\n",
    "latex_table += '\\\\hline\\n'\n",
    "latex_table += '\\\\end{tabular}}\\n'\n",
    "latex_table += '\\\\caption{Weighted Delta Score by Model and Modification Type}\\n'\n",
    "latex_table += '\\\\label{tab:ner_results}\\n'\n",
    "latex_table += '\\\\end{table}'\n",
    "\n",
    "# Save to file\n",
    "with open('ner_results_table.tex', 'w') as f:\n",
    "    f.write(latex_table)\n",
    "\n",
    "print(\"LaTeX table saved to ner_results_table.tex\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to ner_results_df.csv\n",
      "Negation results saved to ner_negation_results_df.csv\n"
     ]
    }
   ],
   "source": [
    "# Load results from CSV\n",
    "results_df = pd.read_csv('ner_modification_results_combined.csv')\n",
    "negation_results_df = pd.read_csv('ner_negation_type_results_combined.csv')\n",
    "# Create lists of unique modifications and models\n",
    "modification_order = list(mod_mapping.keys())\n",
    "model_order = ['BERT', 'GPT2', 'T5', 'gpt4o', 'claude', 'llama']\n",
    "negation_order = ['verbal', 'lexical', 'double', 'approximate', 'absolute']\n",
    "# Create empty DataFrame with multi-level columns\n",
    "columns = pd.MultiIndex.from_product([model_order, ['original', 'modified', 'diff']])\n",
    "results_df_pivot = pd.DataFrame(index=modification_order, columns=columns)\n",
    "\n",
    "negation_columns = pd.MultiIndex.from_product([model_order, ['original', 'modified', 'diff']])\n",
    "negation_results_df_pivot = pd.DataFrame(index=negation_order, columns=negation_columns)\n",
    "# Fill DataFrame\n",
    "for mod in modification_order:\n",
    "    for model in model_order:\n",
    "        row = results_df[(results_df['modification'] == mod) & (results_df['model'] == model)]\n",
    "        if not row.empty:\n",
    "            results_df_pivot.loc[mod, (model, 'original')] = row['original_mean_f1'].values[0]\n",
    "            results_df_pivot.loc[mod, (model, 'modified')] = row['modified_mean_f1'].values[0]\n",
    "            results_df_pivot.loc[mod, (model, 'diff')] = row['mean_f1_pct_change'].values[0]\n",
    "            if mod == 'temporal_bias' and model == 'bert':\n",
    "                print(row)\n",
    "        \n",
    "for mod in negation_order:\n",
    "    for model in model_order:\n",
    "        row = negation_results_df[(negation_results_df['negation_type'] == mod) & (negation_results_df['model'] == model)]\n",
    "        if not row.empty:\n",
    "            negation_results_df_pivot.loc[mod, (model, 'original')] = row['original_mean_f1'].values[0]\n",
    "            negation_results_df_pivot.loc[mod, (model, 'modified')] = row['modified_mean_f1'].values[0]\n",
    "            negation_results_df_pivot.loc[mod, (model, 'diff')] = row['mean_f1_pct_change'].values[0]\n",
    "            if mod == 'temporal_bias' and model == 'bert':\n",
    "                print(row)\n",
    "\n",
    "\n",
    "# Save to CSV\n",
    "results_df_pivot.to_csv('ner_results_df.csv')\n",
    "negation_results_df_pivot.to_csv('ner_negation_results_df.csv')\n",
    "\n",
    "print(\"Results saved to ner_results_df.csv\")\n",
    "print(\"Negation results saved to ner_negation_results_df.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model              BERT      GPT2        T5    claude     gpt4o     llama\n",
      "negation_type                                                            \n",
      "Verbal        -2.743259  0.104785  1.494364 -1.137108 -1.479953 -0.365473\n",
      "Lexical       -2.725502 -3.123246  0.476221 -2.665256  1.513276  0.119679\n",
      "Double         0.380551 -3.122061 -7.861116  2.605664  7.031438  2.042326\n",
      "Approximate    0.224439  0.149489 -1.748570  3.908497  2.926344 -6.231944\n",
      "Absolute      -2.978162  3.583626  2.732454 -0.084966 -4.571956 -0.338853\n",
      "LaTeX table saved to ner_results_table.tex\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Read the combined results\n",
    "df = pd.read_csv('ner_negation_type_results_combined.csv')\n",
    "\n",
    "# Create mapping from modification names to standardized names\n",
    "negation_order= ['Verbal', 'Lexical', 'Double', 'Approximate', 'Absolute']\n",
    "mod_mapping = {\n",
    "    'verbal': 'Verbal',\n",
    "    'lexical': 'Lexical',\n",
    "    'double': 'Double',\n",
    "    'approximate': 'Approximate',\n",
    "    'absolute': 'Absolute',\n",
    "}\n",
    "\n",
    "# Define model order\n",
    "model_order = ['BERT', 'GPT2', 'T5', 'gpt4o', 'claude', 'llama']\n",
    "\n",
    "# Map the modification names\n",
    "df['negation_type'] = df['negation_type'].map(mod_mapping)\n",
    "\n",
    "# Pivot the data to get modifications as rows and models as columns\n",
    "pivot_df = df.pivot(index='negation_type', columns='model', values='weighted_delta')\n",
    "p_values = df.pivot(index='negation_type', columns='model', values='p_value')\n",
    "significance = df.pivot(index='negation_type', columns='model', values='significance')\n",
    "\n",
    "# Reorder rows according to modification_order\n",
    "pivot_df = pivot_df.reindex(negation_order)\n",
    "p_values = p_values.reindex(negation_order)\n",
    "significance = significance.reindex(negation_order)\n",
    "\n",
    "print(pivot_df)\n",
    "\n",
    "# Function to generate color based on value\n",
    "def get_color(val, sig):\n",
    "    if np.isnan(val):\n",
    "        return ''\n",
    "    elif val > 0:\n",
    "        # Green gradient for positive values\n",
    "        intensity = min(abs(val)/10, 1)  # Scale to max 10% change\n",
    "        val_str = f'+{val:.2f}'\n",
    "        if sig == '.':  # Just bold for '.'\n",
    "            val_str = f'\\\\textbf{{{val_str}}}'\n",
    "        elif sig == '*':  # One asterisk\n",
    "            val_str = f'\\\\textbf{{{val_str}}}*'\n",
    "        elif sig == '**':  # Two asterisks\n",
    "            val_str = f'\\\\textbf{{{val_str}}}**'\n",
    "        return f'\\\\cellcolor{{green!{int(intensity*30)}}} {val_str}'\n",
    "    else:\n",
    "        # Red gradient for negative values\n",
    "        intensity = min(abs(val)/10, 1)  # Scale to max 10% change\n",
    "        val_str = f'{val:.2f}'\n",
    "        if sig == '.':  # Just bold for '.'\n",
    "            val_str = f'\\\\textbf{{{val_str}}}'\n",
    "        elif sig == '*':  # One asterisk\n",
    "            val_str = f'\\\\textbf{{{val_str}}}*'\n",
    "        elif sig == '**':  # Two asterisks\n",
    "            val_str = f'\\\\textbf{{{val_str}}}**'\n",
    "        return f'\\\\cellcolor{{red!{int(intensity*30)}}} {val_str}'\n",
    "\n",
    "# Generate LaTeX table\n",
    "latex_table = '\\\\begin{table}[h]\\n\\\\centering\\n\\\\begin{tabular}{l' + 'r'*len(model_order) + '}\\n'\n",
    "latex_table += '\\\\hline\\n'\n",
    "latex_table += 'Modification & ' + ' & '.join([f'\\\\textbf{{{col}}}' for col in model_order]) + ' \\\\\\\\\\n'\n",
    "latex_table += '\\\\hline\\n'\n",
    "\n",
    "prev_category = None\n",
    "for idx, row in pivot_df.iterrows():\n",
    "    current_category = idx[0]  # Get first character of modification name\n",
    "    if prev_category is not None and current_category != prev_category:\n",
    "        latex_table += '\\\\hline\\n'\n",
    "    prev_category = current_category\n",
    "    \n",
    "    latex_table += f'\\\\textbf{{{idx}}} & '\n",
    "    latex_table += ' & '.join([get_color(row[col], significance.loc[idx, col]) for col in model_order]) + ' \\\\\\\\\\n'\n",
    "\n",
    "latex_table += '\\\\hline\\n'\n",
    "latex_table += '\\\\end{tabular}\\n'\n",
    "latex_table += '\\\\caption{Weighted Delta Score by Model and Modification Type}\\n'\n",
    "latex_table += '\\\\label{tab:ner_results}\\n'\n",
    "latex_table += '\\\\end{table}'\n",
    "\n",
    "# Save to file\n",
    "with open('ner_negation_type_results_table.tex', 'w') as f:\n",
    "    f.write(latex_table)\n",
    "\n",
    "print(\"LaTeX table saved to ner_results_table.tex\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted predictions from 'bert/bert_casual_100_ori.txt' to JSON format and saved to 'parsed_NER_BERT/bert_casual_ori.json'\n",
      "Converted predictions from 'bert/bert_casual_100_modif.txt' to JSON format and saved to 'parsed_NER_BERT/bert_casual_modif.json'\n",
      "Converted predictions from 'gpt2/gpt2_casual_100_modif.txt' to JSON format and saved to 'parsed_NER_GPT2/gpt2_casual_modif.json'\n",
      "Converted predictions from 'gpt2/gpt2_casual_100_ori.txt' to JSON format and saved to 'parsed_NER_GPT2/gpt2_casual_ori.json'\n",
      "Converted predictions from 't5/t5_casual_100_ori.txt' to JSON format and saved to 'parsed_NER_T5/t5_casual_ori.json'\n",
      "Converted predictions from 't5/t5_casual_100_modif.txt' to JSON format and saved to 'parsed_NER_T5/t5_casual_modif.json'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def convert_predictions_to_json(input_file, output_file):\n",
    "    \"\"\"\n",
    "    Converts a tab-separated prediction file to JSON format.\n",
    "    Assumes input file format: token gold_label predicted_label\n",
    "    Sentences are separated by empty lines.\n",
    "    Output JSON format: list of records, each record is a dict with \"text\", \"gold\", \"prediction\" keys.\n",
    "    \"\"\"\n",
    "    records = []\n",
    "    current_tokens = []\n",
    "    current_gold_labels = []\n",
    "    current_pred_labels = []\n",
    "\n",
    "    with open(input_file, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:  # Empty line indicates sentence boundary\n",
    "                if current_tokens:\n",
    "                    text = \" \".join(current_tokens)\n",
    "                    gold_entities = []\n",
    "                    pred_entities = []\n",
    "\n",
    "                    # Process gold entities\n",
    "                    i = 0\n",
    "                    while i < len(current_gold_labels):\n",
    "                        if current_gold_labels[i].startswith('B-'):\n",
    "                            entity_type = current_gold_labels[i][2:]\n",
    "                            start_index = i\n",
    "                            end_index = i\n",
    "                            while end_index + 1 < len(current_gold_labels) and current_gold_labels[end_index + 1].startswith('I-') and current_gold_labels[end_index + 1][2:] == entity_type:\n",
    "                                end_index += 1\n",
    "                            entity_text = \" \".join(current_tokens[start_index:end_index+1])\n",
    "                            gold_entities.append({\"text\": entity_text, \"value\": entity_type})\n",
    "                            i = end_index + 1\n",
    "                        else:\n",
    "                            i += 1\n",
    "\n",
    "                    # Process predicted entities\n",
    "                    i = 0\n",
    "                    while i < len(current_pred_labels):\n",
    "                        if current_pred_labels[i].startswith('B-'):\n",
    "                            entity_type = current_pred_labels[i][2:]\n",
    "                            start_index = i\n",
    "                            end_index = i\n",
    "                            while end_index + 1 < len(current_pred_labels) and current_pred_labels[end_index + 1].startswith('I-') and current_pred_labels[end_index + 1][2:] == entity_type:\n",
    "                                end_index += 1\n",
    "                            entity_text = \" \".join(current_tokens[start_index:end_index+1])\n",
    "                            pred_entities.append({\"text\": entity_text, \"value\": entity_type})\n",
    "                            i = end_index + 1\n",
    "                        else:\n",
    "                            i += 1\n",
    "\n",
    "                    records.append({\n",
    "                        \"text\": text,\n",
    "                        \"gold\": gold_entities,\n",
    "                        \"prediction\": pred_entities\n",
    "                    })\n",
    "                    current_tokens = []\n",
    "                    current_gold_labels = []\n",
    "                    current_pred_labels = []\n",
    "            else:\n",
    "                parts = line.split('\\t')\n",
    "                if len(parts) == 3:\n",
    "                    token, gold_label, pred_label = parts\n",
    "                    current_tokens.append(token)\n",
    "                    current_gold_labels.append(gold_label)\n",
    "                    current_pred_labels.append(pred_label)\n",
    "                elif len(parts) == 2: # in case there is no prediction column\n",
    "                    token, gold_label = parts\n",
    "                    current_tokens.append(token)\n",
    "                    current_gold_labels.append(gold_label)\n",
    "                    current_pred_labels.append(\"O\") # default prediction as O\n",
    "\n",
    "        # Process the last sentence if file does not end with empty line\n",
    "        if current_tokens:\n",
    "            text = \" \".join(current_tokens)\n",
    "            gold_entities = []\n",
    "            pred_entities = []\n",
    "\n",
    "            # Process gold entities\n",
    "            i = 0\n",
    "            while i < len(current_gold_labels):\n",
    "                if current_gold_labels[i].startswith('B-'):\n",
    "                    entity_type = current_gold_labels[i][2:]\n",
    "                    start_index = i\n",
    "                    end_index = i\n",
    "                    while end_index + 1 < len(current_gold_labels) and current_gold_labels[end_index + 1].startswith('I-') and current_gold_labels[end_index + 1][2:] == entity_type:\n",
    "                        end_index += 1\n",
    "                    entity_text = \" \".join(current_tokens[start_index:end_index+1])\n",
    "                    gold_entities.append({\"text\": entity_text, \"value\": entity_type})\n",
    "                    i = end_index + 1\n",
    "                else:\n",
    "                    i += 1\n",
    "\n",
    "            # Process predicted entities\n",
    "            i = 0\n",
    "            while i < len(current_pred_labels):\n",
    "                if current_pred_labels[i].startswith('B-'):\n",
    "                    entity_type = current_pred_labels[i][2:]\n",
    "                    start_index = i\n",
    "                    end_index = i\n",
    "                    while end_index + 1 < len(current_pred_labels) and current_pred_labels[end_index + 1].startswith('I-') and current_pred_labels[end_index + 1][2:] == entity_type:\n",
    "                        end_index += 1\n",
    "                    entity_text = \" \".join(current_tokens[start_index:end_index+1])\n",
    "                    pred_entities.append({\"text\": entity_text, \"value\": entity_type})\n",
    "                    i = end_index + 1\n",
    "                else:\n",
    "                    i += 1\n",
    "            records.append({\n",
    "                \"text\": text,\n",
    "                \"gold\": gold_entities,\n",
    "                \"prediction\": pred_entities\n",
    "            })\n",
    "\n",
    "    with open(output_file, 'w') as outfile:\n",
    "        json.dump(records, outfile, indent=2)\n",
    "\n",
    "# Example usage:\n",
    "for model in ['bert', 'gpt2', 't5']:\n",
    "    input_prediction_files = glob.glob(f\"{model}/*.txt\") # Get all .txt files in t5 directory\n",
    "    for input_prediction_file in input_prediction_files:\n",
    "        if \"casual\" not in input_prediction_file:\n",
    "            continue\n",
    "        output_file = input_prediction_file.split(\".\")[0].replace(\"_100\", \"\")\n",
    "        output_json_file = f\"parsed_NER_{model.upper()}/{os.path.basename(output_file)}.json\"\n",
    "        convert_predictions_to_json(input_prediction_file, output_json_file)\n",
    "        print(f\"Converted predictions from '{input_prediction_file}' to JSON format and saved to '{output_json_file}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
